{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiTj9SYcWav5"
   },
   "outputs": [],
   "source": [
    "def tweet_covid_relational_explanation():\n",
    "                    import random\n",
    "                    import sys\n",
    "                    import random\n",
    "                    import re\n",
    "                    from collections import defaultdict\n",
    "                    import pandas as pd\n",
    "                    tid=0\n",
    "                    c=0\n",
    "                    lng=[]\n",
    "                    text_t=[]\n",
    "                    twitter_id=defaultdict(list)\n",
    "                    users = defaultdict(list)\n",
    "                    twit=defaultdict(list)\n",
    "                    trate=defaultdict(list)\n",
    "                    #pd.read_csv\n",
    "                    ifile1 =pd.read_csv(\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Corona/sampled_medical.csv\",encoding='latin-1')\n",
    "                    header=[]\n",
    "                    users = defaultdict(list)\n",
    "                    for ln in ifile1:\n",
    "                        parts = ln.strip('\" \"\"').split(\",\")\n",
    "                        #print(parts)\n",
    "                        header.append(parts)\n",
    "                        #users[parts[4]].append(tid)\n",
    "                       # twit[tid].append(parts[5])\n",
    "                       # trate[tid]=parts[0].strip('\"')\n",
    "\n",
    "\n",
    "                    for t1 in ifile1['text']:\n",
    "                        text_t.append(t1)\n",
    "                    #sr=[]\n",
    "                    #c=0\n",
    "                    #ver=[]\n",
    "                    sr=[]\n",
    "                    users = defaultdict(list)\n",
    "                    for tt in ifile1['user_id']:#screen_name\n",
    "                            sr.append(tt)\n",
    "                    #Pre-process the Reviews Run2\n",
    "                    import random\n",
    "                    import sys\n",
    "                    import random\n",
    "                    import re\n",
    "                    from collections import defaultdict\n",
    "                    import pandas as pd\n",
    "                    tid=0\n",
    "                    c=0\n",
    "                    lng=[]\n",
    "                    #text_t=[]\n",
    "                    twitter_id=defaultdict(list)\n",
    "                    users = defaultdict(list)\n",
    "                    twit=defaultdict(list)\n",
    "                    trate=defaultdict(list)\n",
    "                    #pd.read_csv\n",
    "                    ifile1 =pd.read_csv(\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Corona/sampled.csv\",encoding='latin-1')\n",
    "                    header=[]\n",
    "\n",
    "                    for ln in ifile1:\n",
    "                        parts = ln.strip('\" \"\"').split(\",\")\n",
    "                        #print(parts)\n",
    "                        header.append(parts)\n",
    "                        #users[parts[4]].append(tid)\n",
    "                       # twit[tid].append(parts[5])\n",
    "                       # trate[tid]=parts[0].strip('\"')\n",
    "\n",
    "\n",
    "                    for t1 in ifile1['text']:\n",
    "                        text_t.append(t1)\n",
    "                    #for t2 in ifile1['screen_name']:\n",
    "                       # sr.append(t2)\n",
    "                    for tt in ifile1['user_id']:#screen_name\n",
    "                            sr.append(tt)\n",
    "                    #Pre-process the Reviews Run3\n",
    "                    #Sample File\n",
    "                    import random\n",
    "                    import sys\n",
    "                    import random\n",
    "                    import re\n",
    "                    from collections import defaultdict\n",
    "                    import pandas as pd\n",
    "                    tid=0\n",
    "                    c=0\n",
    "                    lng=[]\n",
    "                    #text_t=[]\n",
    "                    twitter_id=defaultdict(list)\n",
    "                    users = defaultdict(list)\n",
    "                    twit=defaultdict(list)\n",
    "                    trate=defaultdict(list)\n",
    "                    #pd.read_csv\n",
    "                    ifile1 =pd.read_csv(\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Corona/sampled_textonly_science.csv\",encoding='latin-1')\n",
    "                    header=[]\n",
    "                    users = defaultdict(list)\n",
    "                    for ln in ifile1:\n",
    "                        parts = ln.strip('\" \"\"').split(\",\")\n",
    "                        #print(parts)\n",
    "                        header.append(parts)\n",
    "                        #users[parts[4]].append(tid)\n",
    "                       # twit[tid].append(parts[5])\n",
    "                       # trate[tid]=parts[0].strip('\"')\n",
    "                    rt_ct=[]\n",
    "                    label=[]\n",
    "                    text_t1=[]\n",
    "                    #for t in ifile1['retweet_count']:\n",
    "                            #rt_ct.append(t)\n",
    "                    for t in ifile1['label']:\n",
    "                            label.append(t)\n",
    "                    for t1 in ifile1['text']:\n",
    "                       # print(t1)\n",
    "                        text_t1.append(t1)\n",
    "\n",
    "                    #for tt in ifile1['verified']:\n",
    "                            #ver.append(tt)     \n",
    "\n",
    "\n",
    "                    #concatenate Run 4\n",
    "                    print(len(text_t1),len(label))\n",
    "                    text_t2=[]\n",
    "                    for tt in text_t:\n",
    "                        text_t1.append(tt)\n",
    "                    for ty in range(0,len(text_t)):\n",
    "                        label.append(1)\n",
    "                    print(len(text_t1),len(label))\n",
    "                    #filter twits based on positive and negative twits Run 5\n",
    "                    twiter_id=0\n",
    "                    twit1={}\n",
    "                    twit={}\n",
    "                    twit_count={}\n",
    "                    vcc=0\n",
    "                    sr1=[]\n",
    "                    for t in range(0,len(text_t)):\n",
    "                        #if lng[t]=='en':# and ver[t]==True:\n",
    "                           # if vcc<20000:\n",
    "                                twit[twiter_id]=text_t1[t]\n",
    "                                twit_count[twiter_id]=label[t]\n",
    "                                #print(ver[t])\n",
    "                                #sr1.append(sr[t])\n",
    "                                twiter_id=twiter_id+1\n",
    "                                vcc=vcc+1\n",
    "\n",
    "                    #for y in range(1000,2000):\n",
    "                           #twit[y]=text_t1[y-20000]\n",
    "                    #for y in range(0,40000):\n",
    "                        #print(y,twit[y])\n",
    "                    #Run 7 checking scitific and non-scientific tweets count\n",
    "                    print(len(twit),len(twit_count))\n",
    "                    t_1=[]\n",
    "                    t_0=[]\n",
    "                    vv=0\n",
    "                    for t in twit_count:\n",
    "                        if twit_count[t]==1:\n",
    "                                t_1.append(t)\n",
    "                        elif twit_count[t]==0:\n",
    "                             # if vv<54:\n",
    "                                    t_1.append(t)\n",
    "                                    vv=vv+1\n",
    "\n",
    "                    print(len(t_1))\n",
    "                    # Run 8 check count of atoms\n",
    "                    twit2={}\n",
    "                    for kk in twit:\n",
    "                        if kk in t_1:\n",
    "                            twit2[kk]=twit[kk]\n",
    "                    print(len(twit2))\n",
    "\n",
    "                    #source of tweeter. \n",
    "\n",
    "                    vcc=0\n",
    "                    sr1=[]\n",
    "                    for t in range(0,len(lng)):\n",
    "                        if lng[t]=='en':\n",
    "                            #if vcc<100000:\n",
    "                                sr1.append(sr[t])\n",
    "                                vcc=vcc+1\n",
    "\n",
    "\n",
    "                    #Checking # of tweets. Mapping # of airlines per tweet Run 9\n",
    "                    import sys\n",
    "                    ti=[]\n",
    "                    m_sr={}\n",
    "                    for g in twit.keys():\n",
    "                        ti.append(g)\n",
    "                    #print(ti)   \n",
    "                    ss=set(sr)\n",
    "                    sr2=[]\n",
    "                    for k in ss:\n",
    "                        sr2.append(k)\n",
    "                    #print(sr2)  \n",
    "                    #sys.exit()\n",
    "                    vv=-1\n",
    "                    for j in sr2:\n",
    "                        gh=[]\n",
    "                        vv=vv+1\n",
    "                        for tt in range(0,len(sr)):\n",
    "                                        if str(j)==str(sr[tt]):\n",
    "                                            #print(j)\n",
    "                                           # if ti[tt] not in gh:\n",
    "                                            gh.append(ti[tt])\n",
    "                                    #if len(gh)>=1:\n",
    "                        #print(j,gh)\n",
    "                        m_sr[j]=gh\n",
    "\n",
    "\n",
    "\n",
    "                    #data preprocessing Run 10\n",
    "                    import sys\n",
    "                    import os\n",
    "                    import re\n",
    "                    import string\n",
    "                    from collections import defaultdict\n",
    "                    print(len(twit))\n",
    "                    flags = (re.UNICODE if sys.version < '3' and type(text) is unicode else 0)\n",
    "                    stopwords=[]\n",
    "                    sfile = open(\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Sentiment Twitter/dataworld/stopwords.txt\")\n",
    "                    for ln in sfile:\n",
    "                        stopwords.append(ln.strip().lower())\n",
    "                    sfile.close()\n",
    "\n",
    "                    sentwords=[]\n",
    "                    windex=defaultdict(list)\n",
    "                    sfile1 = open(\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Sentiment Twitter/dataworld/Words.txt\")\n",
    "                    for ln in sfile1:\n",
    "                        sentwords.append(ln.strip().lower())\n",
    "                    sfile1.close()\n",
    "                    WORDS={}\n",
    "                    for t in twit2:\n",
    "                        keep=[]\n",
    "                        #for kk in twit[t]:\n",
    "                        #print(kk)\n",
    "                        kk=twit2[t]\n",
    "                        for word in re.findall(r\"\\w[\\w':]*\", kk, flags=flags):\n",
    "                            if word.isdigit() or len(word)==1:\n",
    "                                                continue\n",
    "                            word_lower = word.lower()\n",
    "                            #print(word_lower)\n",
    "\n",
    "                            if word_lower in stopwords:\n",
    "                                                  continue\n",
    "\n",
    "                            for ty in string.punctuation:\n",
    "                                                if str(ty) in word_lower:\n",
    "                                                    continue\n",
    "                            #for tr in arlu1:\n",
    "                                #if str(tr) in word_lower:\n",
    "                                    #continue\n",
    "                            if word_lower.isalnum():# and word_lower in sentwords:\n",
    "                                if word_lower not in keep:# and word_lower not in vv:\n",
    "                                    if not any(c.isdigit() for c in word_lower):\n",
    "                                            keep.append(word_lower)\n",
    "                            if len(keep)>=6:\n",
    "                                WORDS[t]=keep\n",
    "                                for zzw in keep:\n",
    "                                    if t not in windex:\n",
    "                                        windex[zzw].append(t)\n",
    "\n",
    "\n",
    "                    # Run 11 maping a word occurs  in mutliple tweets\n",
    "                    windex1={}\n",
    "                    for c in windex:\n",
    "                        gh=[]\n",
    "                        ss=set(windex[c])\n",
    "                        for ty in ss:\n",
    "                            gh.append(ty)\n",
    "                        windex1[c]=gh\n",
    "                    for k in windex1:\n",
    "                        pass#print(k,windex1[k])\n",
    "                    #Doc Embedding\n",
    "                    #Sentance generation\n",
    "                    sent2=[]\n",
    "                    sent1=[]\n",
    "                    sent_map=defaultdict(list)\n",
    "                    for ty in WORDS:\n",
    "                        gh=[]\n",
    "                        gh.append(str(ty))\n",
    "                        #gh1=[]\n",
    "                        #gh2=[]\n",
    "                        for j in WORDS[ty]:\n",
    "\n",
    "                            j1=str(j)\n",
    "                            #gh.append(str(ty))\n",
    "                            if j1 not in gh:\n",
    "                                gh.append(j1)\n",
    "                            #print(gh)\n",
    "\n",
    "\n",
    "                        if gh not in sent2:\n",
    "                                sent2.append(gh)\n",
    "\n",
    "\n",
    "                    documents1=[]\n",
    "                    #documents1=[]\n",
    "                    for t in sent2:\n",
    "                        s=''\n",
    "                        for jh in t:\n",
    "                            if  jh.isdigit():\n",
    "                                 documents1.append(jh)\n",
    "                            else:\n",
    "                                s=\" \"+str(jh)+s+\" \"\n",
    "                        documents1.append(s)\n",
    "\n",
    "\n",
    "                    #sentence embedding\n",
    "                    from gensim.test.utils import common_texts\n",
    "                    from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "                    documents2 = [TaggedDocument(doc, [i]) for i, doc in enumerate(sent2)]\n",
    "                    for t in documents2:\n",
    "                        pass# print(t)\n",
    "                    model = Doc2Vec(documents2, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "                    #K-Means Run 14 to find the neighbors per query \n",
    "\n",
    "                    #cluster generation with k-means\n",
    "                    import sys\n",
    "                    from nltk.cluster import KMeansClusterer\n",
    "                    import nltk\n",
    "                    from sklearn import cluster\n",
    "                    from sklearn import metrics\n",
    "                    import gensim \n",
    "                    import operator\n",
    "                    #from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "                    #model = Word2Vec(sent, min_count=1) dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=False)\n",
    "                    import operator\n",
    "                    X = model[model.wv.vocab]\n",
    "                    c=0\n",
    "                    cluster={}\n",
    "                    num=[]\n",
    "                    weight_map={}\n",
    "                    similar_r_map={}\n",
    "                    fg={}\n",
    "                    for jj in WORDS:\n",
    "                        gh1=[]\n",
    "                        gh2=[]\n",
    "                        s=0\n",
    "\n",
    "                        for k in documents1:\n",
    "                            if str(k)==str(jj):\n",
    "                                gh=model.most_similar(positive=str(k),topn=600)\n",
    "                                #print(gh)\n",
    "                                for tt in gh:\n",
    "                                    if float(tt[1]) not in gh1:\n",
    "                                        gh1.append(float(tt[1]))\n",
    "                                    #if tt[0] not in gh2:\n",
    "                                    if tt[0].isdigit():\n",
    "                                            #if ccc<5:\n",
    "                                                    #gh2.append(tt[0])\n",
    "                                                    fg[tt[0]]=tt[1]\n",
    "                                                    #ccc=ccc+1\n",
    "                        #for ffg in gh1:\n",
    "                            #s=s+ffg\n",
    "                        dd=sorted(fg.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                        ccc=0\n",
    "                        for t5 in dd:\n",
    "                            if ccc<500:\n",
    "                                        #gh2.append(t5[0])\n",
    "                                        gh2.append(t5[0])\n",
    "                                        ccc=ccc+1\n",
    "\n",
    "                        if len(gh2)>=2:\n",
    "                                similar_r_map[jj]=gh2\n",
    "                                #ccc=ccc+1\n",
    "                    for ww in similar_r_map:\n",
    "                        pass#print(ww,similar_r_map[ww])\n",
    "                    #user annotated feedback\n",
    "                    t_tword=[]\n",
    "                    f=open(\"annotations_2.txt\")\n",
    "                    for k in f:\n",
    "                        p=k.strip(\"\\n\").split(\":\")\n",
    "                        pp=p[2].split(\",\")\n",
    "                        for kk in pp:\n",
    "                            t_tword.append(kk.strip(\" \"))\n",
    "                    f.close()\n",
    "                    #print(t_tword)\n",
    "\n",
    "                    #annotated explsanation per query\n",
    "                    ann1={}\n",
    "                    c=0\n",
    "                    for k in WORDS:\n",
    "                        c1=0\n",
    "                        c2=0\n",
    "                        if twit_count[k]==1:\n",
    "                            c=c+1\n",
    "                            gff=[]\n",
    "                            for gg in WORDS[k]:\n",
    "                                if gg in t_tword:\n",
    "                                    if c1<5:\n",
    "                                        gff.append(gg)\n",
    "                                        c1=c1+1\n",
    "                            if len(gff)>0:\n",
    "                               # if k in WORDSt:\n",
    "                                    ann1[k]=gff\n",
    "\n",
    "                        elif twit_count[k]==0:\n",
    "                            c=c+1\n",
    "                            gff1=[]\n",
    "                            for gg in WORDS[k]:\n",
    "                                if gg in t_tword:\n",
    "                                    if c2<5:\n",
    "                                            gff1.append(gg)\n",
    "                                            c2=c2+1\n",
    "                            if len(gff1)>0:\n",
    "                                #if k WORDSt:\n",
    "                                    ann1[k]=gff1\n",
    "\n",
    "                    ann={}\n",
    "                    for t in ann1:\n",
    "                        if t in WORDS:\n",
    "                            ann[t]=ann1[t]\n",
    "                    #ff=open(\"annotated.txt\")\n",
    "                    for k in ann:\n",
    "                        pass#print(k,ann[k])\n",
    "                    #related queries \n",
    "                    pqsu={}\n",
    "                    pqsh={}\n",
    "\n",
    "                    for bb in WORDS:\n",
    "                        for kk in m_sr:\n",
    "                            if bb in m_sr[kk] or str(bb) in m_sr[kk] :\n",
    "                                pqsu[bb]=m_sr[kk]\n",
    "                                pass#print(bb,Samuser_index[kk])\n",
    "\n",
    "\n",
    "                    relq={}\n",
    "                    for k in pqsu:\n",
    "                        #print(k,pqsh[k]+pqsu[k])\n",
    "                        relq[k]=pqsu[k]\n",
    "\n",
    "                    qf={}\n",
    "                    for t in similar_r_map:\n",
    "                        if t in WORDS and t in relq:\n",
    "                            h=relq[t]+WORDS[t]\n",
    "                            #print(t,qrat[t],h),\n",
    "                            qf[t]=h\n",
    "                            #print(\"\\n\\n\")\n",
    "                    # train test data\n",
    "                    import math\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    train=[]\n",
    "                    target=[]\n",
    "                    test=[]\n",
    "                    test_tr=[]\n",
    "                    for tt in qf:\n",
    "                        for v in qf[tt]:\n",
    "                            #print(v)\n",
    "                            if v not in train:\n",
    "                               # if v.isdigit()==False:\n",
    "                                    train.append(str(v))\n",
    "                                    target.append(str(twit_count[tt]))\n",
    "                    bb=math.ceil(len(train)*0.5)\n",
    "                    for gg in range(0,bb):\n",
    "                        test.append(train[gg])\n",
    "                        test_tr.append(target[gg])\n",
    "                    #LIME w11, trg\n",
    "                    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    #Shap\n",
    "                    import sklearn\n",
    "                    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    import numpy as np\n",
    "                    import shap\n",
    "                    import transformers\n",
    "                    import shap\n",
    "                    from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "                    #shap.initjs()\n",
    "                    # Kernal Shap words_train targets\n",
    "\n",
    "                    from sklearn import svm\n",
    "                    from sklearn.svm import SVC\n",
    "                    from sklearn.svm import LinearSVC\n",
    "                    corpus_train, corpus_test, y_train, y_test = train_test_split(train,target, test_size=0.5, random_state=7)\n",
    "                    vectorizer = TfidfVectorizer(min_df=1)\n",
    "                    X_train = vectorizer.fit_transform(train)\n",
    "                    X_test = vectorizer.transform(corpus_test)\n",
    "                    model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                    model.fit(X_train,target)\n",
    "                    p = model.predict(X_train)\n",
    "                    prr={}\n",
    "                    for jj in range(0,len(train)):\n",
    "                        prr[train[jj]]=int(p[jj])\n",
    "                    explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"dependent\")\n",
    "                    shap_values = explainer.shap_values(X_train)\n",
    "                    #X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                    feature_names=vectorizer.get_feature_names()\n",
    "                    #print(len(feature_names),len(shap_values))\n",
    "                    #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                    shape_w={}\n",
    "                    fr={}\n",
    "                    feature_sh_v=[]\n",
    "                    for jj in range(0,len(train)):\n",
    "                            if abs(sum(shap_values[jj]))>0.2:\n",
    "                                                  m=abs(sum(shap_values[jj]))\n",
    "                                                  if train[jj] not in fr:\n",
    "                                                                  fr[train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                  elif train[jj]  in fr:\n",
    "                                                        if m>fr[train[jj]]:\n",
    "                                                            fr[train[jj]]=m\n",
    "                    dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                    for tt in dd1:\n",
    "                        #if tt[0].isdigit()==True:\n",
    "                               feature_sh_v.append(tt[0])\n",
    "                       # elif tt[0].isdigit()==False:\n",
    "                            #feature_sh_v.append(tt[0])\n",
    "                            #for vvv5 in WORDS22:\n",
    "                                #if tt[0] in WORDS22[vvv5] or str(tt[0]) in WORDS22[vvv5]:\n",
    "                                                                  # feature_sh_v.append(tt[0])\n",
    "                    '''\n",
    "                    feature_sh_v1=[]\n",
    "                    for v in feature_sh_v:\n",
    "                        n=v.split()\n",
    "                        for k in n:\n",
    "                            if k not in feature_sh_v1:\n",
    "                                feature_sh_v1.append(k)\n",
    "                    '''\n",
    "                    #shap explanations\n",
    "\n",
    "                    shap_exp={}\n",
    "                    for t in qf:\n",
    "                        gh=[]\n",
    "                        c=0\n",
    "                        for k in qf[t]:\n",
    "                            if k in prr or str(k) in prr:\n",
    "                                if twit_count[t]==prr[str(k)]:\n",
    "                                    if k in feature_sh_v or str(k) in feature_sh_v:\n",
    "                                        if k not in gh:\n",
    "                                            #if c<20:\n",
    "                                                gh.append(k)\n",
    "                                                c=c+1\n",
    "                        shap_exp[t]=gh\n",
    "                    shap_word_exp={}\n",
    "                    shap_rel_exp={}\n",
    "                    for hh in shap_exp:\n",
    "                        gh=[]\n",
    "                        gh1=[]\n",
    "                        cc=0\n",
    "                        cc1=0\n",
    "                        for hh1 in shap_exp[hh]:\n",
    "                            if str(hh1).isdigit()==True:\n",
    "                                if cc<5:\n",
    "                                    gh.append(hh1)\n",
    "                                    cc=cc+1\n",
    "                            else:\n",
    "                                if cc1<5:\n",
    "                                    gh1.append(hh1)\n",
    "                                    cc1=cc1+1\n",
    "                        shap_word_exp[hh]=gh1\n",
    "                        shap_rel_exp[hh]=gh\n",
    "                    #words accuracy\n",
    "                    wordacc={}\n",
    "                    for kk in shap_word_exp:\n",
    "                        c=0\n",
    "                        vx=0\n",
    "                        if kk in ann:\n",
    "                            for vc in shap_word_exp[kk]:\n",
    "                                if vc in ann[kk]:\n",
    "                                    if vx<5:\n",
    "                                        c=c+1\n",
    "                                        vx=vx+1\n",
    "                            try:\n",
    "                                gg=c/float(len(shap_word_exp[kk]))\n",
    "                                if gg>0:\n",
    "                                        wordacc[kk]=gg\n",
    "                            except:\n",
    "                                continue \n",
    "                    #average word exp accuracy\n",
    "\n",
    "                    svv=0\n",
    "                    for t in wordacc:\n",
    "                            svv=svv+float(wordacc[t])\n",
    "                    print(\"Word_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "                    print(svv/float(len(wordacc)))\n",
    "\n",
    "                    #relational explanation accuracy\n",
    "                    relacc={}\n",
    "\n",
    "                    for kk in shap_rel_exp:\n",
    "                        c=0\n",
    "                        cn=0\n",
    "                        if kk in similar_r_map and kk in ann:\n",
    "                            for vc in shap_rel_exp[kk]:\n",
    "                                if vc in similar_r_map[kk] or int(vc) in similar_r_map[kk] or str(vc) in similar_r_map[kk]:\n",
    "                                    if cn<5:\n",
    "                                        c=c+1\n",
    "                                        cn=cn+1\n",
    "                            try:\n",
    "                                    gg=c/float(len(shap_rel_exp[kk]))\n",
    "                            except:\n",
    "                                continue \n",
    "                            if gg>0:\n",
    "                                relacc[kk]=gg\n",
    "                    #average relational exp accuracy\n",
    "                    svv=0\n",
    "                    for t in relacc:\n",
    "                            svv=svv+float(relacc[t])\n",
    "                    print(\"\\n\")\n",
    "                    print(\"Relational_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "                    print(svv/float(len(relacc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656 656\n",
      "1323 1323\n",
      "667 667\n",
      "667\n",
      "667\n",
      "667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Explanation_Average_Accuracy_Shap\n",
      "\n",
      "0.5289999999999998\n",
      "\n",
      "\n",
      "Relational_Explanation_Average_Accuracy_Shap\n",
      "\n",
      "0.7425742574257421\n"
     ]
    }
   ],
   "source": [
    "tweet_covid_relational_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "covid_19_exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
