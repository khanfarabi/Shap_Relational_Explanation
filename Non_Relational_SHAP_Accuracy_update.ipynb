{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews=58112\n",
      "500 0 500 1000\n",
      "1000 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5868973561430794\n",
      "5469 25720\n",
      "Shap Accuracy without human-feedback\n",
      "\n",
      "Shap Accuracy\n",
      "0.3006432748538001\n",
      "Shap only positive reviews\n",
      "Shap Accuracy for positive reviews\n",
      "0.3297273526824995\n",
      "Shap only negative reviews\n",
      "Shap Accuracy for negative reviews\n",
      "0.24293193717277517\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import transformers\n",
    "import shap\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import catboost\n",
    "from catboost import *\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xg \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xg \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statistics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from sklearn import tree\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import transformers\n",
    "import shap\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import catboost\n",
    "from catboost import *\n",
    "\n",
    "    \n",
    "\n",
    "def data_processing():\n",
    "                        #Pre-process the Reviews /Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/\n",
    "\n",
    "                        ifile1 = open(\"full-meta-data.txt\")\n",
    "                        revid = 0\n",
    "                        users = defaultdict(list)\n",
    "                        for ln in ifile1:\n",
    "                            parts = ln.strip().split(\"\\t\")\n",
    "                            users[parts[0]].append(revid)\n",
    "                            revid = revid + 1\n",
    "                        ifile1.close()\n",
    "                        #print(users)\n",
    "\n",
    "\n",
    "                        H11=defaultdict(list)\n",
    "                        #sys.exit()\n",
    "                        userids = []\n",
    "                        #print(users)\n",
    "                        c =0\n",
    "                        #Select reviewer subset based on tunable parameters (max-reviews and min-reviews limit,sampling ratio)\n",
    "                        minreviews =5\n",
    "                        maxreviews =30\n",
    "                        samplingratio =0.65\n",
    "                        for u in users:\n",
    "                            if len(users[u])>minreviews and len(users[u])<maxreviews:        \n",
    "                                if random.random() < samplingratio:\n",
    "                                    userids.append(u)\n",
    "                                    c= c + len(users[u])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        ifile = open(\"reviewContent.txt\",encoding='ISO-8859-1')\n",
    "                        ifile1 = open(\"full-meta-data.txt\")\n",
    "                        s_words = []\n",
    "                        sfile = open(\"Words.txt\")\n",
    "                        for ln in sfile:\n",
    "                            s_words.append(ln.strip())\n",
    "                        sfile.close()\n",
    "                        stopwords = []\n",
    "                        sfile = open(\"stopwords.txt\")\n",
    "                        for ln in sfile:\n",
    "                            stopwords.append(ln.strip())\n",
    "                        sfile.close()\n",
    "\n",
    "                        flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n",
    "                                 else 0)\n",
    "                        ofile = open(\"all_revs1.txt\",'w')\n",
    "                        ofile1 = open(\"metadata.txt\",'w')\n",
    "                        cnt = 0\n",
    "                        revid = 0\n",
    "                        qrat={}\n",
    "                        windex = defaultdict(list)\n",
    "                        #Tunable parameter to keep non-sentiment words\n",
    "                        PNonSentWords = 0.50\n",
    "                        WORDS={}\n",
    "                        w_per_ht=defaultdict(list)\n",
    "                        w_per_user=defaultdict(list)\n",
    "                        Rev_text_map={}\n",
    "                        for ln in ifile:\n",
    "                            ln1 = ifile1.readline()\n",
    "                            parts1 = ln1.strip().split(\"\\t\")\n",
    "                            #print(parts1)\n",
    "                            if parts1[0] not in userids:\n",
    "                                continue\n",
    "                            #if cnt >= 10000:\n",
    "                            #    break\n",
    "                            keep = []\n",
    "                            parts = ln.strip().split(\"\\t\")\n",
    "                            for word in re.findall(r\"\\w[\\w']*\", parts[3], flags=flags):\n",
    "                                if word.isdigit() or len(word)==1:\n",
    "                                    continue\n",
    "                                word_lower = word.lower()\n",
    "                                if word_lower in stopwords:\n",
    "                                    continue\n",
    "                               # if word_lower in s_words:\n",
    "                                    #keep.append(word_lower)\n",
    "                                elif random.random() < PNonSentWords:\n",
    "                                    if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                        keep.append(word_lower)\n",
    "                            if float(parts1[2])<=2:\n",
    "                                cl = 0\n",
    "                            elif float(parts1[2])==3:\n",
    "                                cl = 1\n",
    "                            elif float(parts1[2])>=4:\n",
    "                                cl = 2\n",
    "                            if len(keep)>=25:\n",
    "                                cnt = cnt + 1\n",
    "                                ofile.write(\" \".join(keep)+\"\\t\"+str(cl)+\"\\n\")\n",
    "                                WORDS[revid]=keep\n",
    "\n",
    "                                qrat[revid]=cl\n",
    "                                Rev_text_map[revid]=parts[3]\n",
    "                                H11[parts1[1]].append(revid)\n",
    "                                ofile1.write(ln1)\n",
    "                                for w in keep:\n",
    "                                    windex[w].append(revid)\n",
    "                                    w_per_ht[w].append(parts1[1])\n",
    "                                    w_per_user[w].append(parts1[0])\n",
    "                                revid = revid + 1\n",
    "                        ofile.close()\n",
    "                        ofile1.close()\n",
    "                        ifile.close()\n",
    "                        ifile1.close()\n",
    "                        '''\n",
    "                        #Tunable parameter (keep words only if repeated in > NumReps reviews)\n",
    "                        NumReps = 10\n",
    "\n",
    "                        #Filter review words\n",
    "                        ifile = open(\"all_revs1.txt\",encoding=\"ISO-8859-1\")\n",
    "                        ofile = open(\"processed_revs_1.txt\",'w')\n",
    "                        for ln in ifile:\n",
    "                            parts = ln.strip().split(\"\\t\")\n",
    "                            keep = []\n",
    "                            for w in parts[0].split(\" \"):\n",
    "                                if len(windex[w])>NumReps:\n",
    "                                    keep.append(w)\n",
    "                            ofile.write(\" \".join(keep)+\"\\t\"+parts[1]+\"\\n\")\n",
    "                        ofile.close()\n",
    "                        ifile.close()\n",
    "                        '''\n",
    "                        print(\"Total Reviews=\"+str(len(WORDS)))\n",
    "                        return s_words,stopwords,WORDS,qrat,H11,Rev_text_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Shap_Explanation_Engine(s_words,stopwords,WORDS,qrat,H11,Rev_text_map):\n",
    "\n",
    "                                                    #Balancing Review Data for both positive and negative\n",
    "                                                    WORDSt={}\n",
    "                                                    t1=[]\n",
    "                                                    t2=[]\n",
    "                                                    t3=[]\n",
    "                                                    c1=0\n",
    "                                                    c2=0\n",
    "                                                    c3=0\n",
    "                                                    for y in WORDS:\n",
    "                                                        if qrat[y]==0:\n",
    "                                                            if c3<500:\n",
    "                                                                t1.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c3=c3+1\n",
    "                                                        elif qrat[y]==1:\n",
    "                                                            continue\n",
    "                                                            #if c1<102:\n",
    "                                                                #t2.append(y)\n",
    "                                                                #WORDS1[y]=WORDS[y]\n",
    "                                                                #c1=c1+1\n",
    "                                                        elif qrat[y]==2:\n",
    "                                                            if c2<500:\n",
    "                                                                t3.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c2=c2+1\n",
    "                                                    print(len(t1),len(t2),len(t3),len(WORDSt))\n",
    "                                                    for k in WORDSt:\n",
    "                                                        if qrat[k]==1:\n",
    "                                                            print(k)\n",
    "                                                    d_tt={}\n",
    "                                                    d_tt[0]='negative'\n",
    "                                                    d_tt[2]='positive'\n",
    "                                                    WORDS_u={}\n",
    "                                                    aw={}\n",
    "                                                    c0=0\n",
    "                                                    c1=0\n",
    "                                                    K=500\n",
    "                                                    for t in WORDSt:\n",
    "                                                        if qrat[t]==0:\n",
    "                                                            if c0<K:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c0=c0+1\n",
    "                                                        elif qrat[t]==2:\n",
    "                                                            if c1<K:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c1=c1+1\n",
    "                                                    print(len(WORDS_u),len(aw))\n",
    "                                                    #annotation\n",
    "                                                    WORDS_uf={}\n",
    "                                                    ann={}\n",
    "                                                    kk=1.0\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        m=0\n",
    "                                                        gg=[]\n",
    "                                                        for cvv in WORDS_u[t]:\n",
    "                                                            if cvv not in gg:\n",
    "                                                                #if m/float(len(WORDS_u[t]))<=kk:\n",
    "                                                                    gg.append(cvv)\n",
    "\n",
    "                                                        WORDS_uf[t]=gg\n",
    "                                                       # print(len(gg),len(WORDS_u[t])         \n",
    "\n",
    "                                                    for k1 in WORDS_uf:\n",
    "                                                        vcc=[]\n",
    "                                                        c=0\n",
    "                                                        for t2 in WORDS_uf[k1]:\n",
    "                                                            if t2 in s_words:\n",
    "                                                                #print(t2)\n",
    "                                                                if t2 not in vcc:\n",
    "                                                                    #if c<500:\n",
    "                                                                        vcc.append(t2)\n",
    "                                                                        c=c+1\n",
    "                                                        if len(vcc)>0:\n",
    "                                                            ann[k1]=vcc\n",
    "                                                    #annotted word features\n",
    "                                                    wf2=[]\n",
    "                                                    for t in ann:\n",
    "                                                        for j in ann[t]:\n",
    "                                                            if j not in wf2:\n",
    "                                                                wf2.append(j)\n",
    "\n",
    "                                                    #train and target data\n",
    "\n",
    "                                                    train_r=[]\n",
    "                                                    targets_r=[]\n",
    "                                                    m_tid_tr1={}\n",
    "                                                    wr=[]\n",
    "                                                    tr=[]\n",
    "                                                    wr1=[]\n",
    "                                                    tr1=[]\n",
    "                                                    c=0\n",
    "                                                    c1=0\n",
    "                                                    tw_wm={}\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        s=''\n",
    "                                                        vb=[]\n",
    "                                                        #if twit_count[t]==1:\n",
    "                                                        for tt in WORDS_u[t]:\n",
    "                                                                s=s+str(tt)+\" \"\n",
    "                                                                train_r.append(tt)\n",
    "                                                                targets_r.append(qrat[t])\n",
    "                                                        vb.append(s)\n",
    "                                                        wr.append(s)\n",
    "                                                        wr1.append(vb)\n",
    "                                                        tr.append(qrat[t])\n",
    "                                                        tw_wm[t]=s\n",
    "                                                    unique_words=[]\n",
    "                                                    ss=set( train_r)\n",
    "                                                    for w1 in ss:\n",
    "                                                            if w1 not in unique_words:\n",
    "                                                                unique_words.append(w1)\n",
    "                                                    #Shap\n",
    "\n",
    "                                                    #shap.initjs()\n",
    "                                                    # Kernal Shap words_train targets\n",
    "\n",
    "\n",
    "                                                    corpus_train, corpus_test, y_train, y_test = train_test_split(train_r,targets_r, test_size=0.5, random_state=7)\n",
    "                                                    vectorizer = TfidfVectorizer(min_df=1)\n",
    "                                                    X_train = vectorizer.fit_transform(corpus_train)\n",
    "                                                    X_test = vectorizer.transform(corpus_test)\n",
    "                                                    model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                                    #KNeighborsClassifier(n_neighbors=5)\n",
    "                                                    #RandomForestClassifier(max_depth=2, random_state=1)\n",
    "                                                    #svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                                    model.fit(X_train,y_train)\n",
    "                                                    p = model.predict(X_test)\n",
    "                                                    prr={}\n",
    "                                                    for jj in range(0,len(corpus_test)):\n",
    "                                                        prr[corpus_test[jj]]=int(p[jj])\n",
    "                                                    print(f1_score(y_test,p,average='micro'))\n",
    "                                                    explainer =shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "                                                    shap_values = explainer.shap_values(X_test)\n",
    "                                                    X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                                                    feature_names=vectorizer.get_feature_names()\n",
    "                                                    print(len(feature_names),len(shap_values))\n",
    "                                                    #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                                                    shape_w={}\n",
    "                                                    fr={}\n",
    "                                                    feature_sh_v=[]\n",
    "                                                    for jj in range(0,len(corpus_train)):\n",
    "                                                          if abs(sum(shap_values[jj]))>0.4:\n",
    "                                                                                  m=abs(sum(shap_values[jj]))\n",
    "                                                                                  if corpus_train[jj] not in fr:\n",
    "                                                                                                  fr[corpus_train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                                                  elif corpus_train[jj]  in fr:\n",
    "                                                                                        if m>fr[corpus_train[jj]]:\n",
    "                                                                                            fr[corpus_train[jj]]=m\n",
    "                                                    dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                    for tt in dd1:\n",
    "                                                           feature_sh_v.append(tt[0])\n",
    "                                                    '''\n",
    "                                                    feature_sh_v1=[]\n",
    "                                                    for v in feature_sh_v:\n",
    "                                                        n=v.split()\n",
    "                                                        for k in n:\n",
    "                                                            if k not in feature_sh_v1:\n",
    "                                                                feature_sh_v1.append(k)\n",
    "                                                    '''\n",
    "                                                    #shap explanations\n",
    "\n",
    "                                                    shap_exp={}\n",
    "                                                    for t in WORDSt:\n",
    "                                                        gh=[]\n",
    "                                                        c=0\n",
    "                                                        for k in WORDSt[t]:\n",
    "                                                            if k in prr:\n",
    "                                                                if qrat[t]==prr[k]:\n",
    "                                                                    if k in feature_sh_v:\n",
    "                                                                        if k not in gh:\n",
    "                                                                            if c<5:\n",
    "                                                                                gh.append(k)\n",
    "                                                                                c=c+1\n",
    "                                                        if len(gh)>0:\n",
    "                                                                shap_exp[t]=gh\n",
    "\n",
    "                                                    for tt in shap_exp:\n",
    "                                                         pass#print(tt,shap_exp[tt])\n",
    "                                                    shap_all={}\n",
    "                                                    shap_all_p={}\n",
    "                                                    shap_all_n={}\n",
    "\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            c=0\n",
    "                                                            vb=0\n",
    "                                                            for zz in shap_exp[t]:\n",
    "                                                                if zz in ann[t]:\n",
    "                                                                    if vb<5:\n",
    "                                                                            c=c+1\n",
    "                                                                            vb=vb+1\n",
    "                                                            if len(shap_exp[t])>0:\n",
    "                                                                s=float(c)/len(shap_exp[t])\n",
    "                                                                if s>0:\n",
    "                                                                    shap_all[t]=s\n",
    "                                                    ss=0\n",
    "                                                    for k in shap_all:\n",
    "                                                        ss=ss+float(shap_all[k])\n",
    "\n",
    "                                                    acc=ss/len(shap_all)\n",
    "                                                    print(\"Shap Accuracy without human-feedback\"+\"\\n\")\n",
    "                                                    print(\"Shap Accuracy\")\n",
    "                                                    print(acc)\n",
    "                                                    print(\"Shap only positive reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==2:\n",
    "                                                                        c=0\n",
    "                                                                        vb1=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann[t]:\n",
    "                                                                                if vb1<5:\n",
    "                                                                                    c=c+1\n",
    "                                                                                    vb1=vb1+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_p[t]=s\n",
    "                                                    ss1=0\n",
    "                                                    for k in shap_all_p:\n",
    "                                                        ss1=ss1+float(shap_all_p[k])\n",
    "\n",
    "                                                    acc_p=ss1/len(shap_all_p)\n",
    "                                                    print(\"Shap Accuracy for positive reviews\")\n",
    "                                                    print(acc_p)\n",
    "                                                    print(\"Shap only negative reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==0:\n",
    "                                                                        c=0\n",
    "                                                                        vb2=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann[t]:\n",
    "                                                                                if vb2<5:\n",
    "                                                                                    c=c+1\n",
    "                                                                                    vb2=vb2+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_n[t]=s\n",
    "                                                    ss2=0\n",
    "                                                    for k in shap_all_n:\n",
    "                                                        ss2=ss2+float(shap_all_n[k])\n",
    "\n",
    "                                                    acc_n=ss2/len(shap_all_n)\n",
    "                                                    print(\"Shap Accuracy for negative reviews\")\n",
    "                                                    print(acc_n)\n",
    "                                                    return acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train, corpus_test, y_train, y_test,p,qrat,WORDSt\n",
    "                                    \n",
    "\n",
    "\n",
    "s_words,stopwords,WORDS,qrat,H11,Rev_text_map=data_processing()\n",
    "acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train,corpus_test,y_train,y_test,p,qrat,WORDSt=Shap_Explanation_Engine(s_words,stopwords,WORDS,qrat,H11,Rev_text_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def cluster_feed(KK,qrat,WORDSt,shap_exp,Rev_text_map):\n",
    "            #Sentance generation\n",
    "            sent=[]\n",
    "            sent1=[]\n",
    "            sent_map=defaultdict(list)\n",
    "            for ty in WORDSt:\n",
    "                gh=[]\n",
    "                gh.append(str(ty))\n",
    "                #gh1=[]\n",
    "                #gh2=[]\n",
    "                for j in WORDSt[ty]:\n",
    "\n",
    "                    j1=str(j)\n",
    "                    #gh.append(str(ty))\n",
    "                    if j1 not in gh:\n",
    "                        gh.append(j1)\n",
    "\n",
    "                    #print(gh)\n",
    "\n",
    "\n",
    "                if gh not in sent:\n",
    "                        sent.append(gh)\n",
    "\n",
    "\n",
    "            documents=[]\n",
    "            #documents1=[]\n",
    "            for t in sent:\n",
    "                for jh in t:\n",
    "                    documents.append(jh)\n",
    "\n",
    "            for w in sent:\n",
    "                pass#print(w)\n",
    "            #K-Means Run 14\n",
    "            #cluster generation with k-means\n",
    "            model = Word2Vec(sent, min_count=1)\n",
    "            X = model[model.wv.vocab]\n",
    "            NUM_CLUSTERS=KK\n",
    "            kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "            assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "            #print (assigned_clusters)\n",
    "            cluster={}\n",
    "            words = list(model.wv.vocab)\n",
    "            for i, word in enumerate(words):\n",
    "              gh=[] \n",
    "              gh1=[] \n",
    "              gh2=[] \n",
    "              if word.isdigit(): \n",
    "                cluster[word]=assigned_clusters1[i]\n",
    "                #print (word + \":\" + str(assigned_clusters[i]))\n",
    "            cluster_final={}\n",
    "            for j in range(NUM_CLUSTERS):\n",
    "                gg=[]\n",
    "                for tt in cluster:\n",
    "                    if int(cluster[tt])==int(j):\n",
    "                        if tt not in gg:\n",
    "                            gg.append(tt)\n",
    "                if len(gg)>0:\n",
    "                            cluster_final[j]=gg\n",
    "            cc=0\n",
    "            final_clu={}\n",
    "            lmm=(KK*2)+5\n",
    "            for t in cluster_final:\n",
    "                ghh=[]\n",
    "                vx=0\n",
    "                for k in cluster_final[t]:\n",
    "                    if int(k) in WORDS and int(k) in shap_exp or str(k) in shap_exp:\n",
    "                        if vx<lmm:\n",
    "                                ghh.append(int(k))\n",
    "                                vx=vx+1\n",
    "                if len(ghh)>=2:\n",
    "                        final_clu[cc]=ghh\n",
    "                        cc=cc+1\n",
    "            for k in final_clu:\n",
    "                pass#print(k,final_clu[k],len(final_clu[k]))\n",
    "            return final_clu\n",
    "           \n",
    "\n",
    "        \n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "cl={}\n",
    "for kk in range(15,56,10):\n",
    "        final_clu=cluster_feed(kk,qrat,WORDSt,shap_exp,Rev_text_map)\n",
    "        cl[kk]=final_clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_form(KK,WORDSt,qrat,final_clu,shap_exp):\n",
    "        hd=['Review_ID','Truly_Predicted_Class','Original_Text','Explanation','Label']\n",
    "        with open('feedback_shap_'+str(KK)+'.csv', 'w',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(hd)\n",
    "                for k in final_clu:\n",
    "                        #c=-1\n",
    "                        #c=c+1\n",
    "                        ghh=[]\n",
    "                        ghh1=[]\n",
    "                        c=0\n",
    "                        c1=0\n",
    "                        k111=0\n",
    "                        k222=0\n",
    "                        try:\n",
    "                                for t in final_clu[k]:\n",
    "                                    if qrat[t]==2:\n",
    "                                        if c<2:\n",
    "                                            k111=int(t)\n",
    "                                            c=c+1\n",
    "                                    elif qrat[t]==0:\n",
    "                                        if c1<3:\n",
    "                                            k222=int(t)\n",
    "                                            pass#print(k,t)\n",
    "                                            c1=c1+1\n",
    "\n",
    "                                ghh.append(k111)\n",
    "                                ghh1.append(k222)\n",
    "                                if qrat[k111]==2:\n",
    "                                    ghh.append(\"Positive\")\n",
    "                                else:\n",
    "                                    ghh.append(\"Negative\")\n",
    "                                if qrat[k222]==2:\n",
    "                                    ghh1.append(\"Positive\")\n",
    "                                else:\n",
    "                                    ghh1.append(\"Negative\")\n",
    "                                s=''\n",
    "                                for xz1 in WORDSt[k111]:\n",
    "                                    s=s+xz1+\" \"\n",
    "                                ghh.append(s)\n",
    "                                s1=''\n",
    "                                for xz1 in WORDSt[k222]:\n",
    "                                    s1=s1+xz1+\" \"\n",
    "                                ghh1.append(s1)\n",
    "                                #Exp\n",
    "                                s2=''\n",
    "                                for xz1 in shap_exp[k111]:\n",
    "                                    s2=s2+xz1+\" \"\n",
    "                                ghh.append(s2)\n",
    "                                s3=''\n",
    "                                for xz1 in shap_exp[k222]:\n",
    "                                    s3=s3+xz1+\" \"\n",
    "                                ghh1.append(s3)\n",
    "                                writer.writerow(ghh)\n",
    "                                writer.writerow(ghh1)\n",
    "                        except:\n",
    "                                continue \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "for k in cl:\n",
    "    feedback_form(k,WORDSt,qrat,cl[k],shap_exp)\n",
    "\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "235\n",
      "0.576530612244898\n",
      "423 587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n",
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "167\n",
      "0.49760765550239233\n",
      "320 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n",
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "275\n",
      "0.5058139534883721\n",
      "473 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n",
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "93\n",
      "0.5064377682403434\n",
      "199 232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n",
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "226\n",
      "0.5008849557522124\n",
      "403 565\n",
      "15 0.7416666666666667 0.7467948717948718 0.7083333333333333\n",
      "\n",
      "\n",
      "\n",
      "25 0.746031746031746 0.788888888888889 0.6388888888888888\n",
      "\n",
      "\n",
      "\n",
      "35 0.7738095238095238 0.8333333333333334 0.625\n",
      "\n",
      "\n",
      "\n",
      "45 0.7727272727272727 0.8 0.75\n",
      "\n",
      "\n",
      "\n",
      "55 0.7777777777777778 0.7745098039215685 0.7857142857142857\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import shap\n",
    "def shapp_all_acc():\n",
    "                def feedback_accuracy(WORDS22,qrat,ann):\n",
    "                        train_r=[]\n",
    "                        targets_r=[]\n",
    "                        m_tid_tr1={}\n",
    "                        wr=[]\n",
    "                        tr=[]\n",
    "                        wr1=[]\n",
    "                        tr1=[]\n",
    "                        c=0\n",
    "                        c1=0\n",
    "                        tw_wm={}\n",
    "                        for t in WORDS22:\n",
    "                            s=''\n",
    "                            vb=[]\n",
    "                            #if twit_count[t]==1:\n",
    "                            for tt in WORDS22[t]:\n",
    "                                    s=s+str(tt)+\" \"\n",
    "                                    train_r.append(tt)\n",
    "                                    targets_r.append(qrat[t])\n",
    "                            vb.append(s)\n",
    "                            wr.append(s)\n",
    "                            wr1.append(vb)\n",
    "                            tr.append(qrat[t])\n",
    "                            tw_wm[t]=s\n",
    "                        unique_words=[]\n",
    "                        ss=set( train_r)\n",
    "                        for w1 in ss:\n",
    "                                if w1 not in unique_words:\n",
    "                                    unique_words.append(w1)\n",
    "                        #Shap\n",
    "\n",
    "                        corpus_train, corpus_test, y_train, y_test = train_test_split(train_r,targets_r, test_size=0.5, random_state=7)\n",
    "                        vectorizer = TfidfVectorizer(min_df=1)\n",
    "                        X_train = vectorizer.fit_transform(corpus_train)\n",
    "                        X_test = vectorizer.transform(corpus_test)\n",
    "                        model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                        #KNeighborsClassifier(n_neighbors=5)\n",
    "                        #RandomForestClassifier(max_depth=2, random_state=1)\n",
    "                        #svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                        model.fit(X_train,y_train)\n",
    "                        p = model.predict(X_test)\n",
    "                        prr={}\n",
    "                        for jj in range(0,len(corpus_test)):\n",
    "                            prr[corpus_test[jj]]=int(p[jj])\n",
    "                        print(f1_score(y_test,p,average='micro'))\n",
    "                        explainer =shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "                        shap_values = explainer.shap_values(X_train)\n",
    "                        X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                        feature_names=vectorizer.get_feature_names()\n",
    "                        print(len(feature_names),len(shap_values))\n",
    "                        #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                        shape_w={}\n",
    "                        fr={}\n",
    "                        feature_sh_v=[]\n",
    "                        for jj in range(0,len(corpus_train)):\n",
    "                              if abs(sum(shap_values[jj]))>0.4:\n",
    "                                                      m=abs(sum(shap_values[jj]))\n",
    "                                                      if corpus_train[jj] not in fr:\n",
    "                                                                      fr[corpus_train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                      elif corpus_train[jj]  in fr:\n",
    "                                                            if m>fr[corpus_train[jj]]:\n",
    "                                                                fr[corpus_train[jj]]=m\n",
    "                        dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                        for tt in dd1:\n",
    "                               feature_sh_v.append(tt[0])\n",
    "                        '''\n",
    "                        feature_sh_v1=[]\n",
    "                        for v in feature_sh_v:\n",
    "                            n=v.split()\n",
    "                            for k in n:\n",
    "                                if k not in feature_sh_v1:\n",
    "                                    feature_sh_v1.append(k)\n",
    "                        '''\n",
    "                        #shap explanations\n",
    "\n",
    "                        shap_exp={}\n",
    "                        for t in WORDS22:\n",
    "                            gh=[]\n",
    "                            c=0\n",
    "                            for k in WORDS22[t]:\n",
    "                                if k in prr:\n",
    "                                    if qrat[t]==prr[k]:\n",
    "                                        if k in feature_sh_v:\n",
    "                                            if k not in gh:\n",
    "                                                if c<5:\n",
    "                                                    gh.append(k)\n",
    "                                                    c=c+1\n",
    "                            if len(gh)>0:\n",
    "                                    shap_exp[t]=gh\n",
    "\n",
    "                        for tt in shap_exp:\n",
    "                             pass#print(tt,shap_exp[tt])\n",
    "                        shap_all={}\n",
    "                        shap_all_p={}\n",
    "                        shap_all_n={}\n",
    "\n",
    "                        for t in shap_exp:\n",
    "                            vc=0\n",
    "                            if t in ann:\n",
    "                                c=0\n",
    "                                for zz in shap_exp[t]:\n",
    "                                    if zz in ann[t]:\n",
    "                                        if vc<5:\n",
    "                                            c=c+1\n",
    "                                            vc=vc+1\n",
    "                                if len(shap_exp[t])>0:\n",
    "                                    s=float(c)/len(shap_exp[t])\n",
    "                                    if s>0:\n",
    "                                        shap_all[t]=s\n",
    "                        ss=0\n",
    "                        for k in shap_all:\n",
    "                            ss=ss+float(shap_all[k])\n",
    "\n",
    "                        acc=ss/len(shap_all)\n",
    "                        #print(\"Shap Accuracy without human-feedback\"+\"\\n\")\n",
    "                        #print(\"Shap Accuracy\")\n",
    "                        #print(acc)\n",
    "                        #print(\"Shap only positive reviews\")\n",
    "                        for t in shap_exp:\n",
    "                            if t in ann:\n",
    "                                if qrat[t]==2:\n",
    "                                            c=0\n",
    "                                            vc1=0\n",
    "                                            for zz in shap_exp[t]:\n",
    "                                                if zz in ann[t]:\n",
    "                                                    if vc1<5:\n",
    "                                                        c=c+1\n",
    "                                                        vc1=vc1+1\n",
    "                                            if len(shap_exp[t])>0:\n",
    "                                                s=float(c)/len(shap_exp[t])\n",
    "                                                if s>0:\n",
    "                                                    shap_all_p[t]=s\n",
    "                        ss1=0\n",
    "                        for k in shap_all_p:\n",
    "                            ss1=ss1+float(shap_all_p[k])\n",
    "                        if len(shap_all_p)>0:\n",
    "                            acc_p=ss1/len(shap_all_p)\n",
    "                        else:\n",
    "                            acc_p=0\n",
    "                        #print(\"Shap Accuracy for positive reviews\")\n",
    "                        #print(acc_p)\n",
    "                        #print(\"Shap only negative reviews\")\n",
    "                        for t in shap_exp:\n",
    "                            if t in ann:\n",
    "                                if qrat[t]==0:\n",
    "                                            c=0\n",
    "                                            vc2=0\n",
    "                                            for zz in shap_exp[t]:\n",
    "                                                if zz in ann[t]:\n",
    "                                                    if vc2<5:\n",
    "                                                        c=c+1\n",
    "                                                        vc2=vc2+1\n",
    "                                            if len(shap_exp[t])>0:\n",
    "                                                s=float(c)/len(shap_exp[t])\n",
    "                                                if s>0:\n",
    "                                                    shap_all_n[t]=s\n",
    "                        ss2=0\n",
    "                        for k in shap_all_n:\n",
    "                            ss2=ss2+float(shap_all_n[k])\n",
    "                        if len(shap_all_n)>0:\n",
    "                            acc_n=ss2/len(shap_all_n)\n",
    "                        else:\n",
    "                            acc_n=0\n",
    "                        #print(\"Shap Accuracy for negative reviews\")\n",
    "                        #print(acc_n)\n",
    "                        return acc,acc_p,acc_n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                def feed(mn,qrat,cl,w3):\n",
    "                                sent=[]\n",
    "                                sent1=[]\n",
    "                                w33={}\n",
    "                                cp=0\n",
    "                                cn=0\n",
    "                                for kkk in w3:\n",
    "                                    gvv=[]\n",
    "                                    if qrat[kkk]==2:\n",
    "                                        if cp<250:\n",
    "                                            w33[kkk]=w3[kkk]\n",
    "                                            cp=cp+1\n",
    "                                    elif qrat[kkk]==0:\n",
    "                                        if cn<250:\n",
    "                                            w33[kkk]=w3[kkk]\n",
    "                                            cn=cn+1\n",
    "                                            \n",
    "                                            \n",
    "                                sent_map=defaultdict(list)\n",
    "                                for ty in w3:\n",
    "                                    gh=[]\n",
    "                                    gh.append(str(ty))\n",
    "                                    #gh1=[]\n",
    "                                    #gh2=[]\n",
    "                                    for j in w3[ty]:\n",
    "\n",
    "                                        j1=str(j)\n",
    "                                        #gh.append(str(ty))\n",
    "                                        if j1 not in gh:\n",
    "                                            gh.append(j1)\n",
    "\n",
    "                                    if gh not in sent:\n",
    "                                            sent.append(gh)\n",
    "                                documents=[]\n",
    "                                #documents1=[]\n",
    "                                for t in sent:\n",
    "                                    for jh in t:\n",
    "                                        documents.append(jh)\n",
    "                                hh=\"feedback_shap_\"+str(mn)+\".csv\"\n",
    "                                ps={}\n",
    "                                ns={}\n",
    "                                vot={}\n",
    "                                vtt={}\n",
    "                                f1=pd.read_csv(hh)\n",
    "                                vot={}\n",
    "                                vtt={}\n",
    "                                ll=0\n",
    "                                for col in f1.columns:\n",
    "                                    if 'Label' in col:\n",
    "                                        ll=ll+1\n",
    "                                m=len(f1['Review_ID'])\n",
    "                                for t in range(0,m):\n",
    "                                    #print(f1['Review_ID'][t])\n",
    "                                    vtt[f1['Review_ID'][t]]=f1['Explanation'][t]\n",
    "                                    gh=[]\n",
    "                                    for vv in range(1,ll+1):\n",
    "                                                  vb1=\"Label\"+str(vv)\n",
    "                                                  #print(f1[vb1][t])\n",
    "                                                  gh.append(f1[vb1][t])\n",
    "                                    vot[f1['Review_ID'][t]]=gh\n",
    "\n",
    "\n",
    "                                for uy in vot:\n",
    "                                    cp=0\n",
    "                                    cn=0\n",
    "                                    for kk in vot[uy]:\n",
    "                                        if int(kk)==1:\n",
    "                                            cp=cp+1\n",
    "                                        else:\n",
    "                                            cn=cn+1\n",
    "                                    if cp>=cn:\n",
    "                                        ps[uy]=vtt[uy]\n",
    "                                    elif cp<cn:\n",
    "                                        ns[uy]=vtt[uy]\n",
    "                    \n",
    "                                cl1=cl[mn]\n",
    "\n",
    "                                WORDS23={}\n",
    "                                rm=[]\n",
    "                                for jj in ns:\n",
    "                                    for kj in ns[jj]:\n",
    "                                        rm.append(kj)\n",
    "\n",
    "                                model = Word2Vec(sent, min_count=1)\n",
    "                                rme={}\n",
    "                                for uu in ns:\n",
    "                                    for k in cl1:\n",
    "                                        if uu in cl1[k]:\n",
    "                                            for kk in cl1[k]:\n",
    "                                                gg=[]\n",
    "                                                zz=0\n",
    "                                                if kk in w33:\n",
    "                                                    for vv in w33[kk]:\n",
    "                                                        if vv in ns[uu]:\n",
    "                                                            continue\n",
    "                                                        else:\n",
    "                                                            if zz<5:\n",
    "                                                                gg.append(vv)\n",
    "                                                                zz=zz+1\n",
    "                                                    rme[kk]=gg\n",
    "\n",
    "\n",
    "                                for t in cl1:\n",
    "                                    for k in ps:\n",
    "                                        if k in cl1[t]:\n",
    "                                            for kk in cl1[t]:\n",
    "                                                chu1=[]\n",
    "                                                vb={}\n",
    "                                                for v in ps[k]:\n",
    "                                                    vb1={}\n",
    "                                                    if kk in w33:\n",
    "                                                        for v1 in w33[kk]:\n",
    "                                                            try:\n",
    "                                                                    gh1=model.similarity(v,v1)\n",
    "                                                                    if gh1>0.4:\n",
    "                                                                                  vb1[v1]=float(gh1)\n",
    "                                                            except:\n",
    "                                                                continue \n",
    "                                                        for jk in vb1:\n",
    "                                                                        if jk in vb:\n",
    "                                                                            if float(vb1[jk])>=float(vb[jk]):\n",
    "                                                                                #print(jk,vb1[jk],vb[jk])\n",
    "                                                                                vb[jk]=vb1[jk]\n",
    "                                                                        else:\n",
    "                                                                            vb[jk]=vb1[jk]\n",
    "\n",
    "                                                dd1=sorted(vb.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                cc=0\n",
    "                                                for kkk in dd1:\n",
    "                                                    if kkk[0] not in chu1:\n",
    "                                                        if cc<5:\n",
    "                                                                chu1.append(kkk[0])\n",
    "                                                                cc=cc+1\n",
    "                                                if len(chu1)>0 :\n",
    "                                                    if kk in w33:\n",
    "                                                        WORDS23[kk]=chu1 \n",
    "                                WORDS25={}\n",
    "                                for t in WORDS23:\n",
    "                                    cc=0\n",
    "                                    vcx=[]\n",
    "                                    if t not in rme:\n",
    "                                        WORDS25[t]=WORDS23[t]\n",
    "                                    elif t in rme:\n",
    "                                        vcc=WORDS23[t]+rme[t]\n",
    "                                        for zz in vcc:\n",
    "                                            if cc<5:\n",
    "                                                vcx.append(zz)\n",
    "                                                cc=cc+1\n",
    "                                        WORDS25[t]=vcx\n",
    "                                print(len(WORDS25))\n",
    "                                for cc in rme:\n",
    "                                    fg=[]\n",
    "                                    vc4=0\n",
    "                                    if cc not in WORDS25:\n",
    "                                        for bb in rme[cc]:\n",
    "                                            if vc4<5:\n",
    "                                                fg.append(bb)\n",
    "                                                vc4=vc4+1\n",
    "                                        WORDS25[cc]=fg\n",
    "                                print(len(WORDS25))\n",
    "\n",
    "                                return WORDS25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                all_a={}\n",
    "                on_p={}\n",
    "                on_n={}\n",
    "                for mn in range(15,56,10):     \n",
    "                        WORDS25=feed(mn,qrat,cl,WORDSt)\n",
    "                        acc,acc_p,acc_n=feedback_accuracy(WORDS25,qrat,ann)  \n",
    "                        all_a[mn]=acc\n",
    "                        on_p[mn]=acc_p\n",
    "                        on_n[mn]=acc_n\n",
    "\n",
    "\n",
    "                for tt in all_a:\n",
    "                    print(tt,all_a[tt],on_p[tt],on_n[tt])\n",
    "                    print(\"\\n\\n\")\n",
    "\n",
    "                    \n",
    "shapp_all_acc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open('standard_Result.csv', 'w',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                hd=['id','standard exp']\n",
    "                writer.writerow(hd)\n",
    "                for k in ann:\n",
    "                    gh=[]\n",
    "                    gh.append(k)\n",
    "                    s=''\n",
    "                    for bb in ann[k]:\n",
    "                        s=s+str(bb)+\" \"\n",
    "                    gh.append(s)\n",
    "                    writer.writerow(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Original_Evid.csv', 'w',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                hd=['rid','word_evid']\n",
    "                writer.writerow(hd)\n",
    "                for k in WORDSt:\n",
    "                    gh=[]\n",
    "                    gh.append(k)\n",
    "                    s=''\n",
    "                    for bb in WORDSt[k]:\n",
    "                        s=s+str(bb)+\" \"\n",
    "                    gh.append(s)\n",
    "                    writer.writerow(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_rating.csv', 'w',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                hd=['rid','rating']\n",
    "                writer.writerow(hd)\n",
    "                for k in qrat:\n",
    "                    gh=[]\n",
    "                    gh.append(k)\n",
    "                    #s=''\n",
    "                    #for bb in WORDSt[k]:\n",
    "                       # s=s+str(bb)+\" \"\n",
    "                    gh.append(qrat[k])\n",
    "                    writer.writerow(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cluster_25.csv', 'w',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                hd=['cid','queries']\n",
    "                writer.writerow(hd)\n",
    "                for k in cl[25]:\n",
    "                    gh=[]\n",
    "                    gh.append(k)\n",
    "                    s=''\n",
    "                    for bb in cl[25][k]:\n",
    "                        s=s+str(bb)+\" \"\n",
    "                    gh.append(s)\n",
    "                    writer.writerow(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
