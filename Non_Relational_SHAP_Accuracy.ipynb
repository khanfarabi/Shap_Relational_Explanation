{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import transformers\n",
    "import shap\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import catboost\n",
    "from catboost import *\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import xgboost as xg \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xg \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statistics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from sklearn import tree\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import transformers\n",
    "import shap\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import catboost\n",
    "from catboost import *\n",
    "\n",
    "\n",
    "def SHAP_NON_RELATIONAL_EXPLANATION():\n",
    "    \n",
    "\n",
    "\n",
    "                        def data_processing():\n",
    "                                                    #Pre-process the Reviews /Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/\n",
    "\n",
    "                                                    ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                    revid = 0\n",
    "                                                    users = defaultdict(list)\n",
    "                                                    for ln in ifile1:\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        users[parts[0]].append(revid)\n",
    "                                                        revid = revid + 1\n",
    "                                                    ifile1.close()\n",
    "                                                    #print(users)\n",
    "\n",
    "\n",
    "                                                    H11=defaultdict(list)\n",
    "                                                    #sys.exit()\n",
    "                                                    userids = []\n",
    "                                                    #print(users)\n",
    "                                                    c =0\n",
    "                                                    #Select reviewer subset based on tunable parameters (max-reviews and min-reviews limit,sampling ratio)\n",
    "                                                    minreviews =3\n",
    "                                                    maxreviews =30\n",
    "                                                    samplingratio =0.35\n",
    "                                                    for u in users:\n",
    "                                                        if len(users[u])>minreviews and len(users[u])<maxreviews:        \n",
    "                                                            if random.random() < samplingratio:\n",
    "                                                                userids.append(u)\n",
    "                                                                c= c + len(users[u])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                    ifile = open(\"reviewContent.txt\",encoding='ISO-8859-1')\n",
    "                                                    ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                    s_words = []\n",
    "                                                    sfile = open(\"Words.txt\")\n",
    "                                                    for ln in sfile:\n",
    "                                                        s_words.append(ln.strip())\n",
    "                                                    sfile.close()\n",
    "                                                    stopwords = []\n",
    "                                                    sfile = open(\"stopwords.txt\")\n",
    "                                                    for ln in sfile:\n",
    "                                                        stopwords.append(ln.strip())\n",
    "                                                    sfile.close()\n",
    "\n",
    "                                                    flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n",
    "                                                             else 0)\n",
    "                                                    ofile = open(\"all_revs1.txt\",'w')\n",
    "                                                    ofile1 = open(\"metadata.txt\",'w')\n",
    "                                                    cnt = 0\n",
    "                                                    revid = 0\n",
    "                                                    qrat={}\n",
    "                                                    windex = defaultdict(list)\n",
    "                                                    #Tunable parameter to keep non-sentiment words\n",
    "                                                    PNonSentWords = 0.30\n",
    "                                                    WORDS={}\n",
    "                                                    w_per_ht=defaultdict(list)\n",
    "                                                    w_per_user=defaultdict(list)\n",
    "                                                    Rev_text_map={}\n",
    "                                                    for ln in ifile:\n",
    "                                                        ln1 = ifile1.readline()\n",
    "                                                        parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                        #print(parts1)\n",
    "                                                        if parts1[0] not in userids:\n",
    "                                                            continue\n",
    "                                                        #if cnt >= 10000:\n",
    "                                                        #    break\n",
    "                                                        keep = []\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        for word in re.findall(r\"\\w[\\w']*\", parts[3], flags=flags):\n",
    "                                                            if word.isdigit() or len(word)==1:\n",
    "                                                                continue\n",
    "                                                            word_lower = word.lower()\n",
    "                                                            if word_lower in stopwords:\n",
    "                                                                continue\n",
    "                                                           # if word_lower in s_words:\n",
    "                                                                #keep.append(word_lower)\n",
    "                                                            elif random.random() < PNonSentWords:\n",
    "                                                                if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                                                    keep.append(word_lower)\n",
    "                                                        if float(parts1[2])<=2:\n",
    "                                                            cl = 0\n",
    "                                                        elif float(parts1[2])==3:\n",
    "                                                            cl = 1\n",
    "                                                        elif float(parts1[2])>=4:\n",
    "                                                            cl = 2\n",
    "                                                        if len(keep)>=10:\n",
    "                                                            cnt = cnt + 1\n",
    "                                                            ofile.write(\" \".join(keep)+\"\\t\"+str(cl)+\"\\n\")\n",
    "                                                            WORDS[revid]=keep\n",
    "\n",
    "                                                            qrat[revid]=cl\n",
    "                                                            Rev_text_map[revid]=parts[3]\n",
    "                                                            H11[parts1[1]].append(revid)\n",
    "                                                            ofile1.write(ln1)\n",
    "                                                            for w in keep:\n",
    "                                                                windex[w].append(revid)\n",
    "                                                                w_per_ht[w].append(parts1[1])\n",
    "                                                                w_per_user[w].append(parts1[0])\n",
    "                                                            revid = revid + 1\n",
    "                                                    ofile.close()\n",
    "                                                    ofile1.close()\n",
    "                                                    ifile.close()\n",
    "                                                    ifile1.close()\n",
    "                                                    '''\n",
    "                                                    #Tunable parameter (keep words only if repeated in > NumReps reviews)\n",
    "                                                    NumReps = 10\n",
    "\n",
    "                                                    #Filter review words\n",
    "                                                    ifile = open(\"all_revs1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                    ofile = open(\"processed_revs_1.txt\",'w')\n",
    "                                                    for ln in ifile:\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        keep = []\n",
    "                                                        for w in parts[0].split(\" \"):\n",
    "                                                            if len(windex[w])>NumReps:\n",
    "                                                                keep.append(w)\n",
    "                                                        ofile.write(\" \".join(keep)+\"\\t\"+parts[1]+\"\\n\")\n",
    "                                                    ofile.close()\n",
    "                                                    ifile.close()\n",
    "                                                    '''\n",
    "                                                    print(\"Total Reviews=\"+str(len(WORDS)))\n",
    "                                                    return s_words,stopwords,WORDS,qrat,H11,Rev_text_map\n",
    "                        def Shap_Explanation_Engine(KK,s_words,stopwords,WORDS,qrat,H11,Rev_text_map):\n",
    "\n",
    "                                                    #Balancing Review Data for both positive and negative\n",
    "                                                    WORDSt={}\n",
    "                                                    kkk=100\n",
    "                                                    t1=[]\n",
    "                                                    t2=[]\n",
    "                                                    t3=[]\n",
    "                                                    c1=0\n",
    "                                                    c2=0\n",
    "                                                    c3=0\n",
    "                                                    for y in WORDS:\n",
    "                                                        if qrat[y]==0:\n",
    "                                                            if c3<kkk:\n",
    "                                                                t1.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c3=c3+1\n",
    "                                                        elif qrat[y]==1:\n",
    "                                                            continue\n",
    "                                                            #if c1<102:\n",
    "                                                                #t2.append(y)\n",
    "                                                                #WORDS1[y]=WORDS[y]\n",
    "                                                                #c1=c1+1\n",
    "                                                        elif qrat[y]==2:\n",
    "                                                            if c2<kkk:\n",
    "                                                                t3.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c2=c2+1\n",
    "                                                  #  print(len(t1),len(t2),len(t3),len(WORDSt))\n",
    "                                                    for k in WORDSt:\n",
    "                                                        if qrat[k]==1:\n",
    "                                                            print(k)\n",
    "                                                    d_tt={}\n",
    "                                                    d_tt[0]='negative'\n",
    "                                                    d_tt[2]='positive'\n",
    "                                                    WORDS_u={}\n",
    "                                                    aw={}\n",
    "                                                    c0=0\n",
    "                                                    c1=0\n",
    "                                                    #K=250\n",
    "                                                    for t in WORDSt:\n",
    "                                                        if qrat[t]==0:\n",
    "                                                            if c0<kkk:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c0=c0+1\n",
    "                                                        elif qrat[t]==2:\n",
    "                                                            if c1<kkk:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c1=c1+1\n",
    "                                                   # print(len(WORDS_u),len(aw))\n",
    "                                                    #annotation\n",
    "                                                    WORDS_uf={}\n",
    "                                                    ann={}\n",
    "                                                    ann1={}\n",
    "                                                    kk=1.0\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        m=0\n",
    "                                                        gg=[]\n",
    "                                                        for cvv in WORDS_u[t]:\n",
    "                                                            if cvv not in gg:\n",
    "                                                                #if m/float(len(WORDS_u[t]))<=kk:\n",
    "                                                                    gg.append(cvv)\n",
    "                                                                   # m=m+1\n",
    "                                                        WORDS_uf[t]=gg\n",
    "                                                       # print(len(gg),len(WORDS_u[t])         \n",
    "\n",
    "                                                    for k1 in WORDS_uf:\n",
    "                                                        vcc=[]\n",
    "                                                        vx=[]\n",
    "                                                        c=0\n",
    "                                                        for t2 in WORDS_uf[k1]:\n",
    "                                                            if t2 in s_words:\n",
    "                                                                #print(t2)\n",
    "                                                                if t2 not in vcc:\n",
    "                                                                   # if c<500:\n",
    "                                                                    vcc.append(t2)\n",
    "                                                                    #if c<500:\n",
    "                                                                    vx.append(t2)\n",
    "                                                                    #c=c+1\n",
    "                                                        if len(vcc)>0:\n",
    "                                                            ann[k1]=vcc\n",
    "                                                        if len(vx)>0:\n",
    "                                                            ann1[k1]=vx\n",
    "                                                    #annotted word features\n",
    "                                                    wf2=[]\n",
    "                                                    for t in ann:\n",
    "                                                        for j in ann[t]:\n",
    "                                                            if j not in wf2:\n",
    "                                                                wf2.append(j)\n",
    "                                                    #Sentance generation\n",
    "                                                    sent=[]\n",
    "                                                    sent1=[]\n",
    "                                                    sent_map=defaultdict(list)\n",
    "                                                    for ty in WORDS_u:\n",
    "                                                        gh=[]\n",
    "                                                        gh.append(str(ty))\n",
    "                                                        #gh1=[]\n",
    "                                                        #gh2=[]\n",
    "                                                        for j in WORDS_u[ty]:\n",
    "\n",
    "                                                            j1=str(j)\n",
    "                                                            #gh.append(str(ty))\n",
    "                                                            if j1 not in gh:\n",
    "                                                                gh.append(j1)\n",
    "\n",
    "                                                            #print(gh)\n",
    "\n",
    "\n",
    "                                                        if gh not in sent:\n",
    "                                                                sent.append(gh)\n",
    "\n",
    "\n",
    "                                                    documents=[]\n",
    "                                                    #documents1=[]\n",
    "                                                    for t in sent:\n",
    "                                                        for jh in t:\n",
    "                                                            documents.append(jh)\n",
    "\n",
    "                                                    for w in sent:\n",
    "                                                        pass#print(w)\n",
    "                                                    #K-Means Run 14\n",
    "                                                    #cluster generation with k-means\n",
    "                                                    model = Word2Vec(sent, min_count=1)\n",
    "                                                    X = model[model.wv.vocab]\n",
    "                                                    NUM_CLUSTERS=KK\n",
    "                                                    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                                                    assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                                                    #print (assigned_clusters)\n",
    "                                                    cluster={}\n",
    "                                                    words = list(model.wv.vocab)\n",
    "                                                    for i, word in enumerate(words):\n",
    "                                                      gh=[] \n",
    "                                                      gh1=[] \n",
    "                                                      gh2=[] \n",
    "                                                      if word.isdigit(): \n",
    "                                                        cluster[word]=assigned_clusters1[i]\n",
    "                                                        #print (word + \":\" + str(assigned_clusters[i]))\n",
    "                                                    cluster_final={}\n",
    "                                                    for j in range(NUM_CLUSTERS):\n",
    "                                                        gg=[]\n",
    "                                                        for tt in cluster:\n",
    "                                                            if int(cluster[tt])==int(j):\n",
    "                                                                if tt not in gg:\n",
    "                                                                    gg.append(tt)\n",
    "                                                        if len(gg)>0:\n",
    "                                                                    cluster_final[j]=gg\n",
    "                                                    cc=0\n",
    "                                                    final_clu={}\n",
    "                                                    for t in cluster_final:\n",
    "                                                        ghh=[]\n",
    "                                                        for k in cluster_final[t]:\n",
    "                                                            if int(k) in WORDS:\n",
    "                                                                   ghh.append(int(k))\n",
    "                                                        if len(ghh)>=2:\n",
    "                                                                final_clu[cc]=ghh\n",
    "                                                                cc=cc+1\n",
    "                                                    for k in final_clu:\n",
    "                                                        pass#print(k,final_clu[k],len(final_clu[k]))\n",
    "                                                    # test data\n",
    "                                                    # test data\n",
    "                                                    # test data\n",
    "                                                    # TEST DATA\n",
    "                                                    WORDS_t={}\n",
    "                                                    c=0\n",
    "                                                    c1=0\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        if qrat[t]==0:\n",
    "                                                            if c<kkk//2:\n",
    "                                                                WORDS_t[t]=WORDS_u[t]\n",
    "                                                                c=c+1\n",
    "                                                        elif qrat[t]==2:\n",
    "                                                            if c1<kkk//2:\n",
    "                                                                WORDS_t[t]=WORDS_u[t]\n",
    "                                                                c1=c1+1\n",
    "                                                    #print(len(WORDS_t))\n",
    "                                                    # utilize cluster to provide impact on updating test data\n",
    "                                                    wr={}\n",
    "                                                    wrr=[]\n",
    "                                                    for k in final_clu:\n",
    "                                                            #c=-1\n",
    "                                                            #c=c+1\n",
    "\n",
    "                                                            md=int(len(final_clu[k])/2)\n",
    "                                                            c=0      \n",
    "                                                            #k1= final_clu[k][md+c]\n",
    "                                                            for k1 in final_clu[k]:\n",
    "                                                                #print(k1,md)        \n",
    "                                                                if k1 in ann:\n",
    "                                                                            for k3 in ann[k1]:\n",
    "                                                                                if k3 not in wrr:\n",
    "                                                                                    wrr.append(k3)\n",
    "                                                               # else:\n",
    "                                                                    #c=c+1\n",
    "                                                                   # continue \n",
    "\n",
    "\n",
    "                                                                #print(k,k1,md,d_tt[qrat[k1]],w)\n",
    "                                                                #wr[k1]=wrr\n",
    "                                                    #print(wrr)\n",
    "                                                    #Update Evidence Based on Survey input Update 2 dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=True)   \n",
    "                                                    #data_extract11 similar_r_map1 data_extract11\n",
    "                                                    model = Word2Vec(sent, min_count=1)\n",
    "                                                    data_g={}\n",
    "                                                    for t in WORDS_t:\n",
    "                                                        chu=[]\n",
    "                                                        #try:\n",
    "                                                        vb={}\n",
    "                                                        for v in wrr:\n",
    "                                                            vb1={}\n",
    "                                                            for v1 in WORDS_t[t]:\n",
    "                                                                    #print(v1,v)\n",
    "                                                                    gh1=model.similarity(v,v1)\n",
    "                                                                    if gh1>0.5:\n",
    "                                                                          vb1[v1]=float(gh1)\n",
    "                                                            for jk in vb1:\n",
    "                                                                if jk in vb:\n",
    "                                                                    if float(vb1[jk])>=float(vb[jk]):\n",
    "                                                                        #print(jk,vb1[jk],vb[jk])\n",
    "                                                                        vb[jk]=vb1[jk]\n",
    "                                                                else:\n",
    "                                                                    vb[jk]=vb1[jk]\n",
    "                                                        #print(t, vb)\n",
    "                                                        #print(\"\\n\")\n",
    "\n",
    "\n",
    "                                                        dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                        cc=0\n",
    "                                                        for kkk in dd:\n",
    "                                                            if kkk[0] not in chu:\n",
    "                                                                #if len(kkk[0])>2:#if cc<5:\n",
    "                                                                        chu.append(kkk[0])\n",
    "                                                                        cc=cc+1\n",
    "\n",
    "                                                        if len(chu)>0:\n",
    "                                                            data_g[t]=chu\n",
    "                                                    #survey checking\n",
    "\n",
    "                                                    print(len(WORDS_u))\n",
    "                                                    #Updating the Whole Evidence Based on Survey Input\n",
    "                                                    WORDS22={}\n",
    "                                                    for gg in WORDS_t:\n",
    "                                                        #if gg in data_extract12:\n",
    "                                                            #WORDS2[gg]=data_extract12[gg]\n",
    "                                                        if gg in data_g:\n",
    "                                                            if len(data_g[gg])>0:\n",
    "                                                                WORDS22[gg]=data_g[gg]\n",
    "                                                    #print(WORDS2['d_535'])\n",
    "\n",
    "\n",
    "                                                    #update main\n",
    "                                                    WORDS_utt={}\n",
    "\n",
    "                                                    for fg in WORDS_t:\n",
    "                                                        gh=[]\n",
    "                                                        c1=0\n",
    "                                                        for d in WORDS_t[fg]:\n",
    "                                                                #if c1<5:\n",
    "                                                                    gh.append(d)\n",
    "                                                                    c1=c1+1\n",
    "                                                        WORDS_utt[fg]=gh\n",
    "                                                   # print(len(WORDS22))     \n",
    "                                                    for t in WORDS22:\n",
    "                                                        pass#print(t,WORDS22[t])\n",
    "                                                    #make original feature size equal to the test to avoid bias\n",
    "                                                    WORDS_uu={}\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        gh=[]\n",
    "                                                        c=0\n",
    "                                                        for k in WORDS_u[t]:\n",
    "                                                            if len(k)>2:\n",
    "                                                                gh.append(k)\n",
    "                                                                #c=c+1\n",
    "                                                        WORDS_uu[t]=gh\n",
    "                                                    for g in WORDS_uu:\n",
    "                                                        pass#print(g,WORDS_uu[g])\n",
    "                                                    #train and target data\n",
    "\n",
    "                                                    train_r=[]\n",
    "                                                    targets_r=[]\n",
    "                                                    m_tid_tr1={}\n",
    "                                                    wr=[]\n",
    "                                                    tr=[]\n",
    "                                                    wr1=[]\n",
    "                                                    tr1=[]\n",
    "                                                    c=0\n",
    "                                                    c1=0\n",
    "                                                    tw_wm={}\n",
    "                                                    for t in WORDS22:\n",
    "                                                        s=''\n",
    "                                                        vb=[]\n",
    "                                                        #if twit_count[t]==1:\n",
    "                                                        for tt in WORDS22[t]:\n",
    "                                                                s=s+str(tt)+\" \"\n",
    "                                                                train_r.append(tt)\n",
    "                                                                targets_r.append(qrat[t])\n",
    "                                                        vb.append(s)\n",
    "                                                        wr.append(s)\n",
    "                                                        wr1.append(vb)\n",
    "                                                        tr.append(qrat[t])\n",
    "                                                        tw_wm[t]=s\n",
    "                                                    unique_words=[]\n",
    "                                                    ss=set( train_r)\n",
    "                                                    for w1 in ss:\n",
    "                                                            if w1 not in unique_words:\n",
    "                                                                unique_words.append(w1)\n",
    "                                                    #Shap\n",
    "\n",
    "                                                    #shap.initjs()\n",
    "                                                    # Kernal Shap words_train targets\n",
    "\n",
    "\n",
    "                                                    corpus_train, corpus_test, y_train, y_test = train_test_split(train_r,targets_r, test_size=0.5, random_state=7)\n",
    "                                                    vectorizer = TfidfVectorizer(min_df=1)\n",
    "                                                    X_train = vectorizer.fit_transform(train_r)\n",
    "                                                    X_test = vectorizer.transform(corpus_test)\n",
    "                                                    model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                                    model.fit(X_train,targets_r)\n",
    "                                                    p = model.predict(X_train)\n",
    "                                                    prr={}\n",
    "                                                    for jj in range(0,len(train_r)):\n",
    "                                                        prr[train_r[jj]]=int(p[jj])\n",
    "                                                    #print(f1_score(targets_r,p,average='micro'))\n",
    "                                                    explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "                                                    shap_values = explainer.shap_values(X_train)\n",
    "                                                    #X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                                                    feature_names=vectorizer.get_feature_names()\n",
    "                                                    #print(len(feature_names),len(shap_values))\n",
    "                                                    #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                                                    shape_w={}\n",
    "                                                    fr={}\n",
    "                                                    feature_sh_v=[]\n",
    "                                                    for jj in range(0,len(train_r)):\n",
    "                                                         if abs(sum(shap_values[jj]))>=0.2:\n",
    "                                                                                  m=abs(sum(shap_values[jj]))\n",
    "                                                                                  if train_r[jj] not in fr:\n",
    "                                                                                                  fr[train_r[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                                                  elif train_r[jj]  in fr:\n",
    "                                                                                        if m>fr[train_r[jj]]:\n",
    "                                                                                                fr[train_r[jj]]=m\n",
    "                                                    dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                    for tt in dd1:\n",
    "                                                           #feature_sh_v.append(tt[0])\n",
    "                                                             for vvv5 in ann:\n",
    "                                                                    if vvv5 in WORDS22:\n",
    "                                                                        if tt[0] in ann[vvv5] or str(tt[0]) in ann[vvv5] and tt[0] in WORDS22[vvv5] or str(tt[0]) in WORDS22[vvv5]:\n",
    "                                                                                       feature_sh_v.append(tt[0])\n",
    "                                                    '''\n",
    "                                                    feature_sh_v1=[]\n",
    "                                                    for v in feature_sh_v:\n",
    "                                                        n=v.split()\n",
    "                                                        for k in n:\n",
    "                                                            if k not in feature_sh_v1:\n",
    "                                                                feature_sh_v1.append(k)\n",
    "                                                    '''\n",
    "                                                    #shap explanations\n",
    "\n",
    "                                                    shap_exp={}\n",
    "                                                    for t in WORDS22:\n",
    "                                                        gh=[]\n",
    "                                                        c=0\n",
    "                                                        for k in WORDS22[t]:\n",
    "                                                            if k in prr:\n",
    "                                                                if qrat[t]==prr[k]:\n",
    "                                                                    if k in feature_sh_v:\n",
    "                                                                        if k not in gh:\n",
    "                                                                            if c<5:\n",
    "                                                                                gh.append(k)\n",
    "                                                                                c=c+1\n",
    "                                                        shap_exp[t]=gh\n",
    "\n",
    "                                                    for tt in shap_exp:\n",
    "                                                         pass#print(tt,shap_exp[tt])\n",
    "                                                    shap_all={}\n",
    "                                                    shap_all_p={}\n",
    "                                                    shap_all_n={}\n",
    "\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            c=0\n",
    "                                                            for zz in shap_exp[t]:\n",
    "                                                                if zz in ann1[t]:\n",
    "                                                                    c=c+1\n",
    "                                                            if len(shap_exp[t])>0:\n",
    "                                                                s=float(c)/len(shap_exp[t])\n",
    "                                                                if s>0:\n",
    "                                                                    shap_all[t]=s\n",
    "                                                    ss=0\n",
    "                                                    for k in shap_all:\n",
    "                                                        ss=ss+float(shap_all[k])\n",
    "\n",
    "                                                    acc=ss/len(shap_all)\n",
    "                                                    #print(\"Shap Accuracy\")\n",
    "                                                    #print(acc)\n",
    "                                                    #print(\"Shap only positive reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==2:\n",
    "                                                                        c=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann1[t]:\n",
    "                                                                                c=c+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_p[t]=s\n",
    "                                                    ss1=0\n",
    "                                                    for k in shap_all_p:\n",
    "                                                        ss1=ss1+float(shap_all_p[k])\n",
    "\n",
    "                                                    acc_p=ss1/len(shap_all_p)\n",
    "                                                    #print(\"Shap Accuracy for positive reviews\")\n",
    "                                                    #print(acc_p)\n",
    "                                                    #print(\"Shap only negative reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==0:\n",
    "                                                                        c=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann1[t]:\n",
    "                                                                                c=c+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_n[t]=s\n",
    "                                                    ss2=0\n",
    "                                                    for k in shap_all_n:\n",
    "                                                        ss2=ss2+float(shap_all_n[k])\n",
    "\n",
    "                                                    acc_n=ss2/len(shap_all_n)\n",
    "                                                    #print(\"Shap Accuracy for negative reviews\")\n",
    "                                                    #print(acc_n)\n",
    "                                                    return acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train, corpus_test, y_train, y_test,p,qrat,WORDS,WORDS22\n",
    "\n",
    "\n",
    "                        shap_accuracy_exp={}\n",
    "                        shap_accuracy_exp_p={}\n",
    "                        shap_accuracy_exp_n={}\n",
    "                        shap_weights={}\n",
    "                        shap_explanation={}\n",
    "                        standard_explanation={}\n",
    "                        per_cluster_train_data={}\n",
    "                        per_cluster_test_data={}\n",
    "                        per_cluster_train_target={}\n",
    "                        per_cluster_test_target={}\n",
    "                        per_cluster_predicted_class={}\n",
    "                        per_cluster_or_ev={}\n",
    "                        per_cluster_updated_ev={}\n",
    "\n",
    "                        s_words,stopwords,WORDS,qrat,H11,Rev_text_map=data_processing()\n",
    "                        WORDS_p={}\n",
    "                        WORDS_n={}\n",
    "                        for kk in WORDS:\n",
    "                            if qrat[kk]==2:\n",
    "                                WORDS_p[kk]=WORDS[kk]\n",
    "                            elif qrat[kk]==0:\n",
    "                                WORDS_n[kk]=WORDS[kk]\n",
    "\n",
    "\n",
    "                        def shap_opration_call(s_words,stopwords,WORDS,qrat,H11,Rev_text_map):\n",
    "                            for hh in range(5,76,15):\n",
    "                                acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train, corpus_test, y_train, y_test,p,qrat,WORDS,WORDS22=Shap_Explanation_Engine(hh,s_words,stopwords,WORDS,qrat,H11,Rev_text_map)\n",
    "                                shap_accuracy_exp[hh]=acc\n",
    "                                shap_accuracy_exp_p[hh]=acc_p\n",
    "                                shap_accuracy_exp_n[hh]=acc_n\n",
    "                                shap_weights[hh]=shap_values\n",
    "                                shap_explanation[hh]=shap_exp\n",
    "                                standard_explanation[hh]=ann\n",
    "                                per_cluster_train_data[hh]=corpus_train\n",
    "                                per_cluster_test_data[hh]=corpus_test\n",
    "                                per_cluster_train_target[hh]=y_train\n",
    "                                per_cluster_test_target[hh]=y_test\n",
    "                                per_cluster_predicted_class[hh]=p\n",
    "                                per_cluster_or_ev[hh]=WORDS\n",
    "                                per_cluster_updated_ev[hh]=WORDS22\n",
    "\n",
    "                            return shap_accuracy_exp,shap_accuracy_exp_p,shap_accuracy_exp_n,shap_weights,shap_explanation,standard_explanation,per_cluster_train_data,per_cluster_test_data,per_cluster_train_target,per_cluster_test_target,per_cluster_predicted_class,qrat,per_cluster_or_ev,per_cluster_updated_ev\n",
    "\n",
    "\n",
    "                        shap_accuracy_exp_u,shap_accuracy_exp_p_u,shap_accuracy_exp_n_u,shap_weights_u,shap_explanation_u,standard_explanation_u,per_cluster_train_data_u,per_cluster_test_data_u,per_cluster_train_target_u,per_cluster_test_target_u,per_cluster_predicted_class_u,qrat_u,per_cluster_or_ev_u,per_cluster_updated_ev_u=shap_opration_call(s_words,stopwords,WORDS,qrat,H11,Rev_text_map)\n",
    "                        #shap_accuracy_exp_p,shap_weights_p,shap_explanation_p,standard_explanation_p,per_cluster_train_data_p,per_cluster_test_data_p,per_cluster_train_target_p,per_cluster_test_target_p,per_cluster_predicted_class_p,qrat_p,per_cluster_or_ev_p,per_cluster_updated_ev_p=shap_opration_call(s_words,stopwords,WORDS_p,qrat,H11,Rev_text_map)\n",
    "                        #shap_accuracy_exp_n,shap_weights_n,shap_explanation_n,standard_explanation_n,per_cluster_train_data_n,per_cluster_test_data_n,per_cluster_train_target_n,per_cluster_test_target_n,per_cluster_predicted_class_n,qrat_n,per_cluster_or_ev_n,per_cluster_updated_ev_n=shap_opration_call(s_words,stopwords,WORDS_n,qrat,H11,Rev_text_map)\n",
    "\n",
    "                        print(\"Our Human-Feedback based Word Explanation Acciracy\"+\"\\n\")\n",
    "                        for v in shap_accuracy_exp_u:\n",
    "                            print(shap_accuracy_exp_u[v])\n",
    "                        print(\"\\n\\n\")\n",
    "                        print(\"Only Positive Reviews\")\n",
    "                        for v in shap_accuracy_exp_p_u:\n",
    "                            print(shap_accuracy_exp_p_u[v])\n",
    "                        print(\"\\n\\n\")\n",
    "                        print(\"Only Negative Reviews\")\n",
    "                        for v in shap_accuracy_exp_n_u:\n",
    "                            print(shap_accuracy_exp_n_u[v])\n",
    "\n",
    "\n",
    "                        #Without Feedback Shap explanation\n",
    "\n",
    "\n",
    "                        def data_processing():\n",
    "                                                    #Pre-process the Reviews /Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/\n",
    "\n",
    "                                                    ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                    revid = 0\n",
    "                                                    users = defaultdict(list)\n",
    "                                                    for ln in ifile1:\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        users[parts[0]].append(revid)\n",
    "                                                        revid = revid + 1\n",
    "                                                    ifile1.close()\n",
    "                                                    #print(users)\n",
    "\n",
    "\n",
    "                                                    H11=defaultdict(list)\n",
    "                                                    #sys.exit()\n",
    "                                                    userids = []\n",
    "                                                    #print(users)\n",
    "                                                    c =0\n",
    "                                                    #Select reviewer subset based on tunable parameters (max-reviews and min-reviews limit,sampling ratio)\n",
    "                                                    minreviews =3\n",
    "                                                    maxreviews =30\n",
    "                                                    samplingratio =0.35\n",
    "                                                    for u in users:\n",
    "                                                        if len(users[u])>minreviews and len(users[u])<maxreviews:        \n",
    "                                                            if random.random() < samplingratio:\n",
    "                                                                userids.append(u)\n",
    "                                                                c= c + len(users[u])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                    ifile = open(\"reviewContent.txt\",encoding='ISO-8859-1')\n",
    "                                                    ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                    s_words = []\n",
    "                                                    sfile = open(\"Words.txt\")\n",
    "                                                    for ln in sfile:\n",
    "                                                        s_words.append(ln.strip())\n",
    "                                                    sfile.close()\n",
    "                                                    stopwords = []\n",
    "                                                    sfile = open(\"stopwords.txt\")\n",
    "                                                    for ln in sfile:\n",
    "                                                        stopwords.append(ln.strip())\n",
    "                                                    sfile.close()\n",
    "\n",
    "                                                    flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n",
    "                                                             else 0)\n",
    "                                                    ofile = open(\"all_revs1.txt\",'w')\n",
    "                                                    ofile1 = open(\"metadata.txt\",'w')\n",
    "                                                    cnt = 0\n",
    "                                                    revid = 0\n",
    "                                                    qrat={}\n",
    "                                                    windex = defaultdict(list)\n",
    "                                                    #Tunable parameter to keep non-sentiment words\n",
    "                                                    PNonSentWords = 0.30\n",
    "                                                    WORDS={}\n",
    "                                                    w_per_ht=defaultdict(list)\n",
    "                                                    w_per_user=defaultdict(list)\n",
    "                                                    Rev_text_map={}\n",
    "                                                    for ln in ifile:\n",
    "                                                        ln1 = ifile1.readline()\n",
    "                                                        parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                        #print(parts1)\n",
    "                                                        if parts1[0] not in userids:\n",
    "                                                            continue\n",
    "                                                        #if cnt >= 10000:\n",
    "                                                        #    break\n",
    "                                                        keep = []\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        for word in re.findall(r\"\\w[\\w']*\", parts[3], flags=flags):\n",
    "                                                            if word.isdigit() or len(word)==1:\n",
    "                                                                continue\n",
    "                                                            word_lower = word.lower()\n",
    "                                                            if word_lower in stopwords:\n",
    "                                                                continue\n",
    "                                                           # if word_lower in s_words:\n",
    "                                                                #keep.append(word_lower)\n",
    "                                                            elif random.random() < PNonSentWords:\n",
    "                                                                if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                                                    keep.append(word_lower)\n",
    "                                                        if float(parts1[2])<=2:\n",
    "                                                            cl = 0\n",
    "                                                        elif float(parts1[2])==3:\n",
    "                                                            cl = 1\n",
    "                                                        elif float(parts1[2])>=4:\n",
    "                                                            cl = 2\n",
    "                                                        if len(keep)>=10:\n",
    "                                                            cnt = cnt + 1\n",
    "                                                            ofile.write(\" \".join(keep)+\"\\t\"+str(cl)+\"\\n\")\n",
    "                                                            WORDS[revid]=keep\n",
    "\n",
    "                                                            qrat[revid]=cl\n",
    "                                                            Rev_text_map[revid]=parts[3]\n",
    "                                                            H11[parts1[1]].append(revid)\n",
    "                                                            ofile1.write(ln1)\n",
    "                                                            for w in keep:\n",
    "                                                                windex[w].append(revid)\n",
    "                                                                w_per_ht[w].append(parts1[1])\n",
    "                                                                w_per_user[w].append(parts1[0])\n",
    "                                                            revid = revid + 1\n",
    "                                                    ofile.close()\n",
    "                                                    ofile1.close()\n",
    "                                                    ifile.close()\n",
    "                                                    ifile1.close()\n",
    "                                                    '''\n",
    "                                                    #Tunable parameter (keep words only if repeated in > NumReps reviews)\n",
    "                                                    NumReps = 10\n",
    "\n",
    "                                                    #Filter review words\n",
    "                                                    ifile = open(\"all_revs1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                    ofile = open(\"processed_revs_1.txt\",'w')\n",
    "                                                    for ln in ifile:\n",
    "                                                        parts = ln.strip().split(\"\\t\")\n",
    "                                                        keep = []\n",
    "                                                        for w in parts[0].split(\" \"):\n",
    "                                                            if len(windex[w])>NumReps:\n",
    "                                                                keep.append(w)\n",
    "                                                        ofile.write(\" \".join(keep)+\"\\t\"+parts[1]+\"\\n\")\n",
    "                                                    ofile.close()\n",
    "                                                    ifile.close()\n",
    "                                                    '''\n",
    "                                                    print(\"Total Reviews=\"+str(len(WORDS)))\n",
    "                                                    return s_words,stopwords,WORDS,qrat,H11,Rev_text_map\n",
    "                        def Shap_Explanation_Engine(s_words,stopwords,WORDS,qrat,H11,Rev_text_map):\n",
    "\n",
    "                                                    #Balancing Review Data for both positive and negative\n",
    "                                                    WORDSt={}\n",
    "                                                    t1=[]\n",
    "                                                    t2=[]\n",
    "                                                    t3=[]\n",
    "                                                    c1=0\n",
    "                                                    c2=0\n",
    "                                                    c3=0\n",
    "                                                    for y in WORDS:\n",
    "                                                        if qrat[y]==0:\n",
    "                                                            if c3<100:\n",
    "                                                                t1.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c3=c3+1\n",
    "                                                        elif qrat[y]==1:\n",
    "                                                            continue\n",
    "                                                            #if c1<102:\n",
    "                                                                #t2.append(y)\n",
    "                                                                #WORDS1[y]=WORDS[y]\n",
    "                                                                #c1=c1+1\n",
    "                                                        elif qrat[y]==2:\n",
    "                                                            if c2<100:\n",
    "                                                                t3.append(y)\n",
    "                                                                WORDSt[y]=WORDS[y]\n",
    "                                                                c2=c2+1\n",
    "                                                    print(len(t1),len(t2),len(t3),len(WORDSt))\n",
    "                                                    for k in WORDSt:\n",
    "                                                        if qrat[k]==1:\n",
    "                                                            print(k)\n",
    "                                                    d_tt={}\n",
    "                                                    d_tt[0]='negative'\n",
    "                                                    d_tt[2]='positive'\n",
    "                                                    WORDS_u={}\n",
    "                                                    aw={}\n",
    "                                                    c0=0\n",
    "                                                    c1=0\n",
    "                                                    K=500\n",
    "                                                    for t in WORDSt:\n",
    "                                                        if qrat[t]==0:\n",
    "                                                            if c0<K:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c0=c0+1\n",
    "                                                        elif qrat[t]==2:\n",
    "                                                            if c1<K:\n",
    "                                                                WORDS_u[t]=WORDSt[t]\n",
    "                                                                aw[t]=WORDSt[t]\n",
    "                                                                c1=c1+1\n",
    "                                                    print(len(WORDS_u),len(aw))\n",
    "                                                    #annotation\n",
    "                                                    WORDS_uf={}\n",
    "                                                    ann={}\n",
    "                                                    kk=1.0\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        m=0\n",
    "                                                        gg=[]\n",
    "                                                        for cvv in WORDS_u[t]:\n",
    "                                                            if cvv not in gg:\n",
    "                                                                #if m/float(len(WORDS_u[t]))<=kk:\n",
    "                                                                    gg.append(cvv)\n",
    "\n",
    "                                                        WORDS_uf[t]=gg\n",
    "                                                       # print(len(gg),len(WORDS_u[t])         \n",
    "\n",
    "                                                    for k1 in WORDS_uf:\n",
    "                                                        vcc=[]\n",
    "                                                        c=0\n",
    "                                                        for t2 in WORDS_uf[k1]:\n",
    "                                                            if t2 in s_words:\n",
    "                                                                #print(t2)\n",
    "                                                                if t2 not in vcc:\n",
    "                                                                    #if c<500:\n",
    "                                                                        vcc.append(t2)\n",
    "                                                                        c=c+1\n",
    "                                                        if len(vcc)>0:\n",
    "                                                            ann[k1]=vcc\n",
    "                                                    #annotted word features\n",
    "                                                    wf2=[]\n",
    "                                                    for t in ann:\n",
    "                                                        for j in ann[t]:\n",
    "                                                            if j not in wf2:\n",
    "                                                                wf2.append(j)\n",
    "\n",
    "                                                    #train and target data\n",
    "\n",
    "                                                    train_r=[]\n",
    "                                                    targets_r=[]\n",
    "                                                    m_tid_tr1={}\n",
    "                                                    wr=[]\n",
    "                                                    tr=[]\n",
    "                                                    wr1=[]\n",
    "                                                    tr1=[]\n",
    "                                                    c=0\n",
    "                                                    c1=0\n",
    "                                                    tw_wm={}\n",
    "                                                    for t in WORDS_u:\n",
    "                                                        s=''\n",
    "                                                        vb=[]\n",
    "                                                        #if twit_count[t]==1:\n",
    "                                                        for tt in WORDS_u[t]:\n",
    "                                                                s=s+str(tt)+\" \"\n",
    "                                                                train_r.append(tt)\n",
    "                                                                targets_r.append(qrat[t])\n",
    "                                                        vb.append(s)\n",
    "                                                        wr.append(s)\n",
    "                                                        wr1.append(vb)\n",
    "                                                        tr.append(qrat[t])\n",
    "                                                        tw_wm[t]=s\n",
    "                                                    unique_words=[]\n",
    "                                                    ss=set( train_r)\n",
    "                                                    for w1 in ss:\n",
    "                                                            if w1 not in unique_words:\n",
    "                                                                unique_words.append(w1)\n",
    "                                                    #Shap\n",
    "\n",
    "                                                    #shap.initjs()\n",
    "                                                    # Kernal Shap words_train targets\n",
    "\n",
    "\n",
    "                                                    corpus_train, corpus_test, y_train, y_test = train_test_split(train_r,targets_r, test_size=0.5, random_state=7)\n",
    "                                                    vectorizer = TfidfVectorizer(min_df=1)\n",
    "                                                    X_train = vectorizer.fit_transform(corpus_train)\n",
    "                                                    X_test = vectorizer.transform(corpus_test)\n",
    "                                                    model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                                    #KNeighborsClassifier(n_neighbors=5)\n",
    "                                                    #RandomForestClassifier(max_depth=2, random_state=1)\n",
    "                                                    #svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                                    model.fit(X_train,y_train)\n",
    "                                                    p = model.predict(X_test)\n",
    "                                                    prr={}\n",
    "                                                    for jj in range(0,len(corpus_test)):\n",
    "                                                        prr[corpus_test[jj]]=int(p[jj])\n",
    "                                                    print(f1_score(y_test,p,average='micro'))\n",
    "                                                    explainer =shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "                                                    shap_values = explainer.shap_values(X_test)\n",
    "                                                    X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                                                    feature_names=vectorizer.get_feature_names()\n",
    "                                                    print(len(feature_names),len(shap_values))\n",
    "                                                    #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                                                    shape_w={}\n",
    "                                                    fr={}\n",
    "                                                    feature_sh_v=[]\n",
    "                                                    for jj in range(0,len(corpus_train)):\n",
    "                                                                          m=abs(sum(shap_values[jj]))\n",
    "                                                                          if corpus_train[jj] not in fr:\n",
    "                                                                                          fr[corpus_train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                                          elif corpus_train[jj]  in fr:\n",
    "                                                                                if m>fr[corpus_train[jj]]:\n",
    "                                                                                    fr[corpus_train[jj]]=m\n",
    "                                                    dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                    for tt in dd1:\n",
    "                                                           feature_sh_v.append(tt[0])\n",
    "                                                    '''\n",
    "                                                    feature_sh_v1=[]\n",
    "                                                    for v in feature_sh_v:\n",
    "                                                        n=v.split()\n",
    "                                                        for k in n:\n",
    "                                                            if k not in feature_sh_v1:\n",
    "                                                                feature_sh_v1.append(k)\n",
    "                                                    '''\n",
    "                                                    #shap explanations\n",
    "\n",
    "                                                    shap_exp={}\n",
    "                                                    for t in WORDSt:\n",
    "                                                        gh=[]\n",
    "                                                        c=0\n",
    "                                                        for k in WORDSt[t]:\n",
    "                                                            if k in prr:\n",
    "                                                                if qrat[t]==prr[k]:\n",
    "                                                                    if k in feature_sh_v:\n",
    "                                                                        if k not in gh:\n",
    "                                                                            if c<5:\n",
    "                                                                                gh.append(k)\n",
    "                                                                                c=c+1\n",
    "                                                        if len(gh)>0:\n",
    "                                                                shap_exp[t]=gh\n",
    "\n",
    "                                                    for tt in shap_exp:\n",
    "                                                         pass#print(tt,shap_exp[tt])\n",
    "                                                    shap_all={}\n",
    "                                                    shap_all_p={}\n",
    "                                                    shap_all_n={}\n",
    "\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            c=0\n",
    "                                                            for zz in shap_exp[t]:\n",
    "                                                                if zz in ann[t]:\n",
    "                                                                    c=c+1\n",
    "                                                            if len(shap_exp[t])>0:\n",
    "                                                                s=float(c)/len(shap_exp[t])\n",
    "                                                                if s>0:\n",
    "                                                                    shap_all[t]=s\n",
    "                                                    ss=0\n",
    "                                                    for k in shap_all:\n",
    "                                                        ss=ss+float(shap_all[k])\n",
    "\n",
    "                                                    acc=ss/len(shap_all)\n",
    "                                                    print(\"Shap Accuracy without human-feedback\"+\"\\n\")\n",
    "                                                    print(\"Shap Accuracy\")\n",
    "                                                    print(acc)\n",
    "                                                    print(\"Shap only positive reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==2:\n",
    "                                                                        c=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann[t]:\n",
    "                                                                                c=c+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_p[t]=s\n",
    "                                                    ss1=0\n",
    "                                                    for k in shap_all_p:\n",
    "                                                        ss1=ss1+float(shap_all_p[k])\n",
    "\n",
    "                                                    acc_p=ss1/len(shap_all_p)\n",
    "                                                    print(\"Shap Accuracy for positive reviews\")\n",
    "                                                    print(acc_p)\n",
    "                                                    print(\"Shap only negative reviews\")\n",
    "                                                    for t in shap_exp:\n",
    "                                                        if t in ann:\n",
    "                                                            if qrat[t]==0:\n",
    "                                                                        c=0\n",
    "                                                                        for zz in shap_exp[t]:\n",
    "                                                                            if zz in ann[t]:\n",
    "                                                                                c=c+1\n",
    "                                                                        if len(shap_exp[t])>0:\n",
    "                                                                            s=float(c)/len(shap_exp[t])\n",
    "                                                                            if s>0:\n",
    "                                                                                shap_all_n[t]=s\n",
    "                                                    ss2=0\n",
    "                                                    for k in shap_all_n:\n",
    "                                                        ss2=ss2+float(shap_all_n[k])\n",
    "\n",
    "                                                    acc_n=ss2/len(shap_all_n)\n",
    "                                                    print(\"Shap Accuracy for negative reviews\")\n",
    "                                                    print(acc_n)\n",
    "                                                    return acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train, corpus_test, y_train, y_test,p,qrat,WORDSt\n",
    "\n",
    "\n",
    "                        shap_accuracy_exp={}\n",
    "                        shap_accuracy_exp_p={}\n",
    "                        shap_accuracy_exp_n={}\n",
    "                        shap_weights={}\n",
    "                        shap_explanation={}\n",
    "                        standard_explanation={}\n",
    "                        per_cluster_train_data={}\n",
    "                        per_cluster_test_data={}\n",
    "                        per_cluster_train_target={}\n",
    "                        per_cluster_test_target={}\n",
    "                        per_cluster_predicted_class={}\n",
    "                        per_cluster_or_ev={}\n",
    "                        per_cluster_updated_ev={}\n",
    "\n",
    "                        s_words,stopwords,WORDS,qrat,H11,Rev_text_map=data_processing()\n",
    "                        WORDS_p={}\n",
    "                        WORDS_n={}\n",
    "                        for kk in WORDS:\n",
    "                            if qrat[kk]==2:\n",
    "                                WORDS_p[kk]=WORDS[kk]\n",
    "                            elif qrat[kk]==0:\n",
    "                                WORDS_n[kk]=WORDS[kk]\n",
    "\n",
    "\n",
    "                        def shap_opration_call(s_words,stopwords,WORDS,qrat,H11,Rev_text_map):\n",
    "                            #for hh in range(5,76,15):\n",
    "                            hh=0\n",
    "                            acc,acc_p,acc_n,shap_exp,shap_values,ann,corpus_train, corpus_test, y_train, y_test,p,qrat,WORDSt=Shap_Explanation_Engine(s_words,stopwords,WORDS,qrat,H11,Rev_text_map)\n",
    "                            shap_accuracy_exp[hh]=acc\n",
    "                            shap_accuracy_exp_p[hh]=acc_p\n",
    "                            shap_accuracy_exp_n[hh]=acc_n\n",
    "                            shap_weights[hh]=shap_values\n",
    "                            shap_explanation[hh]=shap_exp\n",
    "                            standard_explanation[hh]=ann\n",
    "                            per_cluster_train_data[hh]=corpus_train\n",
    "                            per_cluster_test_data[hh]=corpus_test\n",
    "                            per_cluster_train_target[hh]=y_train\n",
    "                            per_cluster_test_target[hh]=y_test\n",
    "                            per_cluster_predicted_class[hh]=p\n",
    "                            per_cluster_or_ev[hh]=WORDSt\n",
    "                            per_cluster_updated_ev[hh]=WORDSt\n",
    "\n",
    "                            return shap_accuracy_exp,shap_accuracy_exp_p,shap_accuracy_exp_n,shap_weights,shap_explanation,standard_explanation,per_cluster_train_data,per_cluster_test_data,per_cluster_train_target,per_cluster_test_target,per_cluster_predicted_class,qrat,per_cluster_or_ev,per_cluster_updated_ev\n",
    "\n",
    "\n",
    "                        shap_accuracy_exp_or,shap_accuracy_exp_p_or,shap_accuracy_exp_n_or,shap_weights_or,shap_explanation_or,standard_explanation_or,per_cluster_train_data_or,per_cluster_test_data_or,per_cluster_train_target_or,per_cluster_test_target_or,per_cluster_predicted_class_or,qrat_or,per_cluster_or_ev_or,per_cluster_updated_ev_or=shap_opration_call(s_words,stopwords,WORDS,qrat,H11,Rev_text_map)\n",
    "                        #shap_accuracy_exp_p,shap_weights_p,shap_explanation_p,standard_explanation_p,per_cluster_train_data_p,per_cluster_test_data_p,per_cluster_train_target_p,per_cluster_test_target_p,per_cluster_predicted_class_p,qrat_p,per_cluster_or_ev_p,per_cluster_updated_ev_p=shap_opration_call(s_words,stopwords,WORDS_p,qrat,H11,Rev_text_map)\n",
    "                        #shap_accuracy_exp_n,shap_weights_n,shap_explanation_n,standard_explanation_n,per_cluster_train_data_n,per_cluster_test_data_n,per_cluster_train_target_n,per_cluster_test_target_n,per_cluster_predicted_class_n,qrat_n,per_cluster_or_ev_n,per_cluster_updated_ev_n=shap_opration_call(s_words,stopwords,WORDS_n,qrat,H11,Rev_text_map)\n",
    "\n",
    "                        return shap_explanation_u,qrat_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reviews=55630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The option feature_perturbation=\"independent\" is has been renamed to feature_perturbation=\"interventional\"!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n",
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n"
     ]
    }
   ],
   "source": [
    "shap_explanation_u,qrat_u=SHAP_NON_RELATIONAL_EXPLANATION()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
