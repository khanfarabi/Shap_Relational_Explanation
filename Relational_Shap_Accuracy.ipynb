{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d096d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pre-process the Reviews /Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def Run_program():\n",
    "                class relational_lime:\n",
    "                                @classmethod\n",
    "                                def run_all(cls):\n",
    "\n",
    "                                                def data_pr():\n",
    "                                                            ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                            revid = 0\n",
    "                                                            users = defaultdict(list)\n",
    "                                                            for ln in ifile1:\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                users[parts[0]].append(revid)\n",
    "                                                                revid = revid + 1\n",
    "                                                            ifile1.close()\n",
    "                                                            #pass#pass#print(users)\n",
    "                                                            H11=defaultdict(list)\n",
    "                                                            #sys.exit()\n",
    "                                                            userids = []\n",
    "                                                            #pass#pass#print(users)\n",
    "                                                            c =0\n",
    "                                                            #Select reviewer subset based on tunable parameters (max-reviews and min-reviews limit,sampling ratio)\n",
    "                                                            minreviews =5\n",
    "                                                            maxreviews =20\n",
    "                                                            samplingratio =0.15\n",
    "                                                            for u in users:\n",
    "                                                                if len(users[u])>minreviews and len(users[u])<maxreviews:        \n",
    "                                                                    if random.random() < samplingratio:\n",
    "                                                                        userids.append(u)\n",
    "                                                                        c= c + len(users[u])\n",
    "\n",
    "                                                            ifile = open(\"reviewContent.txt\",encoding='ISO-8859-1')\n",
    "                                                            ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                            s_words = []\n",
    "                                                            sfile = open(\"Words.txt\")\n",
    "                                                            for ln in sfile:\n",
    "                                                                s_words.append(ln.strip())\n",
    "                                                            sfile.close()\n",
    "                                                            stopwords = []\n",
    "                                                            sfile = open(\"stopwords.txt\")\n",
    "                                                            for ln in sfile:\n",
    "                                                                stopwords.append(ln.strip())\n",
    "                                                            sfile.close()\n",
    "\n",
    "                                                            flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n",
    "                                                                     else 0)\n",
    "                                                            ofile = open(\"all_revs1.txt\",'w')\n",
    "                                                            ofile1 = open(\"metadata.txt\",'w')\n",
    "                                                            cnt = 0\n",
    "                                                            revid = 0\n",
    "                                                            qrat={}\n",
    "                                                            windex = defaultdict(list)\n",
    "                                                            #Tunable parameter to keep non-sentiment words\n",
    "                                                            PNonSentWords = 0.30\n",
    "                                                            WORDS={}\n",
    "                                                            w_per_ht=defaultdict(list)\n",
    "                                                            w_per_user=defaultdict(list)\n",
    "                                                            Rev_text_map={}\n",
    "                                                            for ln in ifile:\n",
    "                                                                ln1 = ifile1.readline()\n",
    "                                                                parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                                #pass#pass#print(parts1)\n",
    "                                                                if parts1[0] not in userids:\n",
    "                                                                    continue\n",
    "                                                                #if cnt >= 10000:\n",
    "                                                                #    break\n",
    "                                                                keep = []\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                for word in re.findall(r\"\\w[\\w']*\", parts[3], flags=flags):\n",
    "                                                                    if word.isdigit() or len(word)==1:\n",
    "                                                                        continue\n",
    "                                                                    word_lower = word.lower()\n",
    "                                                                    if word_lower in stopwords:\n",
    "                                                                        continue\n",
    "                                                                   # if word_lower in s_words:\n",
    "                                                                        #keep.append(word_lower)\n",
    "                                                                    elif random.random() < PNonSentWords:\n",
    "                                                                        if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                                                            keep.append(word_lower)\n",
    "                                                                if float(parts1[2])<=2:\n",
    "                                                                    cl = 0\n",
    "                                                                elif float(parts1[2])==3:\n",
    "                                                                    cl = 1\n",
    "                                                                elif float(parts1[2])>=4:\n",
    "                                                                    cl = 2\n",
    "                                                                if len(keep)>=10:\n",
    "                                                                    cnt = cnt + 1\n",
    "                                                                    ofile.write(\" \".join(keep)+\"\\t\"+str(cl)+\"\\n\")\n",
    "                                                                    WORDS[revid]=keep\n",
    "\n",
    "                                                                    qrat[revid]=cl\n",
    "                                                                    Rev_text_map[revid]=parts[3]\n",
    "                                                                    H11[parts1[1]].append(revid)\n",
    "                                                                    ofile1.write(ln1)\n",
    "                                                                    for w in keep:\n",
    "                                                                        windex[w].append(revid)\n",
    "                                                                        w_per_ht[w].append(parts1[1])\n",
    "                                                                        w_per_user[w].append(parts1[0])\n",
    "                                                                    revid = revid + 1\n",
    "                                                            ofile.close()\n",
    "                                                            ofile1.close()\n",
    "                                                            ifile.close()\n",
    "                                                            ifile1.close()\n",
    "\n",
    "                                                            #Tunable parameter (keep words only if repeated in > NumReps reviews)\n",
    "                                                            NumReps = 10\n",
    "\n",
    "                                                            #Filter review words\n",
    "                                                            ifile = open(\"all_revs1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                            ofile = open(\"processed_revs_1.txt\",'w')\n",
    "                                                            for ln in ifile:\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                keep = []\n",
    "                                                                for w in parts[0].split(\" \"):\n",
    "                                                                    if len(windex[w])>NumReps:\n",
    "                                                                        keep.append(w)\n",
    "                                                                ofile.write(\" \".join(keep)+\"\\t\"+parts[1]+\"\\n\")\n",
    "                                                            ofile.close()\n",
    "                                                            ifile.close()\n",
    "                                                            pass#pass#print(\"Total Reviews=\"+str(len(WORDS)))\n",
    "                                                            return WORDS,qrat,H11,Rev_text_map,s_words,stopwords\n",
    "                                                def data_balancing(WORDS):\n",
    "                                                    WORDS1={}\n",
    "                                                    t1=[]\n",
    "                                                    t2=[]\n",
    "                                                    t3=[]\n",
    "                                                    c1=0\n",
    "                                                    c2=0\n",
    "                                                    c3=0\n",
    "                                                    for y in WORDS:\n",
    "                                                        if qrat[y]==0:\n",
    "                                                            if c3<80:\n",
    "                                                                t1.append(y)\n",
    "                                                                WORDS1[y]=WORDS[y]\n",
    "                                                                c3=c3+1\n",
    "                                                        elif qrat[y]==1:\n",
    "                                                            continue\n",
    "                                                            #if c1<515:\n",
    "                                                                #t2.append(y)\n",
    "                                                                #WORDS1[y]=WORDS[y]\n",
    "                                                                #c1=c1+1\n",
    "                                                        elif qrat[y]==2:\n",
    "                                                            if c2<80:\n",
    "                                                                t3.append(y)\n",
    "                                                                WORDS1[y]=WORDS[y]\n",
    "                                                                c2=c2+1\n",
    "                                                    pass#pass#print(len(t1),len(t2),len(t3),len(WORDS1))\n",
    "                                                    return WORDS1\n",
    "\n",
    "                                                def train_data_gen(WORDS1):\n",
    "                                                    #train and target data\n",
    "                                                    w11=[]\n",
    "                                                    trg11=[]\n",
    "                                                    w12=[]\n",
    "                                                    trg1=[]\n",
    "                                                    for tt in WORDS1:\n",
    "                                                        s=' '\n",
    "                                                        #if qrat[tt]!=1:\n",
    "                                                        for kk in WORDS1[tt]:\n",
    "                                                           # if qrat[tt]!=1:\n",
    "                                                                w11.append(kk)\n",
    "                                                                s=str(kk)+s+\"\\t\"\n",
    "                                                                #pass#pass#print(kk)\n",
    "                                                                if float(qrat[tt])==2:\n",
    "                                                                        trg11.append(1)\n",
    "                                                                else:\n",
    "                                                                        trg11.append(0)\n",
    "                                                            #w12.append(s)\n",
    "                                                            #trg1.append(qrat[tt])\n",
    "\n",
    "                                                    return w11,trg11\n",
    "                                                def svm_coeff(w11,trg):\n",
    "                                                                    #SVM Learner and generate feature weights\n",
    "                                                                    #OutputCodeClassifier(LinearSVC(random_state=0),code_size=2, random_state=0)\n",
    "                                                                    #Learn SVM Model\n",
    "                                                                    #ifile = open(\"processed_revs_1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                                    Y = trg\n",
    "                                                                    words =w11\n",
    "                                                                    unique_words =[]\n",
    "                                                                    ss=set(words)\n",
    "                                                                    for w1 in ss:\n",
    "                                                                            if w1 not in unique_words:\n",
    "                                                                                unique_words.append(w1)\n",
    "\n",
    "                                                                    '''\n",
    "                                                                    #tf_transformer = TfidfVectorizer()\n",
    "                                                                    #f = tf_transformer.fit_transform(words)\n",
    "                                                                    #features = [((i, j), f[i,j]) for i, j in zip(*f.nonzero())]\n",
    "                                                                    #unique_word_ids = []\n",
    "                                                                    #for w in unique_words:\n",
    "                                                                       # i = tf_transformer.vocabulary_.get(w)\n",
    "                                                                        #unique_word_ids.append(i)\n",
    "\n",
    "                                                                    #clf =OneVsRestClassifier(SVC(kernel='linear'))#svm.LinearSVC(C=1)\n",
    "                                                                    #clf.fit(f,Y)\n",
    "                                                                    #p = clf.predict(f)\n",
    "                                                                    #pass#pass#print(f1_score(Y,p,average='weighted'))\n",
    "\n",
    "                                                                    '''\n",
    "                                                                    tf_transformer = TfidfVectorizer()\n",
    "                                                                    f = tf_transformer.fit_transform(words)\n",
    "                                                                    features = [((i, j), f[i,j]) for i, j in zip(*f.nonzero())]\n",
    "                                                                    unique_word_ids = []\n",
    "                                                                    for w in unique_words:\n",
    "                                                                        i = tf_transformer.vocabulary_.get(w)\n",
    "                                                                        unique_word_ids.append(i)\n",
    "\n",
    "                                                                    #clf =svm.LinearSVC(C=100,probability=True)\n",
    "                                                                    clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', random_state=None)\n",
    "                                                                    #svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "                                                                    #OneVsRestClassifier(LinearSVC(random_state=0)) #svm.LinearSVC(C=1)\n",
    "                                                                    clf.fit(f,Y)\n",
    "                                                                    #clf.fit(f,Y)\n",
    "                                                                    p = clf.predict(f)\n",
    "                                                                    pass#pass#print(f1_score(Y,p,average='micro'))\n",
    "                                                                    from sklearn.metrics import accuracy_score\n",
    "                                                                    pass#pass#print(accuracy_score(Y,p))\n",
    "\n",
    "                                                                    #Store learned weights\n",
    "\n",
    "                                                                    #Tunable parameter, normalization range of weights\n",
    "                                                                    rangelower=0\n",
    "                                                                    rangehigher=1\n",
    "\n",
    "                                                                    C = clf.coef_\n",
    "                                                                    scaler = MinMaxScaler(feature_range=(rangelower,rangehigher))\n",
    "                                                                    vals = []\n",
    "                                                                    for i, j in zip(*C.nonzero()):\n",
    "                                                                        vals.append([C[i,j]])\n",
    "                                                                    scaler.fit(vals)\n",
    "                                                                    V1 = scaler.transform(vals)\n",
    "                                                                    rows,cols = C.nonzero()\n",
    "                                                                    r_wts =  defaultdict(list)\n",
    "                                                                    ix= 0\n",
    "                                                                    for w in unique_words:\n",
    "                                                                        r_wts[w] = [0]  \n",
    "                                                                    for i, j in zip(*C.nonzero()):\n",
    "                                                                        #pass#pass#print(i,j)\n",
    "                                                                        for k in tf_transformer.vocabulary_.keys():\n",
    "                                                                            if tf_transformer.vocabulary_[k]==j:\n",
    "                                                                                if k not in r_wts:\n",
    "                                                                                    break\n",
    "                                                                                else:\n",
    "                                                                                    #if float(V1[ix][0])>0:\n",
    "                                                                                    r_wts[k][i] = V1[ix][0]\n",
    "                                                                                    break\n",
    "                                                                        ix = ix + 1\n",
    "\n",
    "\n",
    "                                                                    #pass#pass#print(r_wts)\n",
    "                                                                    return r_wts\n",
    "                                                #  Extracting Samehote Relation\n",
    "                                                def sm_h(WORDS1,H11):\n",
    "                                                    H1={}\n",
    "                                                    fl=open(\"samehote.txt\",\"w\")\n",
    "                                                    for tt in H11:\n",
    "                                                        gh=[]\n",
    "                                                        for kk in H11[tt]:\n",
    "                                                            if kk in WORDS1:\n",
    "                                                                if kk not in gh:\n",
    "                                                                    gh.append(str(kk))\n",
    "                                                        if len(gh)>1:\n",
    "                                                            H1[tt]=gh\n",
    "                                                            ggg=str(tt)+\"::\"+str(gh)\n",
    "                                                            fl.write(str(ggg)+\"\\n\")\n",
    "                                                    fl.close()       \n",
    "                                                    for t in H1:\n",
    "                                                        pass#pass#pass#print(t,H1[t])   \n",
    "                                                    #pass#pass#print(len(H1))\n",
    "                                                    return H1\n",
    "                                                #Sentance generation  for Neural Word2Vec Embedding Training\n",
    "                                                def snt_emd(WORDS1):\n",
    "                                                    sent=[]\n",
    "                                                    sent1=[]\n",
    "                                                    sent_map=defaultdict(list)\n",
    "                                                    for ty in WORDS1:\n",
    "                                                        gh=[]\n",
    "                                                        gh.append(str(ty))\n",
    "                                                        #gh1=[]\n",
    "                                                        #gh2=[]\n",
    "                                                        for j in WORDS1[ty]:\n",
    "                                                            j1=str(j)\n",
    "                                                            #gh.append(str(ty))\n",
    "                                                            if j1 not in gh:\n",
    "                                                                gh.append(j1)\n",
    "\n",
    "                                                        if gh not in sent:\n",
    "                                                                sent.append(gh)       \n",
    "                                                    documents=[]\n",
    "                                                    #documents1=[]\n",
    "                                                    for t in sent:\n",
    "                                                        for jh in t:\n",
    "                                                                 documents.append(jh)\n",
    "                                                    return sent,documents\n",
    "                                                # Clustering the queries\n",
    "                                                def kmean_cls(sent,documents,WORDS1):\n",
    "\n",
    "                                                        model = Word2Vec(sent, min_count=1)\n",
    "                                                        X = model[model.wv.vocab]\n",
    "                                                        NUM_CLUSTERS=5\n",
    "                                                        kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                                                        assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                                                        #pass#pass#print (assigned_clusters)\n",
    "                                                        cluster={}\n",
    "                                                        words = list(model.wv.vocab)\n",
    "                                                        for i, word in enumerate(words):\n",
    "                                                                gh=[] \n",
    "                                                                gh1=[] \n",
    "                                                                gh2=[] \n",
    "                                                                if word.isdigit(): \n",
    "                                                                    cluster[word]=assigned_clusters1[i]\n",
    "\n",
    "                                                        cluster_final={}\n",
    "                                                        for j in range(NUM_CLUSTERS):\n",
    "                                                            gg=[]\n",
    "                                                            for tt in cluster:\n",
    "                                                                if int(cluster[tt])==int(j):\n",
    "                                                                    if tt not in gg:\n",
    "                                                                        gg.append(tt)\n",
    "                                                            if len(gg)>0:\n",
    "                                                                        cluster_final[j]=gg\n",
    "                                                        cc=0\n",
    "                                                        final_clu={}\n",
    "                                                        for t in cluster_final:\n",
    "                                                            ghh=[]\n",
    "                                                            for k in cluster_final[t]:\n",
    "                                                                if int(k) in WORDS1:\n",
    "                                                                       ghh.append(int(k))\n",
    "                                                            if len(ghh)>=2:\n",
    "                                                                    final_clu[cc]=ghh\n",
    "                                                                    cc=cc+1\n",
    "                                                        s=0\n",
    "                                                        for k in final_clu:\n",
    "                                                              s=s+len(final_clu[k])\n",
    "                                                       # pass#pass#print(s)\n",
    "                                                        return final_clu\n",
    "\n",
    "                                                def sm_user():\n",
    "                                                    #Store similarity relations based on if they were written by same user\n",
    "                                                        ifile = open(\"processed_revs_1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                        ifile1 = open(\"metadata.txt\")\n",
    "                                                        SIM = collections.defaultdict(list)\n",
    "                                                        userindex = {}\n",
    "                                                        Sho= collections.defaultdict(list)\n",
    "                                                        posrev=[]\n",
    "                                                        negrev=[]\n",
    "                                                        hotelindex={}\n",
    "                                                        evida=[]\n",
    "                                                        revid = 0\n",
    "                                                        for ln in ifile:\n",
    "                                                            parts = ln.strip().split(\"\\t\")\n",
    "                                                            #if float(parts[1])==2:\n",
    "                                                               # ppr=\"Positive(\"+str(revid)+\")\"\n",
    "                                                                #posrev.append(ppr)\n",
    "                                                                #if ppr not in evida:\n",
    "                                                                    #pass#evida.append(ppr)\n",
    "                                                            #elif float(parts[1])==0:\n",
    "                                                                #ppr1=\"Negative(\"+str(revid)+\")\"\n",
    "                                                                #negrev.append(ppr1)\n",
    "                                                                #if ppr1 not in evida:\n",
    "                                                                   # pass#evida.append(ppr1)\n",
    "                                                            ln1 = ifile1.readline()\n",
    "                                                            parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                           # pass#pass#print(parts1)\n",
    "                                                            SIM[parts1[0]].append(revid)\n",
    "                                                            Sho[parts1[1]].append(revid)\n",
    "                                                            userindex[revid] = parts1[0]\n",
    "                                                            hotelindex[revid]=parts1[1]\n",
    "                                                            revid = revid + 1    \n",
    "                                                        ifile.close()\n",
    "                                                        ifile1.close()\n",
    "                                                        #pass#pass#print(SIM)\n",
    "                                                        Samuser_index={}\n",
    "                                                        for t in SIM:\n",
    "                                                            gh=[]\n",
    "                                                            for k in SIM[t]:\n",
    "                                                                if len(SIM[t])>0:\n",
    "                                                                    for e in SIM[t]:\n",
    "                                                                        if e not in gh and e in WORDS:\n",
    "                                                                            gh.append(str(e))\n",
    "                                                            if gh!=[]:\n",
    "                                                                Samuser_index[t]=gh\n",
    "                                                        for d in Samuser_index:\n",
    "                                                            pass#pass#pass#print(d,Samuser_index[d])          \n",
    "                                                        #pass#pass#print(len(Samuser_index))\n",
    "                                                        return Samuser_index\n",
    "                                                #Annotated Word exp Used for validation\n",
    "                                                def annotation_wordexp(WORDS1,s_words,stopwords):\n",
    "                                                    ann1={}\n",
    "                                                    c=0\n",
    "                                                    for k in WORDS1:\n",
    "                                                        if qrat[k]==2:\n",
    "                                                            c=c+1\n",
    "                                                            c2=0\n",
    "                                                            gff=[]\n",
    "                                                            for gg in WORDS1[k]:\n",
    "                                                                if gg in s_words:\n",
    "                                                                    if c2<25:\n",
    "                                                                             gff.append(gg)\n",
    "                                                                             c2=c2+1\n",
    "                                                            if len(gff)>0:\n",
    "                                                               # if k in WORDSt:\n",
    "                                                                    ann1[k]=gff\n",
    "\n",
    "                                                        elif qrat[k]==0:\n",
    "                                                            c=c+1\n",
    "                                                            c3=0\n",
    "                                                            gff1=[]\n",
    "                                                            for gg in WORDS1[k]:\n",
    "                                                                if gg in s_words:\n",
    "                                                                    if c3<25:\n",
    "                                                                            gff1.append(gg)\n",
    "                                                                            c3=c3+1\n",
    "\n",
    "                                                            if len(gff1)>0:\n",
    "                                                                #if k WORDSt:\n",
    "                                                                    ann1[k]=gff1\n",
    "                                                    ann={}\n",
    "                                                    for t in ann1:\n",
    "                                                        if t in WORDS1:\n",
    "                                                            ann[t]=ann1[t]\n",
    "                                                    pass#pass#print(len(ann))\n",
    "                                                    gg=open(\"Review_Word_annotation.txt\",\"w\")\n",
    "                                                    for t in ann:\n",
    "                                                        vv=str(t)+\":\"+str(ann[t])\n",
    "                                                        gg.write(str(vv)+\"\\n\")\n",
    "                                                        pass#pass#print(t,ann[t])\n",
    "                                                    gg.close()\n",
    "                                                    return ann\n",
    "                                                def feedback_gen(final_clu):\n",
    "                                                    d_tt={}\n",
    "                                                    d_tt[0]=\"negative\"\n",
    "                                                    d_tt[2]=\"positive\"\n",
    "                                                    wr={}\n",
    "                                                    w=[]\n",
    "                                                    for k in final_clu:\n",
    "                                                            #c=-1\n",
    "                                                            #c=c+1\n",
    "                                                            md=int(len(final_clu[k])/2)\n",
    "                                                            c=0      \n",
    "                                                            k1= final_clu[k][md+c]\n",
    "                                                            #pass#pass#print(k1,md)        \n",
    "                                                            if k1 in ann:\n",
    "                                                                        for k3 in ann[k1]:\n",
    "                                                                                w.append(k3)\n",
    "                                                            else:\n",
    "                                                                c=c+11\n",
    "                                                                continue \n",
    "\n",
    "\n",
    "                                                            #pass#pass#print(k,k1,md,d_tt[qrat[k1]],w)\n",
    "                                                            wr[k1]=w\n",
    "                                                    #pass#pass#print(w)\n",
    "                                                    return w\n",
    "\n",
    "                                                #Update Evidence Based on manual annotation Update 2 \n",
    "\n",
    "                                                def update_evid_annotation(w,WORDS1):\n",
    "                                                                model = Word2Vec(sent, min_count=1)\n",
    "                                                                data_g={}\n",
    "                                                                for t in WORDS1:\n",
    "                                                                    chu=[]\n",
    "                                                                    #try:\n",
    "                                                                    vb={}\n",
    "                                                                    for v in w:\n",
    "                                                                        vb1={}\n",
    "                                                                        for v1 in WORDS1[t]:\n",
    "                                                                                #pass#pass#print(v1,v)\n",
    "                                                                                gh1=model.similarity(v,v1)\n",
    "                                                                                if gh1>=0.1:\n",
    "                                                                                      vb1[v1]=float(gh1)\n",
    "                                                                                      #pass#pass#print(gh1)\n",
    "                                                                        for jk in vb1:\n",
    "                                                                            if jk in vb:\n",
    "                                                                                if float(vb1[jk])>=float(vb[jk]):\n",
    "                                                                                    #pass#pass#print(jk,vb1[jk],vb[jk])\n",
    "                                                                                    vb[jk]=vb1[jk]\n",
    "                                                                            else:\n",
    "                                                                                vb[jk]=vb1[jk]\n",
    "                                                                    #pass#pass#print(t, vb)\n",
    "                                                                    #pass#pass#print(\"\\n\")             \n",
    "                                                                    dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                                    cc=0\n",
    "                                                                    for kkk in dd:\n",
    "                                                                        if kkk[0] not in chu:\n",
    "                                                                            #if cc<20:\n",
    "                                                                                chu.append(kkk[0])\n",
    "                                                                                cc=cc+1\n",
    "\n",
    "                                                                    if len(chu)>0:\n",
    "                                                                        data_g[t]=chu\n",
    "                                                                #survey checking\n",
    "                                                                #pass#pass#print(len(WORDS1))\n",
    "                                                                #Updating the Whole Evidence Based on manual annotation\n",
    "                                                                WORDS22={}\n",
    "                                                                for gg in WORDS1:\n",
    "                                                                    #if gg in data_extract12:\n",
    "                                                                        #WORDS2[gg]=data_extract12[gg]\n",
    "                                                                    if gg in data_g:\n",
    "                                                                        if len(data_g[gg])>0:\n",
    "                                                                            WORDS22[gg]=data_g[gg]\n",
    "                                                                #pass#pass#print(WORDS2['d_535'])\n",
    "                                                                #pass#pass#print(len(WORDS22))\n",
    "                                                                for t in WORDS22:\n",
    "                                                                    pass#pass#pass#print(t,WORDS22[t])\n",
    "                                                                return WORDS22\n",
    "\n",
    "\n",
    "                                                WORDS,qrat,H11,Rev_text_map,s_words,stopwords=data_pr()\n",
    "                                                WORDS1=data_balancing(WORDS)\n",
    "                                                w11,trg1=train_data_gen(WORDS1)\n",
    "                                                #pass#pass#print(trg1)\n",
    "                                                r_wts=svm_coeff(w11,trg1)\n",
    "                                                H1=sm_h(WORDS1,H11)\n",
    "                                                sent,documents=snt_emd(WORDS1)\n",
    "                                                final_clu=kmean_cls(sent,documents,WORDS1)\n",
    "                                                Samuser_index=sm_user()\n",
    "                                                ann=annotation_wordexp(WORDS1,s_words,stopwords)\n",
    "                                                w=feedback_gen(final_clu)\n",
    "                                                WORDS22=update_evid_annotation(w,WORDS1)\n",
    "                                                return WORDS,WORDS22,qrat,H1,ann,Rev_text_map,s_words,stopwords,WORDS1,r_wts,w11,trg1,H1,sent,documents,final_clu,Samuser_index\n",
    "                                @classmethod\n",
    "                                def relational_embedding_exp(cls,m,WORDS22,WORDS1,Samuser_index,H1):\n",
    "                                    # Relational Exp generatetion based on neural embedding\n",
    "                                                sent2=[]\n",
    "                                                sent1=[]\n",
    "                                                sent_map=defaultdict(list)\n",
    "                                                for ty in WORDS22:\n",
    "                                                    gh=[]\n",
    "                                                    gh.append(str(ty))\n",
    "                                                    #gh1=[]\n",
    "                                                    #gh2=[]\n",
    "                                                    for j in WORDS22[ty]:\n",
    "\n",
    "                                                        j1=str(j)\n",
    "                                                        #gh.append(str(ty))\n",
    "                                                        if j1 not in gh:\n",
    "                                                            gh.append(j1)\n",
    "                                                        ##print(gh)\n",
    "\n",
    "\n",
    "                                                    if gh not in sent2:\n",
    "                                                            sent2.append(gh)\n",
    "\n",
    "\n",
    "                                                documents1=[]\n",
    "                                                #documents1=[]\n",
    "                                                for t in sent2:\n",
    "                                                    s=''\n",
    "                                                    for jh in t:\n",
    "                                                        if jh.isdigit():\n",
    "                                                             documents1.append(jh)\n",
    "                                                        else:\n",
    "                                                            s=\" \"+str(jh)+s+\" \"\n",
    "                                                    documents1.append(s)\n",
    "\n",
    "\n",
    "                                                #sentence embedding\n",
    "                                                from gensim.test.utils import common_texts\n",
    "                                                from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "                                                documents2 = [TaggedDocument(doc, [i]) for i, doc in enumerate(sent2)]\n",
    "                                                for t in documents2:\n",
    "                                                    pass##print(t)\n",
    "                                                model = Doc2Vec(documents2, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "                                                #K-Means Run 14 to find the neighbors per query \n",
    "\n",
    "                                                #cluster generation with k-means\n",
    "                                                import sys\n",
    "                                                from nltk.cluster import KMeansClusterer\n",
    "                                                import nltk\n",
    "                                                from sklearn import cluster\n",
    "                                                from sklearn import metrics\n",
    "                                                import gensim \n",
    "                                                import operator\n",
    "                                                #from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "                                                #model = Word2Vec(sent, min_count=1) dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=False)\n",
    "                                                import operator\n",
    "                                                X = model[model.wv.vocab]\n",
    "                                                c=0\n",
    "                                                cluster={}\n",
    "                                                num=[]\n",
    "                                                weight_map={}\n",
    "                                                similar_r_map={}\n",
    "                                                fg={}\n",
    "                                                wg={}\n",
    "\n",
    "                                                for jj in WORDS1:\n",
    "                                                    gh1=[]\n",
    "                                                    gh2=[]\n",
    "                                                    gh3=[]\n",
    "                                                    s=0\n",
    "\n",
    "                                                    for k in documents1:\n",
    "                                                        if str(k)==str(jj):\n",
    "                                                            gh=model.most_similar(positive=str(k),topn=600)\n",
    "                                                           # #print(gh)\n",
    "                                                            for tt in gh:\n",
    "                                                                if float(tt[1]) not in gh1:\n",
    "                                                                    gh1.append(float(tt[1]))\n",
    "                                                                #if tt[0] not in gh2:\n",
    "                                                                if tt[0].isdigit():\n",
    "                                                                        #if ccc<5:\n",
    "                                                                                #gh2.append(tt[0])\n",
    "                                                                                fg[tt[0]]=tt[1]\n",
    "                                                                                #ccc=ccc+1\n",
    "                                                    #for ffg in gh1:\n",
    "                                                        #s=s+ffg\n",
    "                                                    dd=sorted(fg.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                    ccc=0\n",
    "                                                    ccc1=0\n",
    "                                                    for t5 in dd:\n",
    "                                                        if qrat[int(jj)]==qrat[int(t5[0])]:\n",
    "                                                            if m==5:\n",
    "                                                                for vbc in Samuser_index:\n",
    "                                                                    if int(jj) in Samuser_index[vbc] or  str(jj) in Samuser_index[vbc] and int(t5[0]) in Samuser_index[vbc] or str(t5[0]) in Samuser_index[vbc]:\n",
    "                                                                                 if ccc<500:\n",
    "                                                                                         gh2.append(t5[0])\n",
    "                                                                                         gh3.append(t5[1])\n",
    "                                                                                         ccc=ccc+1\n",
    "                                                                for vbc in H1:\n",
    "                                                                    if int(jj) in H1[vbc] or  str(jj) in H1[vbc] and int(t5[0]) in H1[vbc] or str(t5[0]) in H1[vbc]:\n",
    "                                                                                if ccc1<500:\n",
    "                                                                                         gh2.append(t5[0])\n",
    "                                                                                         gh3.append(t5[1])\n",
    "                                                                                         ccc1=ccc1+1\n",
    "                                                            elif m==10:\n",
    "                                                                if ccc<400:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==15:\n",
    "                                                                if ccc<500:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==20:\n",
    "                                                                if ccc<600:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==25:\n",
    "                                                                if ccc<700:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "\n",
    "                                                    #if len(gh2)>=2:\n",
    "                                                    similar_r_map[jj]=gh2\n",
    "                                                    wg[jj]=gh3\n",
    "                                                            #ccc=ccc+1\n",
    "\n",
    "                                                return similar_r_map,wg\n",
    "\n",
    "                                @classmethod\n",
    "                                def shap_rel(cls,similar_r_map,wg,WORDS22,WORDS1,qrat,Samuser_index,H1):\n",
    "                                        # Relation Vector Generation\n",
    "                                        rlvec=[]\n",
    "                                        for t in wg:\n",
    "                                            gh=[]\n",
    "                                            gh.append(t)\n",
    "                                            for k in wg[t]:\n",
    "                                                gh.append(k)\n",
    "                                            rlvec.append(gh)\n",
    "\n",
    "                                        # organizing feature vector\n",
    "                                        #related queries \n",
    "                                        pqsu={}\n",
    "                                        pqsh={}\n",
    "\n",
    "                                        for bb in WORDS1:\n",
    "                                            for kk in Samuser_index:\n",
    "                                                if bb in Samuser_index[kk] or str(bb) in Samuser_index[kk] :\n",
    "                                                    pqsu[bb]=Samuser_index[kk]\n",
    "                                                    pass#print(bb,Samuser_index[kk])\n",
    "\n",
    "                                        for bb in WORDS1:\n",
    "                                            for kk in H1:\n",
    "                                                if bb in H1[kk] or str(bb) in H1[kk]:\n",
    "                                                    if bb in pqsu:\n",
    "                                                            #print(bb,H1[kk])\n",
    "                                                            pqsh[bb]=H1[kk]\n",
    "                                        relq={}\n",
    "                                        for k in pqsh:\n",
    "                                            #print(k,pqsh[k]+pqsu[k])\n",
    "                                            relq[k]=pqsh[k]+pqsu[k]\n",
    "                                        \n",
    "                                        qf={}\n",
    "                                        for t in similar_r_map:\n",
    "                                            if t in WORDS1 and t in relq:\n",
    "                                                h=relq[t]+WORDS1[t]\n",
    "                                                #print(t,qrat[t],h),\n",
    "                                                qf[t]=h\n",
    "                                                #print(\"\\n\\n\")\n",
    "                                        # train test data\n",
    "                                        import math\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        train=[]\n",
    "                                        target=[]\n",
    "                                        test=[]\n",
    "                                        test_tr=[]\n",
    "                                        for tt in qf:\n",
    "                                            for v in qf[tt]:\n",
    "                                                #print(v)\n",
    "                                                if v not in train:\n",
    "                                                   # if v.isdigit()==False:\n",
    "                                                        train.append(v)\n",
    "                                                        target.append(str(qrat[tt]))\n",
    "                                        bb=math.ceil(len(train)*0.5)\n",
    "                                        for gg in range(0,bb):\n",
    "                                            test.append(train[gg])\n",
    "                                            test_tr.append(target[gg])\n",
    "                                        #LIME w11, trg\n",
    "                                        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        #Shap\n",
    "                                        import sklearn\n",
    "                                        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        import numpy as np\n",
    "                                        import shap\n",
    "                                        import transformers\n",
    "                                        import shap\n",
    "                                        from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "                                        #shap.initjs()\n",
    "                                        # Kernal Shap words_train targets\n",
    "\n",
    "                                        from sklearn import svm\n",
    "                                        from sklearn.svm import SVC\n",
    "                                        from sklearn.svm import LinearSVC\n",
    "                                        corpus_train, corpus_test, y_train, y_test = train_test_split(train,target, test_size=0.5, random_state=7)\n",
    "                                        vectorizer = TfidfVectorizer(min_df=1)\n",
    "                                        X_train = vectorizer.fit_transform(train)\n",
    "                                        X_test = vectorizer.transform(corpus_test)\n",
    "                                        model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                        model.fit(X_train,target)\n",
    "                                        p = model.predict(X_train)\n",
    "                                        prr={}\n",
    "                                        for jj in range(0,len(train)):\n",
    "                                            prr[train[jj]]=int(p[jj])\n",
    "                                        explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"dependent\")\n",
    "                                        shap_values = explainer.shap_values(X_train)\n",
    "                                        #X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                                        feature_names=vectorizer.get_feature_names()\n",
    "                                        #print(len(feature_names),len(shap_values))\n",
    "                                        #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                                        shape_w={}\n",
    "                                        fr={}\n",
    "                                        feature_sh_v=[]\n",
    "                                        for jj in range(0,len(train)):\n",
    "                                                if abs(sum(shap_values[jj]))>=0.2:\n",
    "                                                                      m=abs(sum(shap_values[jj]))\n",
    "                                                                      if train[jj] not in fr:\n",
    "                                                                                      fr[train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                                      elif train[jj]  in fr:\n",
    "                                                                            if m>fr[train[jj]]:\n",
    "                                                                                fr[train[jj]]=m\n",
    "                                        dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                        for tt in dd1:\n",
    "                                            if tt[0].isdigit()==True:\n",
    "                                                   feature_sh_v.append(tt[0])\n",
    "                                            elif tt[0].isdigit()==False:\n",
    "                                                feature_sh_v.append(tt[0])\n",
    "                                                #for vvv5 in WORDS22:\n",
    "                                                    #if tt[0] in WORDS22[vvv5] or str(tt[0]) in WORDS22[vvv5]:\n",
    "                                                                                      # feature_sh_v.append(tt[0])\n",
    "                                        '''\n",
    "                                        feature_sh_v1=[]\n",
    "                                        for v in feature_sh_v:\n",
    "                                            n=v.split()\n",
    "                                            for k in n:\n",
    "                                                if k not in feature_sh_v1:\n",
    "                                                    feature_sh_v1.append(k)\n",
    "                                        '''\n",
    "                                        #shap explanations\n",
    "\n",
    "                                        shap_exp={}\n",
    "                                        for t in qf:\n",
    "                                            gh=[]\n",
    "                                            c=0\n",
    "                                            for k in qf[t]:\n",
    "                                                if k in prr:\n",
    "                                                    if qrat[t]==prr[k]:\n",
    "                                                        if k in feature_sh_v:\n",
    "                                                            if k not in gh:\n",
    "                                                                #if c<20:\n",
    "                                                                    gh.append(k)\n",
    "                                                                    c=c+1\n",
    "                                            shap_exp[t]=gh\n",
    "\n",
    "\n",
    "                                        #shap.plots.beeswarm(shap_values)\n",
    "                                        return shap_exp,fr,shap_values\n",
    "                                @classmethod  \n",
    "                                def organize_results_visualization(cls,Samuser_index,H1,qrat,shap_exp):\n",
    "                                            qrsm={}\n",
    "                                            qrsh={}\n",
    "                                            qrw={}\n",
    "                                            qrall={}\n",
    "                                            cnc=[]\n",
    "                                            cnc1={}\n",
    "                                            for vv in shap_exp:\n",
    "                                                #print(vv,qrat[vv],lexp[vv])\n",
    "                                                sm=[]\n",
    "                                                sh=[]\n",
    "                                                wd=[]\n",
    "                                                arlw=[]\n",
    "                                                for hhh in shap_exp[vv]:\n",
    "                                                    bb=hhh.split(\":\")\n",
    "                                                    arlw.append(bb[0])\n",
    "                                                    if bb[0].isdigit():\n",
    "                                                        for vv1 in Samuser_index:\n",
    "                                                            if int(bb[0])!=int(vv):\n",
    "                                                                if str(vv) in Samuser_index[vv1] or int(vv) in Samuser_index[vv1] and int(bb[0]) in Samuser_index[vv1] or str(bb[0]) in Samuser_index[vv1]:\n",
    "                                                                    #print(vv,bb[0])\n",
    "                                                                    sm.append(bb[0])\n",
    "                                                        for vv1 in H1:\n",
    "                                                            if int(bb[0])!=int(vv):\n",
    "                                                                if str(vv) in H1[vv1] or int(vv) in H1[vv1] and int(bb[0]) in H1[vv1] or str(bb[0]) in H1[vv1]:\n",
    "                                                                    #print(\"Samehotel\")\n",
    "                                                                    #print(vv,bb[0])\n",
    "                                                                    sh.append(bb[0])\n",
    "                                                    else:\n",
    "                                                        wd.append(bb[0])\n",
    "                                                if len(sm)>0:\n",
    "                                                    qrsm[vv]=sm\n",
    "                                                if len(sh)>0:\n",
    "                                                    qrsh[vv]=sh\n",
    "                                                if len(wd)>0:\n",
    "                                                    qrw[vv]=wd\n",
    "                                                if len(arlw)>0:\n",
    "                                                    qrall[vv]=arlw\n",
    "                                            for cv in qrsm:\n",
    "                                                if cv in  qrsh and cv in qrw and cv in qrall:\n",
    "                                                        cnc=qrsm[cv]+qrsh[cv]\n",
    "                                                        cnc1[cv]=cnc\n",
    "                                                        #print(cv,qrsm[cv],qrsh[cv],qrw[cv] )\n",
    "                                                        #print(\"\\n\\n\")\n",
    "\n",
    "                                            vb={}\n",
    "                                            for j in cnc1:\n",
    "                                                gh=[]\n",
    "                                                for jj in cnc1[j]:\n",
    "                                                    if qrat[j]==qrat[int(jj)]:\n",
    "                                                        #print(j,jj,qrat[j],qrat[int(jj)])\n",
    "                                                        #cv=\"(\"+str(j)+\",\"+str(jj)+\")\"\n",
    "                                                        gh.append(jj)\n",
    "                                                vb[j]=gh\n",
    "                                            return vb,qrsm,qrsh,qrw,qrall\n",
    "                                @classmethod\n",
    "                                def draw(cls,vb,qrsm,qrsh,qrw,qrall,ch):\n",
    "                                            if ch==\"Sameuser\":\n",
    "                                                c=0\n",
    "                                                for gh in qrsm:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrsm[gh])\n",
    "                                                        zx1=[0]*len(qrsm[gh])\n",
    "                                                        vcc=qrsm[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Samehotel\":\n",
    "                                                c=0\n",
    "                                                for gh in qrsh:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrsh[gh])\n",
    "                                                        zx1=[0]*len(qrsh[gh])\n",
    "                                                        vcc=qrsh[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Word\":\n",
    "                                                c=0\n",
    "                                                for gh in qrw:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrw[gh])\n",
    "                                                        zx1=[0]*len(qrw[gh])\n",
    "                                                        vcc=qrw[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Justifying_prediction\":\n",
    "                                                    c=0\n",
    "                                                    for gh in vb:\n",
    "                                                        if c<1:\n",
    "                                                            if qrat[gh]==2:\n",
    "                                                                    print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                            else:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "\n",
    "\n",
    "                                                            zx=[0]*len(vb[gh])\n",
    "                                                            zx1=[0]*len(vb[gh])\n",
    "                                                            if qrat[gh]==2:\n",
    "                                                                vcc=[]\n",
    "                                                                for vx in vb[gh]:\n",
    "                                                                    gh1=str(vx)+\"_Positive\"\n",
    "                                                                    vcc.append(gh1)\n",
    "                                                            elif qrat[gh]==0:\n",
    "                                                                vcc=[]\n",
    "                                                                for vx in vb[gh]:\n",
    "                                                                    gh2=str(vx)+\"_Negative\"\n",
    "                                                                    vcc.append(gh2)\n",
    "                                                            #print(vcc)\n",
    "                                                            for jk in range(0,len(zx)):\n",
    "                                                                if  qrat[gh]==2:\n",
    "                                                                    vzz=str(gh)+\"_Positive\"\n",
    "                                                                    zx[jk]=vzz\n",
    "                                                                    zx1[jk]=vcc[jk]\n",
    "                                                                elif  qrat[gh]==0:\n",
    "                                                                    vzz=str(gh)+\"_Negative\"\n",
    "                                                                    zx[jk]=vzz\n",
    "                                                                    zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                            df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                            # Build your graph\n",
    "                                                            G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                            # Custom the nodes:\n",
    "                                                            fig = plt.figure()\n",
    "                                                            nx.draw(G, with_labels=True, node_color='skyblue', node_size=8000, edge_color='white')\n",
    "                                                            fig.set_facecolor(\"#00000F\")\n",
    "                                                            c=c+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                    \n",
    "                WORDS,WORDS22,qrat,H1,ann,Rev_text_map,s_words,stopwords,WORDS1,r_wts,w11,trg1,H1,sent,documents,final_clu,Samuser_index=relational_lime.run_all()\n",
    "                similar_r_map,wg=relational_lime.relational_embedding_exp(5,WORDS22,WORDS1,Samuser_index,H1)\n",
    "                shap_exp,fr,shap_values=relational_lime.shap_rel(similar_r_map,wg,WORDS22,WORDS1,qrat,Samuser_index,H1)\n",
    "               # vb,qrsm,qrsh,qrw,qrall=relational_lime.organize_results_visualization(Samuser_index,H1,qrat,shap_exp)\n",
    "                # Graph Representation of the Explanation: \n",
    "                #Choices: \n",
    "                #'Samehotel' or 'Sameuser' for relational explanation\n",
    "                # 'Word' for the Word explanation graph\n",
    "                #'Justifying_prediction' to show the graph connected with \n",
    "                #review that have the same class as the review class\n",
    "                ch=['Sameuser','Samehotel','Word','Justifying_prediction']\n",
    "                for kk in ch:\n",
    "                        pass#relational_lime.draw(vb,qrsm,qrsh,qrw,qrall,kk)\n",
    "                return shap_exp,fr,shap_values,ann,similar_r_map,WORDS,Samuser_index,H1,qrat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501ef492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shap_accuracy():\n",
    "    shap_exp,fr,shap_values,ann,similar_r_maps,WORDS,Samuser_index,H1,qrat=Run_program()\n",
    "    shap_word_exp={}\n",
    "    shap_rel_exp={}\n",
    "    for hh in shap_exp:\n",
    "        gh=[]\n",
    "        gh1=[]\n",
    "        cc=0\n",
    "        cc1=0\n",
    "        for hh1 in shap_exp[hh]:\n",
    "            if hh1.isdigit()==True:\n",
    "                if cc<5:\n",
    "                    gh.append(hh1)\n",
    "                    cc=cc+1\n",
    "            else:\n",
    "                if cc1<5:\n",
    "                    gh1.append(hh1)\n",
    "                    cc1=cc1+1\n",
    "        shap_word_exp[hh]=gh1\n",
    "        shap_rel_exp[hh]=gh\n",
    "    #words accuracy\n",
    "    wordacc={}\n",
    "    for kk in shap_word_exp:\n",
    "        c=0\n",
    "        vx=0\n",
    "        if kk in ann:\n",
    "            for vc in shap_word_exp[kk]:\n",
    "                if vc in ann[kk]:\n",
    "                    if vx<5:\n",
    "                        c=c+1\n",
    "                        vx=vx+1\n",
    "            try:\n",
    "                gg=c/float(len(shap_word_exp[kk]))\n",
    "                if gg>0:\n",
    "                        wordacc[kk]=gg\n",
    "            except:\n",
    "                continue \n",
    "    #average word exp accuracy\n",
    "\n",
    "    svv=0\n",
    "    for t in wordacc:\n",
    "            svv=svv+float(wordacc[t])\n",
    "    print(\"Word_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "    print(svv/float(len(wordacc)))\n",
    "\n",
    "    #relational explanation accuracy\n",
    "    relacc={}\n",
    "\n",
    "    for kk in shap_rel_exp:\n",
    "        c=0\n",
    "        cn=0\n",
    "        if kk in similar_r_maps and kk in ann:\n",
    "            for vc in shap_rel_exp[kk]:\n",
    "                if vc in similar_r_maps[kk] or int(vc) in similar_r_maps[kk] or str(vc) in similar_r_maps[kk]:\n",
    "                    if cn<5:\n",
    "                        c=c+1\n",
    "                        cn=cn+1\n",
    "            try:\n",
    "                    gg=c/float(len(shap_rel_exp[kk]))\n",
    "            except:\n",
    "                continue \n",
    "            if gg>0:\n",
    "                relacc[kk]=gg\n",
    "    #average relational exp accuracy\n",
    "    svv=0\n",
    "    for t in relacc:\n",
    "            svv=svv+float(relacc[t])\n",
    "    print(\"\\n\")\n",
    "    print(\"Relational_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "    print(svv/float(len(relacc)))\n",
    "    return shap_exp,qrat\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df945ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khanfarabi\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n",
      "<ipython-input-1-82a4b1f9094d>:342: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X = model[model.wv.vocab]\n",
      "<ipython-input-1-82a4b1f9094d>:512: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  gh1=model.similarity(v,v1)\n",
      "<ipython-input-1-82a4b1f9094d>:641: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  gh=model.most_similar(positive=str(k),topn=600)\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Explanation_Average_Accuracy_Shap\n",
      "\n",
      "0.2878205128205125\n",
      "\n",
      "\n",
      "Relational_Explanation_Average_Accuracy_Shap\n",
      "\n",
      "0.7395638629283494\n"
     ]
    }
   ],
   "source": [
    "shap_exp,qrat=shap_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9381118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from statistics import stdev\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import transformers\n",
    "import shap\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "pred_per_patch={}\n",
    "shap_exp_w={}\n",
    "shap_exp_r={}\n",
    "for v in shap_exp:\n",
    "    gh=[]\n",
    "    gh1=[]\n",
    "    for kk in shap_exp[v]:\n",
    "        if kk.isdigit()==True:\n",
    "            gh.append(kk)\n",
    "        elif kk.isdigit()==False:\n",
    "            gh1.append(kk)\n",
    "    shap_exp_w[v]=gh1\n",
    "    shap_exp_r[v]=gh\n",
    "\n",
    "evid_per_batch_tr={}\n",
    "evid_per_batch_tg={}\n",
    "evid_per_batch_all={}\n",
    "def train_exp(n,shap_exp_w,shap_exp_r,qrat):\n",
    "    train={}\n",
    "    for vv in shap_exp_r:\n",
    "        evid=[]\n",
    "        rl=math.ceil(len(shap_exp_r[vv])*n)\n",
    "        wl=math.ceil(len(shap_exp_w[vv])*n)\n",
    "        if rl==0 or wl==0:\n",
    "            continue\n",
    "        else:\n",
    "                for xz in range(0,rl):\n",
    "                    evid.append(shap_exp_r[vv][xz])\n",
    "                for xz1 in range(0,wl):\n",
    "                    evid.append(shap_exp_w[vv][xz1])\n",
    "                #print(vv,evid)\n",
    "                train[vv]=evid\n",
    "    tr=[]\n",
    "    tg=[]\n",
    "    for gg in train:\n",
    "        s=''\n",
    "        for gg1 in train[gg]:\n",
    "            #s=s+str(gg1)+\" \"\n",
    "            tr.append(gg1)\n",
    "            tg.append(qrat[gg])\n",
    "    return tr,tg,train\n",
    "    \n",
    "    \n",
    "for dd in range(1,11):\n",
    "    tr,tg,train=train_exp(dd/10,shap_exp_w,shap_exp_r,qrat)\n",
    "    evid_per_batch_tr[dd]=tr\n",
    "    evid_per_batch_tg[dd]=tg\n",
    "    evid_per_batch_all[dd]=train\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056df134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating MLN using our explanation\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "def mln_gn(trr,lm1):\n",
    "            rt=trr/10.0\n",
    "            hh=\"mln_\"+str(trr)+\".txt\"\n",
    "            hh1=\"evid\"+str(trr)+\".txt\"\n",
    "            hh2=\"qr\"+str(trr)+\".txt\"\n",
    "            path=\"/Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/Folder_Shap\"+str(trr)\n",
    "            if not os.path.exists(path):\n",
    "                                    os.makedirs(path)\n",
    "            fm1=open(os.path.join(path,hh), 'w')\n",
    "            fe1=open(os.path.join(path,hh1), 'w')\n",
    "            fr1=open(os.path.join(path,hh2), 'w')\n",
    "            bvv=\"Same(x,y)\"\n",
    "            bvv1=\"Samehotel(x,y)\"\n",
    "            z1=\"negative(x)\"\n",
    "            z2=\"positive(x)\"\n",
    "            z3=\"negative(y)\"\n",
    "            z4=\"positive(y)\"\n",
    "            z5=\"negative\"\n",
    "            z6=\"positive\"\n",
    "            fr1.write(str(z6)+\"\\n\")\n",
    "            fr1.write(str(z5)+\"\\n\")\n",
    "            fr1.close()\n",
    "            \n",
    "            fm1.write(str(bvv)+\"\\n\")\n",
    "            #fm1.write(str(bvv1)+\"\\n\")\n",
    "            fm1.write(str(z1)+\"\\n\")\n",
    "            fm1.write(str(z2)+\"\\n\")\n",
    "            vccc=[]\n",
    "            fvc=[]\n",
    "            ev=[]\n",
    "            for kk in lm1[trr]:\n",
    "                for kk1 in lm1[trr][kk]:\n",
    "                    if kk1.isdigit()==False:\n",
    "                        gg=\"Has_\"+str(kk1)+\"(x)\"\n",
    "                        gg2=\"Has_\"+str(kk1)+\"(\"+str(kk)+\")\"\n",
    "                        gg1=\"0.75 !Has_\"+str(kk1)+\"(x)\"+\" v \"+\"positive(x)\"\n",
    "                        g1=\"0.75 !Has_\"+str(kk1)+\"(x)\"+\" v \"+\"negative(x)\"\n",
    "                        if gg not in vccc:\n",
    "                            vccc.append(gg)\n",
    "                        if gg1 not in fvc:\n",
    "                            fvc.append(gg1)\n",
    "                        if g1 not in fvc:\n",
    "                            fvc.append(g1)\n",
    "                        if gg2 not in ev:\n",
    "                            ev.append(gg2)\n",
    "                    else:\n",
    "                        gg3=\"Same(\"+str(kk)+\",\"+str(kk1)+\")\"\n",
    "                        if gg3 not in ev:\n",
    "                            ev.append(gg3)\n",
    "            for zz in vccc:\n",
    "                fm1.write(str(zz)+\"\\n\")\n",
    "            fm1.write(\"\\n\")\n",
    "            for zx in fvc:\n",
    "                fm1.write(str(zx)+\"\\n\")\n",
    "\n",
    "            vc=\"1.0 \"+\"!positive(x)\"+\" v \"+\"!Same(x,y)\"+\" v \"+\"positive(y)\"\n",
    "            vc1=\"1.0 \"+\"!negative(x)\"+\" v \"+\"!Same(x,y)\"+\" v \"+\"negative(y)\"\n",
    "            #bc=\"1.0 \"+\"!positive(x)\"+\" v \"+\"!Samehotel(x,y)\"+\" v \"+\"positive(y)\"\n",
    "            #bc1=\"1.0 \"+\"!negative(x)\"+\" v \"+\"!Samehotel(x,y)\"+\" v \"+\"negative(y)\"\n",
    "            fm1.write(str(vc)+\"\\n\")\n",
    "            fm1.write(str(vc1)+\"\\n\")\n",
    "            #fm1.write(str(bc)+\"\\n\")\n",
    "            #fm1.write(str(bc1)+\"\\n\")\n",
    "            for cs in ev:\n",
    "                fe1.write(str(cs)+\"\\n\")\n",
    "            fe1.close()\n",
    "            fm1.close()\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "def all_exp(lm,evid_per_batch_all):\n",
    "            mln_gn(lm,evid_per_batch_all)\n",
    "for j in range(1,11):\n",
    "        all_exp(j,evid_per_batch_all)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ecc2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['4', 'sushi', 'nickname']\n",
      "5 ['9306', 'often', 'disappointments']\n",
      "6 ['4', 'review', 'one']\n",
      "7 ['7', '8', 'pm', 'one', 'late']\n",
      "8 ['7', '8', 'evening', 'want', 'looks']\n",
      "9 ['7', '8', '9', 'came', 'everything', 'tapas']\n",
      "10 ['7', '8', '9', 'us', 'night', 'east', 'underground', 'quaint', 'environment']\n",
      "11 ['7', '8', '9', 'wow', 'tip', 'place', 'flamenco', 'request', 'dining']\n",
      "13 ['7', '8', 'fun', 'saturday']\n",
      "14 ['1891', '4167', 'traditional', 'spanish', 'head', 'tonight']\n",
      "15 ['7', '8', 'entire', 'city', 'every', 'lover', 'tapas']\n",
      "17 ['7', '8', 'soo', 'kind', 'running']\n",
      "18 ['7', '8', 'say', 'experience', 'tapas', 'staff']\n",
      "19 ['7', '8', '9', 'means', 'east']\n",
      "21 ['21', '24', '25', 'check', 'brotherly']\n",
      "24 ['1835', 'used']\n",
      "25 ['1631', 'april', 'philly']\n",
      "26 ['21', '24', '25', 'pretty']\n",
      "27 ['21', '24', '25', '26', 'reasons']\n",
      "28 ['21', '24', '25', 'stopped', 'several']\n",
      "33 ['1605', 'philly', 'ones']\n",
      "34 ['2962', 'across', 'king']\n",
      "35 ['1525', 'cheesesteaks', 'course']\n",
      "36 ['21', '24', '25', 'business', 'sad']\n",
      "38 ['21', '24', '25', '26', 'obnoxious']\n",
      "39 ['21', '24', '25', '26', 'lets', 'intersection']\n",
      "40 ['21', '24', '25', '26', 'pretty']\n",
      "41 ['21', '24', '25', 'wrong']\n",
      "43 ['21', '24', '25', 'visiting']\n",
      "44 ['3020', 'geno', 'slightly']\n",
      "46 ['21', '24', '25', 'bound']\n",
      "47 ['21', '24', '25', '26', 'trying']\n",
      "48 ['21', '24', '25', '26', 'crunchier']\n",
      "50 ['21', '24', '25', '26', 'sucks', 'pulls', 'faulkner', 'english']\n",
      "51 ['21', '24', '25', '26', 'air']\n",
      "52 ['21', '24', '25', '26', 'making', 'arterial', 'nemesis']\n",
      "54 ['21', '24', '25', '26', 'steaks', 'bigoted', 'discuss', 'food', 'joey']\n",
      "55 ['21', '24', '25', '26', 'cheesesteak']\n",
      "57 ['57', '58', 'gotten', 'times']\n",
      "58 ['2364', 'boyfriend']\n",
      "59 ['1317', '1393', 'wonderful', 'rice']\n",
      "63 ['63', 'christmas', 'loved']\n",
      "65 ['63', 'hubby', 'went', 'cedar']\n",
      "66 ['66', 'always', 'good']\n",
      "67 ['66', 'cheap', 'darts']\n",
      "68 ['66', 'tasty', 'lager']\n",
      "70 ['70', '71', 'rican']\n",
      "71 ['70', '71', 'start', 'saying']\n",
      "72 ['70', '71', 'come', 'teach', 'cute']\n",
      "73 ['70', '71', 'authentic']\n",
      "74 ['70', '71', 'born', 'till']\n",
      "75 ['70', '71', 'ruined', 'standard']\n",
      "76 ['396', '570', 'away']\n",
      "77 ['70', '71', 'puerto', 'twice']\n",
      "78 ['70', '71', 'time', 'sight', 'sabor']\n",
      "82 ['82', '83', '84', 'best']\n",
      "83 ['82', '83', '84', 'always', 'restaurant', 'frequent', 'popular']\n",
      "84 ['82', '83', '84', 'think']\n",
      "85 ['82', '83', '84', 'lot', 'good', 'years', 'booth']\n",
      "86 ['82', '83', '84', 'wait']\n",
      "87 ['82', '83', '84', 'husband', 'came']\n",
      "88 ['82', '83', '84', 'hit']\n",
      "89 ['82', '83', '84', 'staff', 'slices']\n",
      "90 ['82', '83', '84', 'dishes']\n",
      "91 ['82', '83', '84', 'party', 'slow', 'knew']\n",
      "92 ['82', '83', '84', 'fast', 'neighborhood']\n",
      "95 ['82', '83', '84', 'may', 'cookies']\n",
      "96 ['82', '83', '84', 'best', 'diners']\n",
      "97 ['82', '83', '84', 'friendly', 'back']\n",
      "98 ['1185', 'fascination', 'satisfied']\n",
      "99 ['82', '83', '84', 've', 'diner']\n",
      "100 ['82', '83', '84', 'remember', 'new', 'lemon']\n",
      "101 ['82', '83', '84', 'boyfriend', 'went', 'enjoyed']\n",
      "102 ['82', '83', '84', 'kind', 'secret', 'filled']\n",
      "103 ['103', '104', 'ok']\n",
      "104 ['2552', 'first', 'end']\n",
      "106 ['106', 'restaurant']\n",
      "108 ['106', 'appetizers', 'ordered']\n",
      "109 ['109', 'place', 'hot']\n",
      "110 ['2174', '2635', 'meh', 'michigan']\n",
      "112 ['109', 'coworkers']\n",
      "113 ['3322', '3960', 'overboard', 'detail', 'lunch']\n",
      "114 ['109', '110', 'think', 'excellent', 'success', 'experience']\n",
      "115 ['115', 'place']\n",
      "116 ['4823', 'much']\n",
      "118 ['115', '116', 'hearing', 'place']\n",
      "124 ['124', '125', 'classic', 'margaritas']\n",
      "125 ['124', '125', '128', 'mexican', 'sums', 'joint']\n",
      "128 ['124', '125', 'love']\n",
      "129 ['124', '125', '128', 'ever']\n",
      "130 ['124', '125', 'one']\n",
      "131 ['124', '125', 'wow', 'partner', 'las', 'bugambilias', 'began', 'upon']\n",
      "132 ['124', '125', 'flavor']\n",
      "133 ['133', '134', 'greeting', 'vibe', 'roadhouse']\n",
      "134 ['2678', 'sides', 'corn', 'cole', 'fanatic']\n",
      "135 ['133', '134', 'excited']\n",
      "137 ['1712', 'jamaican', 'cheese']\n",
      "138 ['4778', 'back', 'bbq']\n",
      "139 ['11965', 'deal', 'service']\n",
      "140 ['133', '134', 'style']\n",
      "141 ['722', 'bbq', 'night']\n",
      "145 ['145', 'roof', 'deck']\n",
      "146 ['154', 'see']\n",
      "147 ['145', '146', 'town', 'continental', 'right', 'issues', 'seemingly', 'waitress']\n",
      "148 ['154', 'girls', 'decided']\n",
      "154 ['145', 'prices']\n",
      "162 ['145', '146', '147', 'advice', 'continental', 'set', 'levels']\n",
      "163 ['145', 'pricey', 'glass']\n",
      "170 ['145', '146', '147', 'high', 'snooty', 'cigarettes', 'sit']\n",
      "185 ['185', 'given', 'finally']\n",
      "187 ['185', 'entree', 'going']\n",
      "196 ['185', 'trick']\n",
      "220 ['220', 'trying', 'hipster', 'restaurants']\n",
      "228 ['220', 'small']\n",
      "257 ['257', '259', 'take']\n",
      "259 ['257', '259', 'meh', 'crappy']\n",
      "260 ['257', 'dinner', 'date']\n",
      "266 ['257', '259', 'making', 'set']\n",
      "339 ['339', 'someone']\n",
      "343 ['339', 'vegetarian']\n",
      "345 ['339', 'apartment', 'cobble', 'hook']\n",
      "408 ['408', 'hand', 'lox']\n",
      "409 ['408', 'pricey']\n",
      "412 ['412', '437', 'tenders']\n",
      "437 ['412', '437', 'okay']\n",
      "452 ['412', '437', 'hype', 'disappointed', 'overall']\n",
      "474 ['412', 'said', 'crap']\n",
      "475 ['412', '437', 'spots', 'excited', 'walking']\n",
      "566 ['566', 'jack']\n",
      "573 ['566', '573', 'meh']\n",
      "574 ['574', 'become', 'iron']\n",
      "577 ['574', 'mine', 'absolute']\n",
      "602 ['602', '603', 'times', 'bags', 'going']\n",
      "603 ['602', '603', 'couple', 'lady', 'said']\n",
      "604 ['602', 'due', 'probably']\n",
      "609 ['602', '603', 'times']\n",
      "626 ['626', 'stout', 'whatever', 'stuff']\n",
      "628 ['626', 'try']\n"
     ]
    }
   ],
   "source": [
    "#for vb in evid_per_batch_all:\n",
    "\n",
    "for kk in evid_per_batch_all[1]:\n",
    "    print(kk,evid_per_batch_all[1][kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67e4c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    }
   ],
   "source": [
    "def reverse_accuracy(m,tr11,tg11):\n",
    "            stacc=[]\n",
    "            for zx in range(0,10):\n",
    "                    test=[]\n",
    "                    trg=[]\n",
    "                    corpus_train, corpus_test, y_train, y_test = train_test_split(tr11,tg11, test_size=0.4, random_state=7)\n",
    "                    ltr=math.ceil(len(tr11)*0.5)\n",
    "                    for vb in range(0,ltr):\n",
    "                        test.append(tr11[vb])\n",
    "                    for vb1 in range(0,ltr):\n",
    "                        trg.append(tg11[vb1])\n",
    "                    vectorizer = TfidfVectorizer(min_df=1)\n",
    "                    X_train = vectorizer.fit_transform(corpus_train)\n",
    "                    X_test = vectorizer.transform(corpus_test)\n",
    "                    model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                    model.fit(X_train,y_train)\n",
    "                    p = model.predict(X_test)\n",
    "                    acc=f1_score(y_test, p, average='weighted')\n",
    "                    stacc.append(float(acc))\n",
    "            return stacc\n",
    "for vc2 in evid_per_batch_tg:\n",
    "        stacc=reverse_accuracy(vc2,evid_per_batch_tr[vc2],evid_per_batch_tg[vc2])\n",
    "        pred_per_patch[vc2]=stacc\n",
    "       # print(vc,evid_per_batch_tr[vc],evid_per_batch_tg[vc],len(evid_per_batch_tr[vc]),len(evid_per_batch_tg[vc]))\n",
    "       # print(\"\\n\\n\")\n",
    "for vzz in pred_per_patch:\n",
    "    pass#print(vzz,pred_per_patch[vzz])\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c81d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321, 0.7305061790316321]\n",
      "2 [0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172, 0.7464712036565172]\n",
      "3 [0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748, 0.7563385725259748]\n",
      "4 [0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552, 0.7291529564029552]\n",
      "5 [0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893, 0.7633841348386893]\n",
      "6 [0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435, 0.7536597963519435]\n",
      "7 [0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803, 0.7597221505071803]\n",
      "8 [0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197, 0.7294677529571197]\n",
      "9 [0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982, 0.7218736169496982]\n",
      "10 [0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368, 0.7213547114487368]\n"
     ]
    }
   ],
   "source": [
    "for vzz in pred_per_patch:\n",
    "    print(vzz,pred_per_patch[vzz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30090530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10065f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
