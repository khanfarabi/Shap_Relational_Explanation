{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d096d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pre-process the Reviews /Users/khanfarabi/OneDrive - The University of Memphis/Explain_MLN/Explanation_yelp/\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn import svm\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "import gensim \n",
    "import operator\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def Run_program():\n",
    "                class relational_lime:\n",
    "                                @classmethod\n",
    "                                def run_all(cls):\n",
    "\n",
    "                                                def data_pr():\n",
    "                                                            ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                            revid = 0\n",
    "                                                            users = defaultdict(list)\n",
    "                                                            for ln in ifile1:\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                users[parts[0]].append(revid)\n",
    "                                                                revid = revid + 1\n",
    "                                                            ifile1.close()\n",
    "                                                            #pass#pass#print(users)\n",
    "                                                            H11=defaultdict(list)\n",
    "                                                            #sys.exit()\n",
    "                                                            userids = []\n",
    "                                                            #pass#pass#print(users)\n",
    "                                                            c =0\n",
    "                                                            #Select reviewer subset based on tunable parameters (max-reviews and min-reviews limit,sampling ratio)\n",
    "                                                            minreviews =5\n",
    "                                                            maxreviews =20\n",
    "                                                            samplingratio =0.15\n",
    "                                                            for u in users:\n",
    "                                                                if len(users[u])>minreviews and len(users[u])<maxreviews:        \n",
    "                                                                    if random.random() < samplingratio:\n",
    "                                                                        userids.append(u)\n",
    "                                                                        c= c + len(users[u])\n",
    "\n",
    "                                                            ifile = open(\"reviewContent.txt\",encoding='ISO-8859-1')\n",
    "                                                            ifile1 = open(\"full-meta-data.txt\")\n",
    "                                                            s_words = []\n",
    "                                                            sfile = open(\"Words.txt\")\n",
    "                                                            for ln in sfile:\n",
    "                                                                s_words.append(ln.strip())\n",
    "                                                            sfile.close()\n",
    "                                                            stopwords = []\n",
    "                                                            sfile = open(\"stopwords.txt\")\n",
    "                                                            for ln in sfile:\n",
    "                                                                stopwords.append(ln.strip())\n",
    "                                                            sfile.close()\n",
    "\n",
    "                                                            flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n",
    "                                                                     else 0)\n",
    "                                                            ofile = open(\"all_revs1.txt\",'w')\n",
    "                                                            ofile1 = open(\"metadata.txt\",'w')\n",
    "                                                            cnt = 0\n",
    "                                                            revid = 0\n",
    "                                                            qrat={}\n",
    "                                                            windex = defaultdict(list)\n",
    "                                                            #Tunable parameter to keep non-sentiment words\n",
    "                                                            PNonSentWords = 0.30\n",
    "                                                            WORDS={}\n",
    "                                                            w_per_ht=defaultdict(list)\n",
    "                                                            w_per_user=defaultdict(list)\n",
    "                                                            Rev_text_map={}\n",
    "                                                            for ln in ifile:\n",
    "                                                                ln1 = ifile1.readline()\n",
    "                                                                parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                                #pass#pass#print(parts1)\n",
    "                                                                if parts1[0] not in userids:\n",
    "                                                                    continue\n",
    "                                                                #if cnt >= 10000:\n",
    "                                                                #    break\n",
    "                                                                keep = []\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                for word in re.findall(r\"\\w[\\w']*\", parts[3], flags=flags):\n",
    "                                                                    if word.isdigit() or len(word)==1:\n",
    "                                                                        continue\n",
    "                                                                    word_lower = word.lower()\n",
    "                                                                    if word_lower in stopwords:\n",
    "                                                                        continue\n",
    "                                                                   # if word_lower in s_words:\n",
    "                                                                        #keep.append(word_lower)\n",
    "                                                                    elif random.random() < PNonSentWords:\n",
    "                                                                        if not any(c.isdigit() for c in word_lower) and \"'\" not in word_lower:\n",
    "                                                                            keep.append(word_lower)\n",
    "                                                                if float(parts1[2])<=2:\n",
    "                                                                    cl = 0\n",
    "                                                                elif float(parts1[2])==3:\n",
    "                                                                    cl = 1\n",
    "                                                                elif float(parts1[2])>=4:\n",
    "                                                                    cl = 2\n",
    "                                                                if len(keep)>=10:\n",
    "                                                                    cnt = cnt + 1\n",
    "                                                                    ofile.write(\" \".join(keep)+\"\\t\"+str(cl)+\"\\n\")\n",
    "                                                                    WORDS[revid]=keep\n",
    "\n",
    "                                                                    qrat[revid]=cl\n",
    "                                                                    Rev_text_map[revid]=parts[3]\n",
    "                                                                    H11[parts1[1]].append(revid)\n",
    "                                                                    ofile1.write(ln1)\n",
    "                                                                    for w in keep:\n",
    "                                                                        windex[w].append(revid)\n",
    "                                                                        w_per_ht[w].append(parts1[1])\n",
    "                                                                        w_per_user[w].append(parts1[0])\n",
    "                                                                    revid = revid + 1\n",
    "                                                            ofile.close()\n",
    "                                                            ofile1.close()\n",
    "                                                            ifile.close()\n",
    "                                                            ifile1.close()\n",
    "\n",
    "                                                            #Tunable parameter (keep words only if repeated in > NumReps reviews)\n",
    "                                                            NumReps = 10\n",
    "\n",
    "                                                            #Filter review words\n",
    "                                                            ifile = open(\"all_revs1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                            ofile = open(\"processed_revs_1.txt\",'w')\n",
    "                                                            for ln in ifile:\n",
    "                                                                parts = ln.strip().split(\"\\t\")\n",
    "                                                                keep = []\n",
    "                                                                for w in parts[0].split(\" \"):\n",
    "                                                                    if len(windex[w])>NumReps:\n",
    "                                                                        keep.append(w)\n",
    "                                                                ofile.write(\" \".join(keep)+\"\\t\"+parts[1]+\"\\n\")\n",
    "                                                            ofile.close()\n",
    "                                                            ifile.close()\n",
    "                                                            pass#pass#print(\"Total Reviews=\"+str(len(WORDS)))\n",
    "                                                            return WORDS,qrat,H11,Rev_text_map,s_words,stopwords\n",
    "                                                def data_balancing(WORDS):\n",
    "                                                    WORDS1={}\n",
    "                                                    t1=[]\n",
    "                                                    t2=[]\n",
    "                                                    t3=[]\n",
    "                                                    c1=0\n",
    "                                                    c2=0\n",
    "                                                    c3=0\n",
    "                                                    for y in WORDS:\n",
    "                                                        if qrat[y]==0:\n",
    "                                                            if c3<1000:\n",
    "                                                                t1.append(y)\n",
    "                                                                WORDS1[y]=WORDS[y]\n",
    "                                                                c3=c3+1\n",
    "                                                        elif qrat[y]==1:\n",
    "                                                            continue\n",
    "                                                            #if c1<515:\n",
    "                                                                #t2.append(y)\n",
    "                                                                #WORDS1[y]=WORDS[y]\n",
    "                                                                #c1=c1+1\n",
    "                                                        elif qrat[y]==2:\n",
    "                                                            if c2<1000:\n",
    "                                                                t3.append(y)\n",
    "                                                                WORDS1[y]=WORDS[y]\n",
    "                                                                c2=c2+1\n",
    "                                                    pass#pass#print(len(t1),len(t2),len(t3),len(WORDS1))\n",
    "                                                    return WORDS1\n",
    "\n",
    "                                                def train_data_gen(WORDS1):\n",
    "                                                    #train and target data\n",
    "                                                    w11=[]\n",
    "                                                    trg11=[]\n",
    "                                                    w12=[]\n",
    "                                                    trg1=[]\n",
    "                                                    for tt in WORDS1:\n",
    "                                                        s=' '\n",
    "                                                        #if qrat[tt]!=1:\n",
    "                                                        for kk in WORDS1[tt]:\n",
    "                                                           # if qrat[tt]!=1:\n",
    "                                                                w11.append(kk)\n",
    "                                                                s=str(kk)+s+\"\\t\"\n",
    "                                                                #pass#pass#print(kk)\n",
    "                                                                if float(qrat[tt])==2:\n",
    "                                                                        trg11.append(1)\n",
    "                                                                else:\n",
    "                                                                        trg11.append(0)\n",
    "                                                            #w12.append(s)\n",
    "                                                            #trg1.append(qrat[tt])\n",
    "\n",
    "                                                    return w11,trg11\n",
    "                                                def svm_coeff(w11,trg):\n",
    "                                                                    #SVM Learner and generate feature weights\n",
    "                                                                    #OutputCodeClassifier(LinearSVC(random_state=0),code_size=2, random_state=0)\n",
    "                                                                    #Learn SVM Model\n",
    "                                                                    #ifile = open(\"processed_revs_1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                                    Y = trg\n",
    "                                                                    words =w11\n",
    "                                                                    unique_words =[]\n",
    "                                                                    ss=set(words)\n",
    "                                                                    for w1 in ss:\n",
    "                                                                            if w1 not in unique_words:\n",
    "                                                                                unique_words.append(w1)\n",
    "\n",
    "                                                                    '''\n",
    "                                                                    #tf_transformer = TfidfVectorizer()\n",
    "                                                                    #f = tf_transformer.fit_transform(words)\n",
    "                                                                    #features = [((i, j), f[i,j]) for i, j in zip(*f.nonzero())]\n",
    "                                                                    #unique_word_ids = []\n",
    "                                                                    #for w in unique_words:\n",
    "                                                                       # i = tf_transformer.vocabulary_.get(w)\n",
    "                                                                        #unique_word_ids.append(i)\n",
    "\n",
    "                                                                    #clf =OneVsRestClassifier(SVC(kernel='linear'))#svm.LinearSVC(C=1)\n",
    "                                                                    #clf.fit(f,Y)\n",
    "                                                                    #p = clf.predict(f)\n",
    "                                                                    #pass#pass#print(f1_score(Y,p,average='weighted'))\n",
    "\n",
    "                                                                    '''\n",
    "                                                                    tf_transformer = TfidfVectorizer()\n",
    "                                                                    f = tf_transformer.fit_transform(words)\n",
    "                                                                    features = [((i, j), f[i,j]) for i, j in zip(*f.nonzero())]\n",
    "                                                                    unique_word_ids = []\n",
    "                                                                    for w in unique_words:\n",
    "                                                                        i = tf_transformer.vocabulary_.get(w)\n",
    "                                                                        unique_word_ids.append(i)\n",
    "\n",
    "                                                                    #clf =svm.LinearSVC(C=100,probability=True)\n",
    "                                                                    clf=svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', random_state=None)\n",
    "                                                                    #svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "                                                                    #OneVsRestClassifier(LinearSVC(random_state=0)) #svm.LinearSVC(C=1)\n",
    "                                                                    clf.fit(f,Y)\n",
    "                                                                    #clf.fit(f,Y)\n",
    "                                                                    p = clf.predict(f)\n",
    "                                                                    pass#pass#print(f1_score(Y,p,average='micro'))\n",
    "                                                                    from sklearn.metrics import accuracy_score\n",
    "                                                                    pass#pass#print(accuracy_score(Y,p))\n",
    "\n",
    "                                                                    #Store learned weights\n",
    "\n",
    "                                                                    #Tunable parameter, normalization range of weights\n",
    "                                                                    rangelower=0\n",
    "                                                                    rangehigher=1\n",
    "\n",
    "                                                                    C = clf.coef_\n",
    "                                                                    scaler = MinMaxScaler(feature_range=(rangelower,rangehigher))\n",
    "                                                                    vals = []\n",
    "                                                                    for i, j in zip(*C.nonzero()):\n",
    "                                                                        vals.append([C[i,j]])\n",
    "                                                                    scaler.fit(vals)\n",
    "                                                                    V1 = scaler.transform(vals)\n",
    "                                                                    rows,cols = C.nonzero()\n",
    "                                                                    r_wts =  defaultdict(list)\n",
    "                                                                    ix= 0\n",
    "                                                                    for w in unique_words:\n",
    "                                                                        r_wts[w] = [0]  \n",
    "                                                                    for i, j in zip(*C.nonzero()):\n",
    "                                                                        #pass#pass#print(i,j)\n",
    "                                                                        for k in tf_transformer.vocabulary_.keys():\n",
    "                                                                            if tf_transformer.vocabulary_[k]==j:\n",
    "                                                                                if k not in r_wts:\n",
    "                                                                                    break\n",
    "                                                                                else:\n",
    "                                                                                    #if float(V1[ix][0])>0:\n",
    "                                                                                    r_wts[k][i] = V1[ix][0]\n",
    "                                                                                    break\n",
    "                                                                        ix = ix + 1\n",
    "\n",
    "\n",
    "                                                                    #pass#pass#print(r_wts)\n",
    "                                                                    return r_wts\n",
    "                                                #  Extracting Samehote Relation\n",
    "                                                def sm_h(WORDS1,H11):\n",
    "                                                    H1={}\n",
    "                                                    fl=open(\"samehote.txt\",\"w\")\n",
    "                                                    for tt in H11:\n",
    "                                                        gh=[]\n",
    "                                                        for kk in H11[tt]:\n",
    "                                                            if kk in WORDS1:\n",
    "                                                                if kk not in gh:\n",
    "                                                                    gh.append(str(kk))\n",
    "                                                        if len(gh)>1:\n",
    "                                                            H1[tt]=gh\n",
    "                                                            ggg=str(tt)+\"::\"+str(gh)\n",
    "                                                            fl.write(str(ggg)+\"\\n\")\n",
    "                                                    fl.close()       \n",
    "                                                    for t in H1:\n",
    "                                                        pass#pass#pass#print(t,H1[t])   \n",
    "                                                    #pass#pass#print(len(H1))\n",
    "                                                    return H1\n",
    "                                                #Sentance generation  for Neural Word2Vec Embedding Training\n",
    "                                                def snt_emd(WORDS1):\n",
    "                                                    sent=[]\n",
    "                                                    sent1=[]\n",
    "                                                    sent_map=defaultdict(list)\n",
    "                                                    for ty in WORDS1:\n",
    "                                                        gh=[]\n",
    "                                                        gh.append(str(ty))\n",
    "                                                        #gh1=[]\n",
    "                                                        #gh2=[]\n",
    "                                                        for j in WORDS1[ty]:\n",
    "                                                            j1=str(j)\n",
    "                                                            #gh.append(str(ty))\n",
    "                                                            if j1 not in gh:\n",
    "                                                                gh.append(j1)\n",
    "\n",
    "                                                        if gh not in sent:\n",
    "                                                                sent.append(gh)       \n",
    "                                                    documents=[]\n",
    "                                                    #documents1=[]\n",
    "                                                    for t in sent:\n",
    "                                                        for jh in t:\n",
    "                                                                 documents.append(jh)\n",
    "                                                    return sent,documents\n",
    "                                                # Clustering the queries\n",
    "                                                def kmean_cls(sent,documents,WORDS1):\n",
    "\n",
    "                                                        model = Word2Vec(sent, min_count=1)\n",
    "                                                        X = model[model.wv.vocab]\n",
    "                                                        NUM_CLUSTERS=5\n",
    "                                                        kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                                                        assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                                                        #pass#pass#print (assigned_clusters)\n",
    "                                                        cluster={}\n",
    "                                                        words = list(model.wv.vocab)\n",
    "                                                        for i, word in enumerate(words):\n",
    "                                                                gh=[] \n",
    "                                                                gh1=[] \n",
    "                                                                gh2=[] \n",
    "                                                                if word.isdigit(): \n",
    "                                                                    cluster[word]=assigned_clusters1[i]\n",
    "\n",
    "                                                        cluster_final={}\n",
    "                                                        for j in range(NUM_CLUSTERS):\n",
    "                                                            gg=[]\n",
    "                                                            for tt in cluster:\n",
    "                                                                if int(cluster[tt])==int(j):\n",
    "                                                                    if tt not in gg:\n",
    "                                                                        gg.append(tt)\n",
    "                                                            if len(gg)>0:\n",
    "                                                                        cluster_final[j]=gg\n",
    "                                                        cc=0\n",
    "                                                        final_clu={}\n",
    "                                                        for t in cluster_final:\n",
    "                                                            ghh=[]\n",
    "                                                            for k in cluster_final[t]:\n",
    "                                                                if int(k) in WORDS1:\n",
    "                                                                       ghh.append(int(k))\n",
    "                                                            if len(ghh)>=2:\n",
    "                                                                    final_clu[cc]=ghh\n",
    "                                                                    cc=cc+1\n",
    "                                                        s=0\n",
    "                                                        for k in final_clu:\n",
    "                                                              s=s+len(final_clu[k])\n",
    "                                                       # pass#pass#print(s)\n",
    "                                                        return final_clu\n",
    "\n",
    "                                                def sm_user():\n",
    "                                                    #Store similarity relations based on if they were written by same user\n",
    "                                                        ifile = open(\"processed_revs_1.txt\",encoding=\"ISO-8859-1\")\n",
    "                                                        ifile1 = open(\"metadata.txt\")\n",
    "                                                        SIM = collections.defaultdict(list)\n",
    "                                                        userindex = {}\n",
    "                                                        Sho= collections.defaultdict(list)\n",
    "                                                        posrev=[]\n",
    "                                                        negrev=[]\n",
    "                                                        hotelindex={}\n",
    "                                                        evida=[]\n",
    "                                                        revid = 0\n",
    "                                                        for ln in ifile:\n",
    "                                                            parts = ln.strip().split(\"\\t\")\n",
    "                                                            #if float(parts[1])==2:\n",
    "                                                               # ppr=\"Positive(\"+str(revid)+\")\"\n",
    "                                                                #posrev.append(ppr)\n",
    "                                                                #if ppr not in evida:\n",
    "                                                                    #pass#evida.append(ppr)\n",
    "                                                            #elif float(parts[1])==0:\n",
    "                                                                #ppr1=\"Negative(\"+str(revid)+\")\"\n",
    "                                                                #negrev.append(ppr1)\n",
    "                                                                #if ppr1 not in evida:\n",
    "                                                                   # pass#evida.append(ppr1)\n",
    "                                                            ln1 = ifile1.readline()\n",
    "                                                            parts1 = ln1.strip().split(\"\\t\")\n",
    "                                                           # pass#pass#print(parts1)\n",
    "                                                            SIM[parts1[0]].append(revid)\n",
    "                                                            Sho[parts1[1]].append(revid)\n",
    "                                                            userindex[revid] = parts1[0]\n",
    "                                                            hotelindex[revid]=parts1[1]\n",
    "                                                            revid = revid + 1    \n",
    "                                                        ifile.close()\n",
    "                                                        ifile1.close()\n",
    "                                                        #pass#pass#print(SIM)\n",
    "                                                        Samuser_index={}\n",
    "                                                        for t in SIM:\n",
    "                                                            gh=[]\n",
    "                                                            for k in SIM[t]:\n",
    "                                                                if len(SIM[t])>0:\n",
    "                                                                    for e in SIM[t]:\n",
    "                                                                        if e not in gh and e in WORDS:\n",
    "                                                                            gh.append(str(e))\n",
    "                                                            if gh!=[]:\n",
    "                                                                Samuser_index[t]=gh\n",
    "                                                        for d in Samuser_index:\n",
    "                                                            pass#pass#pass#print(d,Samuser_index[d])          \n",
    "                                                        #pass#pass#print(len(Samuser_index))\n",
    "                                                        return Samuser_index\n",
    "                                                #Annotated Word exp Used for validation\n",
    "                                                def annotation_wordexp(WORDS1,s_words,stopwords):\n",
    "                                                    ann1={}\n",
    "                                                    c=0\n",
    "                                                    for k in WORDS1:\n",
    "                                                        if qrat[k]==2:\n",
    "                                                            c=c+1\n",
    "                                                            c2=0\n",
    "                                                            gff=[]\n",
    "                                                            for gg in WORDS1[k]:\n",
    "                                                                if gg in s_words:\n",
    "                                                                    if c2<25:\n",
    "                                                                             gff.append(gg)\n",
    "                                                                             c2=c2+1\n",
    "                                                            if len(gff)>0:\n",
    "                                                               # if k in WORDSt:\n",
    "                                                                    ann1[k]=gff\n",
    "\n",
    "                                                        elif qrat[k]==0:\n",
    "                                                            c=c+1\n",
    "                                                            c3=0\n",
    "                                                            gff1=[]\n",
    "                                                            for gg in WORDS1[k]:\n",
    "                                                                if gg in s_words:\n",
    "                                                                    if c3<25:\n",
    "                                                                            gff1.append(gg)\n",
    "                                                                            c3=c3+1\n",
    "\n",
    "                                                            if len(gff1)>0:\n",
    "                                                                #if k WORDSt:\n",
    "                                                                    ann1[k]=gff1\n",
    "                                                    ann={}\n",
    "                                                    for t in ann1:\n",
    "                                                        if t in WORDS1:\n",
    "                                                            ann[t]=ann1[t]\n",
    "                                                    pass#pass#print(len(ann))\n",
    "                                                    gg=open(\"Review_Word_annotation.txt\",\"w\")\n",
    "                                                    for t in ann:\n",
    "                                                        vv=str(t)+\":\"+str(ann[t])\n",
    "                                                        gg.write(str(vv)+\"\\n\")\n",
    "                                                        pass#pass#print(t,ann[t])\n",
    "                                                    gg.close()\n",
    "                                                    return ann\n",
    "                                                def feedback_gen(final_clu):\n",
    "                                                    d_tt={}\n",
    "                                                    d_tt[0]=\"negative\"\n",
    "                                                    d_tt[2]=\"positive\"\n",
    "                                                    wr={}\n",
    "                                                    w=[]\n",
    "                                                    for k in final_clu:\n",
    "                                                            #c=-1\n",
    "                                                            #c=c+1\n",
    "                                                            md=int(len(final_clu[k])/2)\n",
    "                                                            c=0      \n",
    "                                                            k1= final_clu[k][md+c]\n",
    "                                                            #pass#pass#print(k1,md)        \n",
    "                                                            if k1 in ann:\n",
    "                                                                        for k3 in ann[k1]:\n",
    "                                                                                w.append(k3)\n",
    "                                                            else:\n",
    "                                                                c=c+11\n",
    "                                                                continue \n",
    "\n",
    "\n",
    "                                                            #pass#pass#print(k,k1,md,d_tt[qrat[k1]],w)\n",
    "                                                            wr[k1]=w\n",
    "                                                    #pass#pass#print(w)\n",
    "                                                    return w\n",
    "\n",
    "                                                #Update Evidence Based on manual annotation Update 2 \n",
    "\n",
    "                                                def update_evid_annotation(w,WORDS1):\n",
    "                                                                model = Word2Vec(sent, min_count=1)\n",
    "                                                                data_g={}\n",
    "                                                                for t in WORDS1:\n",
    "                                                                    chu=[]\n",
    "                                                                    #try:\n",
    "                                                                    vb={}\n",
    "                                                                    for v in w:\n",
    "                                                                        vb1={}\n",
    "                                                                        for v1 in WORDS1[t]:\n",
    "                                                                                #pass#pass#print(v1,v)\n",
    "                                                                                gh1=model.similarity(v,v1)\n",
    "                                                                                if gh1>=0.1:\n",
    "                                                                                      vb1[v1]=float(gh1)\n",
    "                                                                                      #pass#pass#print(gh1)\n",
    "                                                                        for jk in vb1:\n",
    "                                                                            if jk in vb:\n",
    "                                                                                if float(vb1[jk])>=float(vb[jk]):\n",
    "                                                                                    #pass#pass#print(jk,vb1[jk],vb[jk])\n",
    "                                                                                    vb[jk]=vb1[jk]\n",
    "                                                                            else:\n",
    "                                                                                vb[jk]=vb1[jk]\n",
    "                                                                    #pass#pass#print(t, vb)\n",
    "                                                                    #pass#pass#print(\"\\n\")             \n",
    "                                                                    dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                                    cc=0\n",
    "                                                                    for kkk in dd:\n",
    "                                                                        if kkk[0] not in chu:\n",
    "                                                                            #if cc<20:\n",
    "                                                                                chu.append(kkk[0])\n",
    "                                                                                cc=cc+1\n",
    "\n",
    "                                                                    if len(chu)>0:\n",
    "                                                                        data_g[t]=chu\n",
    "                                                                #survey checking\n",
    "                                                                #pass#pass#print(len(WORDS1))\n",
    "                                                                #Updating the Whole Evidence Based on manual annotation\n",
    "                                                                WORDS22={}\n",
    "                                                                for gg in WORDS1:\n",
    "                                                                    #if gg in data_extract12:\n",
    "                                                                        #WORDS2[gg]=data_extract12[gg]\n",
    "                                                                    if gg in data_g:\n",
    "                                                                        if len(data_g[gg])>0:\n",
    "                                                                            WORDS22[gg]=data_g[gg]\n",
    "                                                                #pass#pass#print(WORDS2['d_535'])\n",
    "                                                                #pass#pass#print(len(WORDS22))\n",
    "                                                                for t in WORDS22:\n",
    "                                                                    pass#pass#pass#print(t,WORDS22[t])\n",
    "                                                                return WORDS22\n",
    "\n",
    "\n",
    "                                                WORDS,qrat,H11,Rev_text_map,s_words,stopwords=data_pr()\n",
    "                                                WORDS1=data_balancing(WORDS)\n",
    "                                                w11,trg1=train_data_gen(WORDS1)\n",
    "                                                #pass#pass#print(trg1)\n",
    "                                                r_wts=svm_coeff(w11,trg1)\n",
    "                                                H1=sm_h(WORDS1,H11)\n",
    "                                                sent,documents=snt_emd(WORDS1)\n",
    "                                                final_clu=kmean_cls(sent,documents,WORDS1)\n",
    "                                                Samuser_index=sm_user()\n",
    "                                                ann=annotation_wordexp(WORDS1,s_words,stopwords)\n",
    "                                                w=feedback_gen(final_clu)\n",
    "                                                WORDS22=update_evid_annotation(w,WORDS1)\n",
    "                                                return WORDS,WORDS22,qrat,H1,ann,Rev_text_map,s_words,stopwords,WORDS1,r_wts,w11,trg1,H1,sent,documents,final_clu,Samuser_index\n",
    "                                @classmethod\n",
    "                                def relational_embedding_exp(cls,m,WORDS22,WORDS1,Samuser_index,H1):\n",
    "                                    # Relational Exp generatetion based on neural embedding\n",
    "                                                sent2=[]\n",
    "                                                sent1=[]\n",
    "                                                sent_map=defaultdict(list)\n",
    "                                                for ty in WORDS22:\n",
    "                                                    gh=[]\n",
    "                                                    gh.append(str(ty))\n",
    "                                                    #gh1=[]\n",
    "                                                    #gh2=[]\n",
    "                                                    for j in WORDS22[ty]:\n",
    "\n",
    "                                                        j1=str(j)\n",
    "                                                        #gh.append(str(ty))\n",
    "                                                        if j1 not in gh:\n",
    "                                                            gh.append(j1)\n",
    "                                                        ##print(gh)\n",
    "\n",
    "\n",
    "                                                    if gh not in sent2:\n",
    "                                                            sent2.append(gh)\n",
    "\n",
    "\n",
    "                                                documents1=[]\n",
    "                                                #documents1=[]\n",
    "                                                for t in sent2:\n",
    "                                                    s=''\n",
    "                                                    for jh in t:\n",
    "                                                        if jh.isdigit():\n",
    "                                                             documents1.append(jh)\n",
    "                                                        else:\n",
    "                                                            s=\" \"+str(jh)+s+\" \"\n",
    "                                                    documents1.append(s)\n",
    "\n",
    "\n",
    "                                                #sentence embedding\n",
    "                                                from gensim.test.utils import common_texts\n",
    "                                                from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "                                                documents2 = [TaggedDocument(doc, [i]) for i, doc in enumerate(sent2)]\n",
    "                                                for t in documents2:\n",
    "                                                    pass##print(t)\n",
    "                                                model = Doc2Vec(documents2, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "                                                #K-Means Run 14 to find the neighbors per query \n",
    "\n",
    "                                                #cluster generation with k-means\n",
    "                                                import sys\n",
    "                                                from nltk.cluster import KMeansClusterer\n",
    "                                                import nltk\n",
    "                                                from sklearn import cluster\n",
    "                                                from sklearn import metrics\n",
    "                                                import gensim \n",
    "                                                import operator\n",
    "                                                #from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "                                                #model = Word2Vec(sent, min_count=1) dd=sorted(vb.items(), key=operator.itemgetter(1),reverse=False)\n",
    "                                                import operator\n",
    "                                                X = model[model.wv.vocab]\n",
    "                                                c=0\n",
    "                                                cluster={}\n",
    "                                                num=[]\n",
    "                                                weight_map={}\n",
    "                                                similar_r_map={}\n",
    "                                                fg={}\n",
    "                                                wg={}\n",
    "\n",
    "                                                for jj in WORDS1:\n",
    "                                                    gh1=[]\n",
    "                                                    gh2=[]\n",
    "                                                    gh3=[]\n",
    "                                                    s=0\n",
    "\n",
    "                                                    for k in documents1:\n",
    "                                                        if str(k)==str(jj):\n",
    "                                                            gh=model.most_similar(positive=str(k),topn=600)\n",
    "                                                           # #print(gh)\n",
    "                                                            for tt in gh:\n",
    "                                                                if float(tt[1]) not in gh1:\n",
    "                                                                    gh1.append(float(tt[1]))\n",
    "                                                                #if tt[0] not in gh2:\n",
    "                                                                if tt[0].isdigit():\n",
    "                                                                        #if ccc<5:\n",
    "                                                                                #gh2.append(tt[0])\n",
    "                                                                                fg[tt[0]]=tt[1]\n",
    "                                                                                #ccc=ccc+1\n",
    "                                                    #for ffg in gh1:\n",
    "                                                        #s=s+ffg\n",
    "                                                    dd=sorted(fg.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                                    ccc=0\n",
    "                                                    ccc1=0\n",
    "                                                    for t5 in dd:\n",
    "                                                        if qrat[int(jj)]==qrat[int(t5[0])]:\n",
    "                                                            if m==5:\n",
    "                                                                for vbc in Samuser_index:\n",
    "                                                                    if int(jj) in Samuser_index[vbc] or  str(jj) in Samuser_index[vbc] and int(t5[0]) in Samuser_index[vbc] or str(t5[0]) in Samuser_index[vbc]:\n",
    "                                                                                 if ccc<500:\n",
    "                                                                                         gh2.append(t5[0])\n",
    "                                                                                         gh3.append(t5[1])\n",
    "                                                                                         ccc=ccc+1\n",
    "                                                                for vbc in H1:\n",
    "                                                                    if int(jj) in H1[vbc] or  str(jj) in H1[vbc] and int(t5[0]) in H1[vbc] or str(t5[0]) in H1[vbc]:\n",
    "                                                                                if ccc1<500:\n",
    "                                                                                         gh2.append(t5[0])\n",
    "                                                                                         gh3.append(t5[1])\n",
    "                                                                                         ccc1=ccc1+1\n",
    "                                                            elif m==10:\n",
    "                                                                if ccc<400:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==15:\n",
    "                                                                if ccc<500:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==20:\n",
    "                                                                if ccc<600:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "                                                            elif m==25:\n",
    "                                                                if ccc<700:\n",
    "                                                                         gh2.append(t5[0])\n",
    "                                                                         ccc=ccc+1\n",
    "\n",
    "                                                    #if len(gh2)>=2:\n",
    "                                                    similar_r_map[jj]=gh2\n",
    "                                                    wg[jj]=gh3\n",
    "                                                            #ccc=ccc+1\n",
    "\n",
    "                                                return similar_r_map,wg\n",
    "\n",
    "                                @classmethod\n",
    "                                def shap_rel(cls,similar_r_map,wg,WORDS22,WORDS1,qrat,Samuser_index,H1):\n",
    "                                        # Relation Vector Generation\n",
    "                                        rlvec=[]\n",
    "                                        for t in wg:\n",
    "                                            gh=[]\n",
    "                                            gh.append(t)\n",
    "                                            for k in wg[t]:\n",
    "                                                gh.append(k)\n",
    "                                            rlvec.append(gh)\n",
    "\n",
    "                                        # organizing feature vector\n",
    "                                        #related queries \n",
    "                                        pqsu={}\n",
    "                                        pqsh={}\n",
    "\n",
    "                                        for bb in WORDS1:\n",
    "                                            for kk in Samuser_index:\n",
    "                                                if bb in Samuser_index[kk] or str(bb) in Samuser_index[kk] :\n",
    "                                                    pqsu[bb]=Samuser_index[kk]\n",
    "                                                    pass#print(bb,Samuser_index[kk])\n",
    "\n",
    "                                        for bb in WORDS1:\n",
    "                                            for kk in H1:\n",
    "                                                if bb in H1[kk] or str(bb) in H1[kk]:\n",
    "                                                    if bb in pqsu:\n",
    "                                                            #print(bb,H1[kk])\n",
    "                                                            pqsh[bb]=H1[kk]\n",
    "                                        relq={}\n",
    "                                        for k in pqsh:\n",
    "                                            #print(k,pqsh[k]+pqsu[k])\n",
    "                                            relq[k]=pqsh[k]+pqsu[k]\n",
    "                                        \n",
    "                                        qf={}\n",
    "                                        for t in similar_r_map:\n",
    "                                            if t in WORDS1 and t in relq:\n",
    "                                                h=relq[t]+WORDS1[t]\n",
    "                                                #print(t,qrat[t],h),\n",
    "                                                qf[t]=h\n",
    "                                                #print(\"\\n\\n\")\n",
    "                                        # train test data\n",
    "                                        import math\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        train=[]\n",
    "                                        target=[]\n",
    "                                        test=[]\n",
    "                                        test_tr=[]\n",
    "                                        for tt in qf:\n",
    "                                            for v in qf[tt]:\n",
    "                                                #print(v)\n",
    "                                                if v not in train:\n",
    "                                                   # if v.isdigit()==False:\n",
    "                                                        train.append(v)\n",
    "                                                        target.append(str(qrat[tt]))\n",
    "                                        bb=math.ceil(len(train)*0.5)\n",
    "                                        for gg in range(0,bb):\n",
    "                                            test.append(train[gg])\n",
    "                                            test_tr.append(target[gg])\n",
    "                                        #LIME w11, trg\n",
    "                                        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        #Shap\n",
    "                                        import sklearn\n",
    "                                        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                                        from sklearn.model_selection import train_test_split\n",
    "                                        import numpy as np\n",
    "                                        import shap\n",
    "                                        import transformers\n",
    "                                        import shap\n",
    "                                        from sklearn.metrics import (precision_score,recall_score,f1_score)\n",
    "                                        #shap.initjs()\n",
    "                                        # Kernal Shap words_train targets\n",
    "\n",
    "                                        from sklearn import svm\n",
    "                                        from sklearn.svm import SVC\n",
    "                                        from sklearn.svm import LinearSVC\n",
    "                                        corpus_train, corpus_test, y_train, y_test = train_test_split(train,target, test_size=0.5, random_state=7)\n",
    "                                        vectorizer = TfidfVectorizer(min_df=1)\n",
    "                                        X_train = vectorizer.fit_transform(train)\n",
    "                                        X_test = vectorizer.transform(corpus_test)\n",
    "                                        model =svm.LinearSVC(C=100) #sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "                                        model.fit(X_train,target)\n",
    "                                        p = model.predict(X_train)\n",
    "                                        prr={}\n",
    "                                        for jj in range(0,len(train)):\n",
    "                                            prr[train[jj]]=int(p[jj])\n",
    "                                        explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"dependent\")\n",
    "                                        shap_values = explainer.shap_values(X_train)\n",
    "                                        #X_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions\n",
    "                                        feature_names=vectorizer.get_feature_names()\n",
    "                                        #print(len(feature_names),len(shap_values))\n",
    "                                        #shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())\n",
    "                                        shape_w={}\n",
    "                                        fr={}\n",
    "                                        feature_sh_v=[]\n",
    "                                        for jj in range(0,len(train)):\n",
    "                                                              m=abs(sum(shap_values[jj]))\n",
    "                                                              if train[jj] not in fr:\n",
    "                                                                              fr[train[jj]]=abs(sum(shap_values[jj]))\n",
    "                                                              elif train[jj]  in fr:\n",
    "                                                                    if m>fr[train[jj]]:\n",
    "                                                                        fr[train[jj]]=m\n",
    "                                        dd1=sorted(fr.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                        for tt in dd1:\n",
    "                                            if tt[0].isdigit()==True:\n",
    "                                                   feature_sh_v.append(tt[0])\n",
    "                                            elif tt[0].isdigit()==False:\n",
    "                                                feature_sh_v.append(tt[0])\n",
    "                                                #for vvv5 in WORDS22:\n",
    "                                                    #if tt[0] in WORDS22[vvv5] or str(tt[0]) in WORDS22[vvv5]:\n",
    "                                                                                      # feature_sh_v.append(tt[0])\n",
    "                                        '''\n",
    "                                        feature_sh_v1=[]\n",
    "                                        for v in feature_sh_v:\n",
    "                                            n=v.split()\n",
    "                                            for k in n:\n",
    "                                                if k not in feature_sh_v1:\n",
    "                                                    feature_sh_v1.append(k)\n",
    "                                        '''\n",
    "                                        #shap explanations\n",
    "\n",
    "                                        shap_exp={}\n",
    "                                        for t in qf:\n",
    "                                            gh=[]\n",
    "                                            c=0\n",
    "                                            for k in qf[t]:\n",
    "                                                if k in prr:\n",
    "                                                    if qrat[t]==prr[k]:\n",
    "                                                        if k in feature_sh_v:\n",
    "                                                            if k not in gh:\n",
    "                                                                #if c<20:\n",
    "                                                                    gh.append(k)\n",
    "                                                                    c=c+1\n",
    "                                            shap_exp[t]=gh\n",
    "\n",
    "\n",
    "                                        #shap.plots.beeswarm(shap_values)\n",
    "                                        return shap_exp,fr,shap_values\n",
    "                                @classmethod  \n",
    "                                def organize_results_visualization(cls,Samuser_index,H1,qrat,shap_exp):\n",
    "                                            qrsm={}\n",
    "                                            qrsh={}\n",
    "                                            qrw={}\n",
    "                                            qrall={}\n",
    "                                            cnc=[]\n",
    "                                            cnc1={}\n",
    "                                            for vv in shap_exp:\n",
    "                                                #print(vv,qrat[vv],lexp[vv])\n",
    "                                                sm=[]\n",
    "                                                sh=[]\n",
    "                                                wd=[]\n",
    "                                                arlw=[]\n",
    "                                                for hhh in shap_exp[vv]:\n",
    "                                                    bb=hhh.split(\":\")\n",
    "                                                    arlw.append(bb[0])\n",
    "                                                    if bb[0].isdigit():\n",
    "                                                        for vv1 in Samuser_index:\n",
    "                                                            if int(bb[0])!=int(vv):\n",
    "                                                                if str(vv) in Samuser_index[vv1] or int(vv) in Samuser_index[vv1] and int(bb[0]) in Samuser_index[vv1] or str(bb[0]) in Samuser_index[vv1]:\n",
    "                                                                    #print(vv,bb[0])\n",
    "                                                                    sm.append(bb[0])\n",
    "                                                        for vv1 in H1:\n",
    "                                                            if int(bb[0])!=int(vv):\n",
    "                                                                if str(vv) in H1[vv1] or int(vv) in H1[vv1] and int(bb[0]) in H1[vv1] or str(bb[0]) in H1[vv1]:\n",
    "                                                                    #print(\"Samehotel\")\n",
    "                                                                    #print(vv,bb[0])\n",
    "                                                                    sh.append(bb[0])\n",
    "                                                    else:\n",
    "                                                        wd.append(bb[0])\n",
    "                                                if len(sm)>0:\n",
    "                                                    qrsm[vv]=sm\n",
    "                                                if len(sh)>0:\n",
    "                                                    qrsh[vv]=sh\n",
    "                                                if len(wd)>0:\n",
    "                                                    qrw[vv]=wd\n",
    "                                                if len(arlw)>0:\n",
    "                                                    qrall[vv]=arlw\n",
    "                                            for cv in qrsm:\n",
    "                                                if cv in  qrsh and cv in qrw and cv in qrall:\n",
    "                                                        cnc=qrsm[cv]+qrsh[cv]\n",
    "                                                        cnc1[cv]=cnc\n",
    "                                                        #print(cv,qrsm[cv],qrsh[cv],qrw[cv] )\n",
    "                                                        #print(\"\\n\\n\")\n",
    "\n",
    "                                            vb={}\n",
    "                                            for j in cnc1:\n",
    "                                                gh=[]\n",
    "                                                for jj in cnc1[j]:\n",
    "                                                    if qrat[j]==qrat[int(jj)]:\n",
    "                                                        #print(j,jj,qrat[j],qrat[int(jj)])\n",
    "                                                        #cv=\"(\"+str(j)+\",\"+str(jj)+\")\"\n",
    "                                                        gh.append(jj)\n",
    "                                                vb[j]=gh\n",
    "                                            return vb,qrsm,qrsh,qrw,qrall\n",
    "                                @classmethod\n",
    "                                def draw(cls,vb,qrsm,qrsh,qrw,qrall,ch):\n",
    "                                            if ch==\"Sameuser\":\n",
    "                                                c=0\n",
    "                                                for gh in qrsm:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrsm[gh])\n",
    "                                                        zx1=[0]*len(qrsm[gh])\n",
    "                                                        vcc=qrsm[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Samehotel\":\n",
    "                                                c=0\n",
    "                                                for gh in qrsh:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrsh[gh])\n",
    "                                                        zx1=[0]*len(qrsh[gh])\n",
    "                                                        vcc=qrsh[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Word\":\n",
    "                                                c=0\n",
    "                                                for gh in qrw:\n",
    "                                                    if c<1.0:\n",
    "                                                        if qrat[gh]==2:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                        else:\n",
    "                                                            print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "                                                        zx=[0]*len(qrw[gh])\n",
    "                                                        zx1=[0]*len(qrw[gh])\n",
    "                                                        vcc=qrw[gh]\n",
    "                                                        #print(vcc)\n",
    "                                                        for jk in range(0,len(zx)):\n",
    "                                                            zx[jk]=gh\n",
    "                                                            zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                        df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                        # Build your graph\n",
    "                                                        G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                        # Custom the nodes:\n",
    "                                                        fig = plt.figure()\n",
    "                                                        nx.draw(G, with_labels=True, node_color='skyblue', node_size=2500, edge_color='white')\n",
    "                                                        fig.set_facecolor(\"#00000F\")\n",
    "                                                        c=c+1\n",
    "                                            elif ch==\"Justifying_prediction\":\n",
    "                                                    c=0\n",
    "                                                    for gh in vb:\n",
    "                                                        if c<1:\n",
    "                                                            if qrat[gh]==2:\n",
    "                                                                    print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Positive Review\"+\"\\n\")\n",
    "                                                            else:\n",
    "                                                                print(\"Review:\"+str(gh)+\" Truly Predicted as\"+\" Negative Review\"+\"\\n\")\n",
    "\n",
    "\n",
    "                                                            zx=[0]*len(vb[gh])\n",
    "                                                            zx1=[0]*len(vb[gh])\n",
    "                                                            if qrat[gh]==2:\n",
    "                                                                vcc=[]\n",
    "                                                                for vx in vb[gh]:\n",
    "                                                                    gh1=str(vx)+\"_Positive\"\n",
    "                                                                    vcc.append(gh1)\n",
    "                                                            elif qrat[gh]==0:\n",
    "                                                                vcc=[]\n",
    "                                                                for vx in vb[gh]:\n",
    "                                                                    gh2=str(vx)+\"_Negative\"\n",
    "                                                                    vcc.append(gh2)\n",
    "                                                            #print(vcc)\n",
    "                                                            for jk in range(0,len(zx)):\n",
    "                                                                if  qrat[gh]==2:\n",
    "                                                                    vzz=str(gh)+\"_Positive\"\n",
    "                                                                    zx[jk]=vzz\n",
    "                                                                    zx1[jk]=vcc[jk]\n",
    "                                                                elif  qrat[gh]==0:\n",
    "                                                                    vzz=str(gh)+\"_Negative\"\n",
    "                                                                    zx[jk]=vzz\n",
    "                                                                    zx1[jk]=vcc[jk]\n",
    "\n",
    "                                                            df = pd.DataFrame({ 'from':zx, 'to':zx1})\n",
    "                                                            # Build your graph\n",
    "                                                            G=nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.Graph() )\n",
    "\n",
    "                                                            # Custom the nodes:\n",
    "                                                            fig = plt.figure()\n",
    "                                                            nx.draw(G, with_labels=True, node_color='skyblue', node_size=8000, edge_color='white')\n",
    "                                                            fig.set_facecolor(\"#00000F\")\n",
    "                                                            c=c+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    \n",
    "                                    \n",
    "                WORDS,WORDS22,qrat,H1,ann,Rev_text_map,s_words,stopwords,WORDS1,r_wts,w11,trg1,H1,sent,documents,final_clu,Samuser_index=relational_lime.run_all()\n",
    "                similar_r_map,wg=relational_lime.relational_embedding_exp(5,WORDS22,WORDS1,Samuser_index,H1)\n",
    "                shap_exp,fr,shap_values=relational_lime.shap_rel(similar_r_map,wg,WORDS22,WORDS1,qrat,Samuser_index,H1)\n",
    "               # vb,qrsm,qrsh,qrw,qrall=relational_lime.organize_results_visualization(Samuser_index,H1,qrat,shap_exp)\n",
    "                # Graph Representation of the Explanation: \n",
    "                #Choices: \n",
    "                #'Samehotel' or 'Sameuser' for relational explanation\n",
    "                # 'Word' for the Word explanation graph\n",
    "                #'Justifying_prediction' to show the graph connected with \n",
    "                #review that have the same class as the review class\n",
    "                ch=['Sameuser','Samehotel','Word','Justifying_prediction']\n",
    "                for kk in ch:\n",
    "                        pass#relational_lime.draw(vb,qrsm,qrsh,qrw,qrall,kk)\n",
    "                return shap_exp,fr,shap_values,ann,similar_r_map,WORDS,Samuser_index,H1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501ef492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shap_accuracy():\n",
    "    shap_exp,fr,shap_values,ann,similar_r_maps,WORDS,Samuser_index,H1=Run_program()\n",
    "    shap_word_exp={}\n",
    "    shap_rel_exp={}\n",
    "    for hh in shap_exp:\n",
    "        gh=[]\n",
    "        gh1=[]\n",
    "        cc=0\n",
    "        cc1=0\n",
    "        for hh1 in shap_exp[hh]:\n",
    "            if hh1.isdigit()==True:\n",
    "                if cc<5:\n",
    "                    gh.append(hh1)\n",
    "                    cc=cc+1\n",
    "            else:\n",
    "                if cc1<5:\n",
    "                    gh1.append(hh1)\n",
    "                    cc1=cc1+1\n",
    "        shap_word_exp[hh]=gh1\n",
    "        shap_rel_exp[hh]=gh\n",
    "    #words accuracy\n",
    "    wordacc={}\n",
    "    for kk in shap_word_exp:\n",
    "        c=0\n",
    "        vx=0\n",
    "        if kk in ann:\n",
    "            for vc in shap_word_exp[kk]:\n",
    "                if vc in ann[kk]:\n",
    "                    if vx<5:\n",
    "                        c=c+1\n",
    "                        vx=vx+1\n",
    "            try:\n",
    "                gg=c/float(len(shap_word_exp[kk]))\n",
    "                if gg>0:\n",
    "                        wordacc[kk]=gg\n",
    "            except:\n",
    "                continue \n",
    "    #average word exp accuracy\n",
    "\n",
    "    svv=0\n",
    "    for t in wordacc:\n",
    "            svv=svv+float(wordacc[t])\n",
    "    print(\"Word_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "    print(svv/float(len(wordacc)))\n",
    "\n",
    "    #relational explanation accuracy\n",
    "    relacc={}\n",
    "\n",
    "    for kk in shap_rel_exp:\n",
    "        c=0\n",
    "        cn=0\n",
    "        if kk in similar_r_maps and kk in ann:\n",
    "            for vc in shap_rel_exp[kk]:\n",
    "                if vc in similar_r_maps[kk] or int(vc) in similar_r_maps[kk] or str(vc) in similar_r_maps[kk]:\n",
    "                    if cn<5:\n",
    "                        c=c+1\n",
    "                        cn=cn+1\n",
    "            try:\n",
    "                    gg=c/float(len(shap_rel_exp[kk]))\n",
    "            except:\n",
    "                continue \n",
    "            if gg>0:\n",
    "                relacc[kk]=gg\n",
    "    #average relational exp accuracy\n",
    "    svv=0\n",
    "    for t in relacc:\n",
    "            svv=svv+float(relacc[t])\n",
    "    print(\"\\n\")\n",
    "    print(\"Relational_Explanation_Average_Accuracy_Shap\"+\"\\n\")\n",
    "    print(svv/float(len(relacc)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df945ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "The option feature_dependence has been renamed to feature_perturbation!\n",
      "The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Explanation_Average_Accuracy_Shap\n",
      "\n",
      "0.278422782037238\n"
     ]
    }
   ],
   "source": [
    "shap_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381118c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
